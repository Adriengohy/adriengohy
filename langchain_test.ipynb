{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>langchain tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"381001efce9a4f468afacd97aba385c4\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"]=\"https://lucasaiservices.openai.azure.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from openai import AzureOpenAI\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create an instance of Azure OpenAI\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Replace the deployment name with your own\u001b[39;00m\n\u001b[0;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m AzureOpenAI(deployment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdavinci-002\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAI\n",
    "#from openai import AzureOpenAI\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "# Replace the deployment name with your own\n",
    "llm = AzureOpenAI(deployment_name=\"davinci-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Can you trust your test tool vendor?\\n\\nThe answer is yes, you can trust Langsmith. And the reason is the team behind Langsmith.\\n\\nLangsmith’s founders and developers are experienced testing experts who are very familiar with the issues that a test tool faces when being applied to a real-life testing project. They know that testing is not an isolated activity that can be done in a vacuum. Testing is an integral part of the project, and it is directly linked to the customer’s needs and the project’s goals.\\n\\nLangsmith’s developers have used the software they create and they are still using it today. They are actively involved in the testing community, sharing their experience and knowledge with the community. The Langsmith team has also developed a proprietary testing framework that is based on their real-life experience and expertise.\\n\\nThere are many test tools on the market today, but there is no other test tool that is better than Langsmith. The software is easy to use and the results are accurate and reliable. The Langsmith team is available and eager to help, and they will take the time to understand your testing needs and provide solutions that will help you achieve your goals.\\n\\nThe Langsmith team is not just committed to developing the best tool on the market, they are also committed to ensuring that'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE786\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:168\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:163\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 163\u001b[0m         sock \u001b[39m=\u001b[39m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(\n\u001b[0;32m    164\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# pragma: nocover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1379\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1380\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:66\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[0;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mstart_tls\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m--> 156\u001b[0m     stream \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mstart_tls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[0;32m    149\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    150\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    151\u001b[0m }\n\u001b[1;32m--> 152\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[0;32m    153\u001b[0m     \u001b[39mtry\u001b[39;49;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 918\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[0;32m    919\u001b[0m         request,\n\u001b[0;32m    920\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[0;32m    921\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    922\u001b[0m     )\n\u001b[0;32m    923\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[0;32m    902\u001b[0m     request,\n\u001b[0;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[0;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[0;32m    930\u001b[0m         request,\n\u001b[0;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[0;32m    933\u001b[0m     )\n\u001b[0;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    964\u001b[0m     hook(request)\n\u001b[1;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[1;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:227\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[0;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[0;32m    226\u001b[0m )\n\u001b[1;32m--> 227\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[0;32m    228\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:83\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[1;32m---> 83\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE786\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:168\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:163\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 163\u001b[0m         sock \u001b[39m=\u001b[39m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(\n\u001b[0;32m    164\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# pragma: nocover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1379\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1380\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:66\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[0;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mstart_tls\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m--> 156\u001b[0m     stream \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mstart_tls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[0;32m    149\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    150\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    151\u001b[0m }\n\u001b[1;32m--> 152\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[0;32m    153\u001b[0m     \u001b[39mtry\u001b[39;49;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 918\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[0;32m    919\u001b[0m         request,\n\u001b[0;32m    920\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[0;32m    921\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    922\u001b[0m     )\n\u001b[0;32m    923\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[0;32m    902\u001b[0m     request,\n\u001b[0;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[0;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[0;32m    930\u001b[0m         request,\n\u001b[0;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[0;32m    933\u001b[0m     )\n\u001b[0;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    964\u001b[0m     hook(request)\n\u001b[1;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[1;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:227\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[0;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[0;32m    226\u001b[0m )\n\u001b[1;32m--> 227\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[0;32m    228\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:83\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[1;32m---> 83\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE786\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:168\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 168\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m SyncStream(sock)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:163\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 163\u001b[0m         sock \u001b[39m=\u001b[39m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(\n\u001b[0;32m    164\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# pragma: nocover\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1108\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1109\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1379\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1380\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:66\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 228\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[0;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m    100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(request)\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mstart_tls\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m--> 156\u001b[0m     stream \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mstart_tls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    157\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_backends\\sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[39m=\u001b[39m {\n\u001b[0;32m    149\u001b[0m     socket\u001b[39m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    150\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    151\u001b[0m }\n\u001b[1;32m--> 152\u001b[0m \u001b[39mwith\u001b[39;49;00m map_exceptions(exc_map):\n\u001b[0;32m    153\u001b[0m     \u001b[39mtry\u001b[39;49;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 918\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[0;32m    919\u001b[0m         request,\n\u001b[0;32m    920\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[0;32m    921\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    922\u001b[0m     )\n\u001b[0;32m    923\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    899\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 901\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[0;32m    902\u001b[0m     request,\n\u001b[0;32m    903\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[0;32m    904\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    905\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m    906\u001b[0m )\n\u001b[0;32m    907\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 929\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[0;32m    930\u001b[0m         request,\n\u001b[0;32m    931\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[0;32m    932\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[0;32m    933\u001b[0m     )\n\u001b[0;32m    934\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    964\u001b[0m     hook(request)\n\u001b[1;32m--> 966\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[1;32m-> 1002\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[0;32m   1004\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:227\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[0;32m    216\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    217\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[0;32m    226\u001b[0m )\n\u001b[1;32m--> 227\u001b[0m \u001b[39mwith\u001b[39;49;00m map_httpcore_exceptions():\n\u001b[0;32m    228\u001b[0m     resp \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\httpx\\_transports\\default.py:83\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[1;32m---> 83\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lcornette001\\Documents\\GitHub\\xavier pharma poc\\langchain_test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lcornette001/Documents/GitHub/xavier%20pharma%20poc/langchain_test.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m output_parser \u001b[39m=\u001b[39m StrOutputParser()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lcornette001/Documents/GitHub/xavier%20pharma%20poc/langchain_test.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m model \u001b[39m|\u001b[39m output_parser\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lcornette001/Documents/GitHub/xavier%20pharma%20poc/langchain_test.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mtopic\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mice cream\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 2056\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   2057\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2058\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2059\u001b[0m             patch_config(\n\u001b[0;32m   2060\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2061\u001b[0m             ),\n\u001b[0;32m   2062\u001b[0m         )\n\u001b[0;32m   2063\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   2064\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    156\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    162\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m    163\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[0;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    165\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 166\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    167\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    168\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    169\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    170\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    171\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    172\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    173\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    174\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    175\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    542\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    543\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 544\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    407\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[1;32m--> 408\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    409\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    410\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    411\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    412\u001b[0m ]\n\u001b[0;32m    413\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    396\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 398\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[0;32m    399\u001b[0m                 m,\n\u001b[0;32m    400\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    401\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    402\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    403\u001b[0m             )\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 577\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    578\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:462\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    457\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m    458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    459\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m    460\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    461\u001b[0m }\n\u001b[1;32m--> 462\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    463\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    613\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    662\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 663\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    664\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    665\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    666\u001b[0m             {\n\u001b[0;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    669\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    670\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m    671\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m    672\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    673\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[0;32m    674\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    675\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    676\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    677\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    678\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    679\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    680\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    681\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    682\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    683\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    684\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[0;32m    685\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    686\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    687\u001b[0m             },\n\u001b[0;32m    688\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    689\u001b[0m         ),\n\u001b[0;32m    690\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    691\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    692\u001b[0m         ),\n\u001b[0;32m    693\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    694\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    695\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    696\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1196\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1197\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1198\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1199\u001b[0m     )\n\u001b[1;32m-> 1200\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    881\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    888\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 889\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    890\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    891\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    892\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    893\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    894\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    895\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    939\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    941\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    943\u001b[0m         options,\n\u001b[0;32m    944\u001b[0m         cast_to,\n\u001b[0;32m    945\u001b[0m         retries,\n\u001b[0;32m    946\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    947\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    948\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    952\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1013\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m   1014\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m   1015\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m   1016\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m   1017\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1018\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1019\u001b[0m )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    939\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    941\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    943\u001b[0m         options,\n\u001b[0;32m    944\u001b[0m         cast_to,\n\u001b[0;32m    945\u001b[0m         retries,\n\u001b[0;32m    946\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    947\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    948\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    951\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    952\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1013\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m   1014\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m   1015\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m   1016\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m   1017\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1018\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1019\u001b[0m )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\nsotest\\Lib\\site-packages\\openai\\_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[0;32m    943\u001b[0m             options,\n\u001b[0;32m    944\u001b[0m             cast_to,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m             response_headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    949\u001b[0m         )\n\u001b[0;32m    951\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 952\u001b[0m     \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    954\u001b[0m log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    955\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[0;32m    956\u001b[0m )\n\u001b[0;32m    958\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\", openai_api_key=\"381001efce9a4f468afacd97aba385c4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>result object check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "def initialize_kernel(model=\"gpt-35-turbo\"):\n",
    "    kernel = sk.Kernel()\n",
    "    \n",
    "    #deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    if model == \"gpt-4\":\n",
    "        deployment = \"gpt-4\"\n",
    "        endpoint = \"https://lucasaiservices.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2023-03-15-preview\"\n",
    "    elif model == \"gpt-4-2\":\n",
    "        deployment = \"gpt-4-2\"\n",
    "        endpoint = \"https://lucasaiservices.openai.azure.com/openai/deployments/gpt-4-2/chat/completions?api-version=2023-03-15-preview\"\n",
    "    else:\n",
    "        deployment = \"gpt-35-turbo\"\n",
    "        endpoint = \"https://lucasaiservices.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2024-02-15-preview\"\n",
    "    \n",
    "    api_key = \"381001efce9a4f468afacd97aba385c4\"\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(service_id=service_id,\n",
    "                            deployment_name=deployment,\n",
    "                            endpoint=endpoint,\n",
    "                            api_key=api_key)\n",
    "    )\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def initialize_functions(kernel):\n",
    "    function_dict = {}\n",
    "    \n",
    "    # load in all functions\n",
    "    plugins_directory = \"./plugins\"\n",
    "\n",
    "    #functies voor interface\n",
    "    function_list = kernel.import_plugin_from_prompt_directory(plugins_directory, \"OutlinePlugin\")\n",
    "    function_dict[\"brainstorm\"] = function_list[\"BrainstormOutline\"]\n",
    "    function_dict[\"check\"] = function_list[\"BrainstormCheck\"]\n",
    "    function_dict[\"create_title\"] = function_list[\"CreateTitle\"]\n",
    "    function_dict[\"outline\"] = function_list[\"Outline\"]\n",
    "    function_dict[\"outline_split\"] = function_list[\"OutlineSplit\"]\n",
    "    function_dict[\"json_create\"] = function_list[\"OutlineJson\"]\n",
    "    \n",
    "    \n",
    "    #functies voor document generation\n",
    "    function_list = kernel.import_plugin_from_prompt_directory(plugins_directory, \"WebSearchPlugin\")\n",
    "    function_dict[\"query\"] = function_list[\"CreateQuery\"]\n",
    "    function_dict[\"search_check\"] = function_list[\"SearchCheck\"]\n",
    "    function_dict[\"public_search\"] = kernel.import_native_plugin_from_directory(plugins_directory+\"/WebSearchPlugin\", \"PublicSearch\")[\"WebSearch\"]\n",
    "    function_dict[\"write_section\"] = kernel.import_plugin_from_prompt_directory(plugins_directory, \"WriterPlugin\")[\"WriteSection\"]\n",
    "    \n",
    "    #plugins_directory = \"./plugins/ResearchPlugin\"\n",
    "    #function_list = kernel.import_native_plugin_from_directory(plugins_directory, \"Websearch\")\n",
    "    #f = function_list[\"QueryToResults\"]\n",
    "    \n",
    "    return function_dict\n",
    "\n",
    "kernel = initialize_kernel()\n",
    "function_dict = initialize_functions(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcornette001\\AppData\\Local\\Temp\\ipykernel_11956\\2226693672.py:1: RuntimeWarning: coroutine 'generate_document' was never awaited\n",
      "  result = await kernel.invoke(function_dict[\"brainstorm\"], history=\"lol\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "result = await kernel.invoke(function_dict[\"brainstorm\"], history=\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KernelFunctionMetadata' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\pydantic\\main.py:792\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KernelFunctionMetadata' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "result.function.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VPN error check en rate limit error check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = \"\"\"Error occurred while invoking function WriteSection: Error occurred while invoking function WriteSection: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-05-15 have exceeded token rate limit of your current AIServices S0 pricing tier. Please retry after 3 seconds. Please contact Azure support service if you would like to further increase the default rate limit.'}}\"))\n",
    "Error occurred while invoking function: 'WriterPlugin.WriteSection'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "match = re.search(r'Please retry after (\\d+) seconds', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _call_api_with_rate_limit_handling(kernel, function, arguments):\n",
    "    def _extract_retry_seconds(error_message):\n",
    "        # Use regex to find the number of seconds to wait\n",
    "        print(re.search(r'Please retry after (\\d+) seconds', error_message))\n",
    "        match = re.search(r'Please retry after (\\d+) seconds', error_message)\n",
    "        if match:\n",
    "            return int(match.group(1))  # Return the number of seconds as an integer\n",
    "        return None  # Return None if no match is found\n",
    "    \n",
    "    try:\n",
    "        # Attempt to call the API\n",
    "        response = await kernel.invoke(function, arguments)\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        wait_time = _extract_retry_seconds(e)\n",
    "        if wait_time is not None:\n",
    "            print(f\"Rate limit exceeded, waiting for {wait_time} seconds...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "            return await _call_api_with_rate_limit_handling(kernel, function, arguments)\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "from ast import literal_eval\n",
    "import semantic_kernel as sk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "from utils.document_handling import get_input_file, fill_in_front_page, write_document\n",
    "\n",
    "\n",
    "async def _call_api_with_rate_limit_handling(kernel, function, arguments):\n",
    "    def _extract_retry_seconds(error_message):\n",
    "        # Use regex to find the number of seconds to wait\n",
    "        match = re.search(r'Please retry after (\\d+) seconds', error_message)\n",
    "        if match:\n",
    "            return int(match.group(1))  # Return the number of seconds as an integer\n",
    "        return None  # Return None if no match is found\n",
    "    \n",
    "    try:\n",
    "        # Attempt to call the API\n",
    "        response = await kernel.invoke(function, arguments)\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Check if the exception has specific attributes you expect from the SDK documentation\n",
    "        error_message = \"\"\n",
    "        if hasattr(e, 'message'):\n",
    "            print(\"eerste if\")\n",
    "            error_message = e.message\n",
    "            print(error_message)\n",
    "        elif hasattr(e, 'args') and len(e.args) > 0:\n",
    "            print(\"tweede if\")\n",
    "            print(len(e.args))\n",
    "            # Sometimes the message might be deeply nested in args\n",
    "            print(\"arg 0: \", e.args[0])\n",
    "        else:\n",
    "            print(\"else\")\n",
    "            error_message = str(e)\n",
    "            print(error_message)\n",
    "\n",
    "        wait_time = _extract_retry_seconds(error_message)\n",
    "        if wait_time is not None:\n",
    "            print(f\"Rate limit exceeded, waiting for {wait_time} seconds...\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "            return await _call_api_with_rate_limit_handling(kernel, function, arguments)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "async def _get_text(kernel, function_dict, prompt, outline):\n",
    "    #search_check = str(await kernel.invoke(function_dict[\"search_check\"], sk.KernelArguments(input=prompt)))\n",
    "    #print(search_check)\n",
    "    search_check = \"False\"\n",
    "    \n",
    "    if search_check == \"True\":\n",
    "        query = str(await kernel.invoke(function_dict[\"query\"], sk.KernelArguments(input=prompt, outline=outline)))\n",
    "        search_results = str(await kernel.invoke(function_dict[\"public_search\"], sk.KernelArguments(input=query)))\n",
    "    else:\n",
    "        search_results = \"\"\n",
    "    \n",
    "    if len(str(search_results)) > 16000:\n",
    "        search_results = str(search_results[:16000])\n",
    "    \n",
    "    text = str(await _call_api_with_rate_limit_handling(kernel, function_dict[\"write_section\"], sk.KernelArguments(input=prompt, searchResults=search_results)))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "### Process each chapter\n",
    "async def _fill_in_outline(kernel, function_dict, section, outline): # recursieve functie die doorheen prompts gaat en telkens de prompts naar tekst omzet\n",
    "    title = section[\"title\"]\n",
    "    \n",
    "    content=[]\n",
    "    for prompt in section[\"prompts\"]:\n",
    "        content.append(await _get_text(kernel, function_dict, prompt, outline))\n",
    "    \n",
    "    sections=[]\n",
    "    for subsection in section[\"sections\"]:\n",
    "        sections.append(await _fill_in_outline(kernel, function_dict, subsection, outline))\n",
    "    \n",
    "    return {\"title\":title, \"content\":content, \"sections\":sections}\n",
    "\n",
    "\n",
    "def _doc_to_bytes(doc):\n",
    "    doc_bytes = io.BytesIO()  \n",
    "    doc.save(doc_bytes)  \n",
    "    doc_bytes.seek(0)\n",
    "    \n",
    "    return doc_bytes\n",
    "\n",
    "\n",
    "async def _generate_document_bytes(kernel, function_dict, title, outline_json: dict):\n",
    "    load_dotenv()\n",
    "    # initiatie template document bovenhalen\n",
    "    doc = get_input_file(\"./data/input_file.docx\")\n",
    "\n",
    "    # prompts naar gpt gegenereerde tekst omzetten\n",
    "    content_json = {\"sections\":[]}\n",
    "    # Create tasks for filling in each section outline\n",
    "    fill_section_tasks = [asyncio.create_task(_fill_in_outline(kernel, function_dict, section, outline_json))\n",
    "                          for section in outline_json[\"sections\"]]\n",
    "    \n",
    "    # Await all tasks to complete and maintain order of sections\n",
    "    filled_sections = await asyncio.gather(*fill_section_tasks)\n",
    "    content_json[\"sections\"].extend(filled_sections)\n",
    "\n",
    "    # maak voorpagina voor document\n",
    "    doc = fill_in_front_page(doc, title)\n",
    "    # tekst in het document steken\n",
    "    for section in content_json[\"sections\"]:\n",
    "        doc = write_document(doc, section)\n",
    "\n",
    "    # bytes object voor download\n",
    "    doc_bytes = _doc_to_bytes(doc)\n",
    "    \n",
    "    return doc_bytes\n",
    "\n",
    "\n",
    "async def _create_section_json(kernel, function_dict, outline_section_string):\n",
    "    # Create tasks for asynchronously processing each outline section to create JSON\n",
    "    outline_section_json_string = str(await _call_api_with_rate_limit_handling(kernel, function_dict[\"json_create\"], sk.KernelArguments(outline=outline_section_string)))\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load the JSON structure from the response\n",
    "        outline_section_json = json.loads(outline_section_json_string)\n",
    "    except json.JSONDecodeError:\n",
    "        # Log any JSON decode errors\n",
    "        print(\"Error decoding JSON:\")\n",
    "        print(outline_section_json_string)\n",
    "        outline_section_json = json.loads(outline_section_json_string[7:-3])\n",
    "    \n",
    "    return outline_section_json\n",
    "\n",
    "\n",
    "async def _create_outline_json(kernel, function_dict, history):\n",
    "    # Retrieve the definitive outline string\n",
    "    outline_string = str(await kernel.invoke(function_dict[\"outline\"], history=history))\n",
    "    \n",
    "    # Convert outline string to a list of outline strings\n",
    "    outline_list_string = str(await kernel.invoke(function_dict[\"outline_split\"], outline=outline_string))\n",
    "    \n",
    "    try:\n",
    "        # Try to evaluate the outline list to a Python list object\n",
    "        outline_list = literal_eval(outline_list_string)\n",
    "    except SyntaxError:\n",
    "        # Handle potential issues with string literals\n",
    "        outline_list = outline_list_string.replace(\"\\\"\", \"\\\"\\\"\\\"\")\n",
    "        outline_list = literal_eval(outline_list_string)\n",
    "    \n",
    "    # Prepare a list of tasks\n",
    "    tasks = [_create_section_json(kernel, function_dict, outline_section_string) for outline_section_string in outline_list]\n",
    "    \n",
    "    # Run tasks concurrently and wait for all to complete\n",
    "    sections = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Collect results into outline_json\n",
    "    outline_json = {\"sections\": sections}\n",
    "    \n",
    "    # maak titel\n",
    "    title = str(await kernel.invoke(function_dict[\"create_title\"], outline=outline_string))\n",
    "    \n",
    "    return title, outline_json\n",
    "\n",
    "\n",
    "async def generate_document(kernel, function_dict, history):\n",
    "    title, outline_json = await _create_outline_json(kernel, function_dict, history)\n",
    "    doc_bytes = await _generate_document_bytes(kernel, function_dict, title, outline_json)\n",
    "    return title, doc_bytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcornette001\\AppData\\Local\\Temp\\ipykernel_11956\\1953984981.py:1: RuntimeWarning: coroutine 'generate_document' was never awaited\n",
      "  result = await generate_document(kernel, function_dict, history=\"\"\"Introduction\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "result = await generate_document(kernel, function_dict, history=\"\"\"Introduction\n",
    "\n",
    "Brief overview of the pharmaceutical industry\n",
    "Importance and impact of the industry on global health and economy\n",
    "Trends in the Pharma Industry\n",
    "\n",
    "Shift towards personalized medicine\n",
    "Increasing focus on biotechnology and biopharmaceuticals\n",
    "Embracing digital health technologies\n",
    "Regulatory changes and their impact on the industry\n",
    "The Position of China and India in Pharma\n",
    "\n",
    "Overview of the pharmaceutical markets in China and India\n",
    "Growth and development of the industry in both countries\n",
    "Challenges and opportunities for pharmaceutical companies operating in China and India\n",
    "Overview of M&A Activity in Pharma\n",
    "\n",
    "Recent mergers and acquisitions in the pharmaceutical industry\n",
    "Impact of M&A on the competitive landscape\n",
    "Regulatory considerations and challenges in M&A activity\n",
    "Status of Biopharma\n",
    "\n",
    "Definition and significance of biopharmaceuticals\n",
    "Growth and market trends in the biopharma sector\n",
    "Regulatory and manufacturing challenges in biopharmaceutical development\n",
    "Biggest Biopharma Players\n",
    "\n",
    "Profiles of leading biopharmaceutical companies\n",
    "Their contributions to the industry and global healthcare\n",
    "Strategies for success and future outlook\n",
    "Biopharma Clusters in Europe and Globally\n",
    "\n",
    "Overview of major biopharma clusters in Europe\n",
    "Comparison with biopharma clusters in other regions\n",
    "Factors contributing to the growth of biopharma clusters\n",
    "Trends in Clinical Trials\n",
    "\n",
    "Evolution of clinical trial methodologies\n",
    "Adoption of virtual and decentralized clinical trials\n",
    "Impact of real-world evidence on clinical research\n",
    "The Impact of AI on Pharma\n",
    "\n",
    "Applications of artificial intelligence in drug discovery and development\n",
    "AI-driven approaches to personalized medicine\n",
    "Regulatory and ethical considerations in AI adoption in pharma\n",
    "This outline covers the main chapters and subtopics you requested. Let me know if you'd like to add more details or if there's anything else you'd like to include!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error occurred while invoking function WriteSection: Error occurred while invoking function WriteSection: (\"<class \\'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion\\'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {\\'error\\': {\\'code\\': \\'429\\', \\'message\\': \\'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-05-15 have exceeded token rate limit of your current AIServices S0 pricing tier. Please retry after 3 seconds. Please contact Azure support service if you would like to further increase the default rate limit.\\'}}\"))\\nError occurred while invoking function: \\'WriterPlugin.WriteSection\\''"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Tell me a joke about NVDA and TESLA stock prices.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm not aware of any specific jokes about NVDA and TESLA stock prices. However, I can create a simple financial joke for you. Here it is:\n",
      "\n",
      "Why don't stock prices ever go to the doctor?\n",
      "Because they prefer to avoid the \"dow\"!\n",
      "\n",
      "If you'd like a specific joke related to NVDA and TESLA stock prices, I can help you create one based on their performance or any other specific details you have in mind. Just let me know!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m UserProxyAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_execution_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Start the chat\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me a joke about NVDA and TESLA stock prices.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1007\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1006\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[0;32m   1009\u001b[0m     summary_method,\n\u001b[0;32m   1010\u001b[0m     summary_args,\n\u001b[0;32m   1011\u001b[0m     recipient,\n\u001b[0;32m   1012\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:645\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    643\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 645\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:810\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    808\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:645\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    643\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 645\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m     )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:808\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1949\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[1;32m-> 1949\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m   1951\u001b[0m         log_event(\n\u001b[0;32m   1952\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1957\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[0;32m   1958\u001b[0m         )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1701\u001b[0m, in \u001b[0;36mConversableAgent.check_termination_and_human_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m   1699\u001b[0m sender_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe sender\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sender \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sender\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuman_input_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALWAYS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1701\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_human_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProvide feedback to \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msender_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m. Press enter to skip and use auto-reply, or type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to end the conversation: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m     no_human_input_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO HUMAN INPUT RECEIVED.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reply \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1705\u001b[0m     \u001b[38;5;66;03m# if the human input is empty, and the message is a termination message, then we will terminate the conversation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2075\u001b[0m, in \u001b[0;36mConversableAgent.get_human_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get human input.\u001b[39;00m\n\u001b[0;32m   2064\u001b[0m \n\u001b[0;32m   2065\u001b[0m \u001b[38;5;124;03mOverride this method to customize the way to get human input.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03m    str: human input.\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2073\u001b[0m iostream \u001b[38;5;241m=\u001b[39m IOStream\u001b[38;5;241m.\u001b[39mget_default()\n\u001b[1;32m-> 2075\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[43miostream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_human_input\u001b[38;5;241m.\u001b[39mappend(reply)\n\u001b[0;32m   2077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\io\\console.py:37\u001b[0m, in \u001b[0;36mIOConsole.input\u001b[1;34m(self, prompt, password)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m password:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m getpass\u001b[38;5;241m.\u001b[39mgetpass(prompt \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassword: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "config_list = {\n",
    "\"model\": \"gpt-35-turbo\",\n",
    "\"azure_endpoint\": \"https://lucasaiservices.openai.azure.com/\",\n",
    "\"api_type\": \"azure\",\n",
    "\"api_version\": \"2024-02-15-preview\",\n",
    "}\n",
    "\n",
    "llm_config = config_list\n",
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n",
    "\n",
    "# Start the chat\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"Tell me a joke about NVDA and TESLA stock prices.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "DockerException",
     "evalue": "Error while fetching server API version: (2, 'CreateFile', 'The system cannot find the file specified.')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\api\\client.py:213\u001b[0m, in \u001b[0;36mAPIClient._retrieve_server_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApiVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\api\\daemon.py:181\u001b[0m, in \u001b[0;36mDaemonApiMixin.version\u001b[1;34m(self, api_version)\u001b[0m\n\u001b[0;32m    180\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/version\u001b[39m\u001b[38;5;124m\"\u001b[39m, versioned_api\u001b[38;5;241m=\u001b[39mapi_version)\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\utils\\decorators.py:44\u001b[0m, in \u001b[0;36mupdate_headers.<locals>.inner\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_general_configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHttpHeaders\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\api\\client.py:236\u001b[0m, in \u001b[0;36mAPIClient._get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;129m@update_headers\u001b[39m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_request_timeout(kwargs))\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    794\u001b[0m     conn,\n\u001b[0;32m    795\u001b[0m     method,\n\u001b[0;32m    796\u001b[0m     url,\n\u001b[0;32m    797\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    798\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    799\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    800\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    801\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    802\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    803\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    804\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\urllib3\\connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\urllib3\\connection.py:400\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\transport\\npipeconn.py:25\u001b[0m, in \u001b[0;36mNpipeHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m---> 25\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpipe_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\transport\\npipesocket.py:25\u001b[0m, in \u001b[0;36mcheck_closed.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCan not reuse socket after connection was closed.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\transport\\npipesocket.py:76\u001b[0m, in \u001b[0;36mNpipeSocket.connect\u001b[1;34m(self, address, retry_count)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect(address, retry_count)\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m win32pipe\u001b[38;5;241m.\u001b[39mGetNamedPipeInfo(handle)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\transport\\npipesocket.py:54\u001b[0m, in \u001b[0;36mNpipeSocket.connect\u001b[1;34m(self, address, retry_count)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43mwin32file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin32file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERIC_READ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwin32file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERIC_WRITE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin32file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPEN_EXISTING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcSECURITY_ANONYMOUS\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcSECURITY_SQOS_PRESENT\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwin32file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFILE_FLAG_OVERLAPPED\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m win32pipe\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# See Remarks:\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# https://msdn.microsoft.com/en-us/library/aa365800.aspx\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: (2, 'CreateFile', 'The system cannot find the file specified.')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDockerException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m assistant \u001b[38;5;241m=\u001b[39m AssistantAgent(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list})\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# create a UserProxyAgent instance named \"user_proxy\" with code execution on docker.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m code_executor \u001b[38;5;241m=\u001b[39m \u001b[43mDockerCommandLineCodeExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m UserProxyAgent(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_execution_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecutor\u001b[39m\u001b[38;5;124m\"\u001b[39m: code_executor})\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\autogen\\coding\\docker_commandline_code_executor.py:88\u001b[0m, in \u001b[0;36mDockerCommandLineCodeExecutor.__init__\u001b[1;34m(self, image, container_name, timeout, work_dir, auto_remove, stop_container)\u001b[0m\n\u001b[0;32m     84\u001b[0m     work_dir \u001b[38;5;241m=\u001b[39m Path(work_dir)\n\u001b[0;32m     86\u001b[0m work_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 88\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mdocker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Check if the image exists\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\client.py:94\u001b[0m, in \u001b[0;36mDockerClient.from_env\u001b[1;34m(cls, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m version \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m use_ssh_client \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_ssh_client\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     95\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m     96\u001b[0m     max_pool_size\u001b[38;5;241m=\u001b[39mmax_pool_size,\n\u001b[0;32m     97\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m     98\u001b[0m     use_ssh_client\u001b[38;5;241m=\u001b[39muse_ssh_client,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_from_env(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    100\u001b[0m )\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\client.py:45\u001b[0m, in \u001b[0;36mDockerClient.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi \u001b[38;5;241m=\u001b[39m APIClient(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\api\\client.py:197\u001b[0m, in \u001b[0;36mAPIClient.__init__\u001b[1;34m(self, base_url, version, timeout, tls, user_agent, num_pools, credstore_env, use_ssh_client, max_pool_size)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# version detection needs to be after unix adapter mounting\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    194\u001b[0m                         version,\n\u001b[0;32m    195\u001b[0m                         \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    196\u001b[0m                         ) \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_server_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_version \u001b[38;5;241m=\u001b[39m version\n",
      "File \u001b[1;32mc:\\MiniConda3\\envs\\document_generator\\lib\\site-packages\\docker\\api\\client.py:220\u001b[0m, in \u001b[0;36mAPIClient._retrieve_server_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DockerException(\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid response from docker daemon: key \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApiVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is missing.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mke\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DockerException(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError while fetching server API version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    222\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mDockerException\u001b[0m: Error while fetching server API version: (2, 'CreateFile', 'The system cannot find the file specified.')"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from autogen.coding import DockerCommandLineCodeExecutor\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "\"model\": \"gpt-35-turbo\",\n",
    "\"azure_endpoint\": \"https://lucasaiservices.openai.azure.com/\",\n",
    "\"api_type\": \"azure\",\n",
    "\"api_version\": \"2024-02-15-preview\",\n",
    "}]\n",
    "\n",
    "# create an AssistantAgent instance named \"assistant\" with the LLM configuration.\n",
    "assistant = AssistantAgent(name=\"assistant\", llm_config={\"config_list\": config_list})\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\" with code execution on docker.\n",
    "code_executor = DockerCommandLineCodeExecutor()\n",
    "user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={\"executor\": code_executor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(\"test.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('   \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '1  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Project Title  Towards an Interoperable MARinE Knowledge GRAPH  \\n'\n",
      " 'Project Acronym  MareGraph  \\n'\n",
      " 'Grant Agreement No.  101100771   \\n'\n",
      " 'Start Date of Project  23/01/2022  \\n'\n",
      " 'Duration of Project  36 Months  \\n'\n",
      " ' \\n'\n",
      " 'D2.2 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " 'Work Package  WP 2, Process tuning, community building and dissemination  \\n'\n",
      " 'Lead Author (Org)  Lorenzo Vylders  (Digitaal Vlaanderen ) \\n'\n",
      " 'Contributing Author(s) \\n'\n",
      " '(Org)  Lucas Cornette  (Digitaal Vlaanderen ), Liesbeth V an der H aegen '\n",
      " '(Digitaal \\n'\n",
      " 'Vlaanderen), Laurens Vercauteren (Digitaal Vlaanderen)  \\n'\n",
      " 'Due Date  30.06.2024 \\n'\n",
      " 'Date  29.01.2024 \\n'\n",
      " 'Version  V1.0  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Dissemination Level  \\n'\n",
      " 'X PU: Public  \\n'\n",
      " ' PP: Restricted to other programme participants (including the '\n",
      " 'Commission)  \\n'\n",
      " ' RE: Restricted to a group specified by the consortium (including the '\n",
      " 'Commission)  \\n'\n",
      " ' CO: Confidential, only for members of the consortium (including the '\n",
      " 'Commission)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " 'Commented [A1]: General comment: pay attention to the \\n'\n",
      " 'formatting: there are changes in the font size and paragraph lines, \\n'\n",
      " 'there are tables outside the margin, most of the figures in yellow are \\n'\n",
      " 'not readable because of their resolution that can be improved.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '2 Versioning and contribution history  \\n'\n",
      " 'Version  Date  Author  Notes  \\n'\n",
      " '0.2 28.03.2024  Lorenzo Vylders (Digitaal Vlaanderen), \\n'\n",
      " 'Lucas Cornette (Digitaal Vlaanderen)  Revision after \\n'\n",
      " 'General Assembly \\n'\n",
      " '22 and 23  February \\n'\n",
      " '2024  \\n'\n",
      " '0.1 29.01.2024  Lorenzo Vylders (Digitaal Vlaanderen), \\n'\n",
      " 'Lucas Cornette (Digitaal Vlaanderen) , Raf \\n'\n",
      " 'Buyle (Digitaal Vlaanderen)  Copy for D2.2 + \\n'\n",
      " 'adding vision and \\n'\n",
      " 'making adjustments \\n'\n",
      " 'to process and \\n'\n",
      " 'method  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Disclaimer  \\n'\n",
      " 'This document contains information which is proprietary to the MareGraph '\n",
      " 'Consortium. Neither this \\n'\n",
      " 'document nor the information contained herein shall be used, duplicated or '\n",
      " 'communicated by any \\n'\n",
      " 'means to a third party, in whole or parts, except with the prior consent of '\n",
      " 'the MareGraph  Consortium.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '3 Table of Contents  \\n'\n",
      " 'D2.2 Guidelines on process and methodology for organisational '\n",
      " 'interoperability (Version 1)  .............  1 \\n'\n",
      " 'Versioning and contribution history  ................................ '\n",
      " '................................ ................................ .......  '\n",
      " '2 \\n'\n",
      " 'Table of Contents  ................................ '\n",
      " '................................ ................................ '\n",
      " '................................ .... 3 \\n'\n",
      " 'Terminology  ................................ '\n",
      " '................................ ................................ '\n",
      " '................................ ............  5 \\n'\n",
      " 'Executive Summary  ................................ '\n",
      " '................................ ................................ '\n",
      " '................................ . 7 \\n'\n",
      " '1 About  ................................ ................................ '\n",
      " '................................ ................................ '\n",
      " '...............  8 \\n'\n",
      " '2 Retrospective at D2.1  ................................ '\n",
      " '................................ ................................ '\n",
      " '......................  9 \\n'\n",
      " 'What did we do in D2.1?  ................................ '\n",
      " '................................ ................................ .... 9 \\n'\n",
      " 'Identified challenges  ................................ '\n",
      " '................................ ................................ '\n",
      " '...........  9 \\n'\n",
      " '3 Vision for co -creation of ontologies  ................................ '\n",
      " '................................ ..............................  12 \\n'\n",
      " 'Inspiration  ................................ '\n",
      " '................................ ................................ '\n",
      " '......................  12 \\n'\n",
      " 'Adopt  ................................ ................................ '\n",
      " '................................ ..............................  12 \\n'\n",
      " 'Contribute  ................................ '\n",
      " '................................ ................................ '\n",
      " '......................  12 \\n'\n",
      " 'Conclusion  ................................ '\n",
      " '................................ ................................ '\n",
      " '......................  13 \\n'\n",
      " '3 Guidelines in creating ontologies  ................................ '\n",
      " '................................ ................................ .. 14 \\n'\n",
      " 'Introduction  ................................ '\n",
      " '................................ ................................ '\n",
      " '......................  16 \\n'\n",
      " 'Context  ................................ ................................ '\n",
      " '................................ ..............................  14 \\n'\n",
      " 'Scope  ................................ ................................ '\n",
      " '................................ ................................ . 16 \\n'\n",
      " 'Principles  ................................ '\n",
      " '................................ ................................ '\n",
      " '...........................  18 \\n'\n",
      " 'One Pager Process and Method  ................................ ..........  '\n",
      " 'Error! Bookmark not defined.  \\n'\n",
      " 'Process  ................................ ................................ '\n",
      " '................................ ..............................  18 \\n'\n",
      " 'Actors and responsibilities  ................................ '\n",
      " '................................ ................................  19 \\n'\n",
      " 'Announce an ontology  ................................ '\n",
      " '................................ ................................ ...... '\n",
      " '20 \\n'\n",
      " 'Step 1.  Develop and communicate a declaration of intent that describes the '\n",
      " 'scope \\n'\n",
      " 'of the to -be-developed ontology  ................................ '\n",
      " '................................ ...................  21 \\n'\n",
      " 'Step 2.  Invite relevant and interested business stakeholders to a workshop '\n",
      " 'meeting \\n'\n",
      " 'to identify pr ocesses and use cases  ................................ '\n",
      " '................................ ...............  22 \\n'\n",
      " 'Step 3.  Further develop declaration of intent into a Working Group Charter '\n",
      " 'by \\n'\n",
      " 'adding requirements and conditions based on input from the business  '\n",
      " '.....................  22 \\n'\n",
      " 'Step 4.  Present the Working Group Charter to the Peer Review Community '\n",
      " 'for \\n'\n",
      " 'approval for starting a thematic working group  '\n",
      " '................................ ............................  22 \\n'\n",
      " 'Announce an ontology  ................................ '\n",
      " '................................ ................................ ...... '\n",
      " '23 \\n'\n",
      " 'Step 5.  Set up the working group and  environment  '\n",
      " '................................ ...............  23 \\n'\n",
      " 'Step 6.  Creating an initial draft  ................................ '\n",
      " '................................ ................  23 \\n'\n",
      " 'Step 7.  Organizing the working groups  ................................ '\n",
      " '................................ .... 24 \\n'\n",
      " 'Step 8.  Elaborate interim draft specification  '\n",
      " '................................ ...........................  24 \\n'\n",
      " 'Step 9.  Mid-term evaluation by the Peer Review Community  '\n",
      " '................................  24 \\n'\n",
      " 'Step 10.  Organizing a public review  ................................ '\n",
      " '................................ ...........  24 \\n'\n",
      " 'Step 11.  Finalizing the specification  ................................ '\n",
      " '................................ ...........  25 \\n'\n",
      " 'Step 12.  Quality control by the Peer Review Community  '\n",
      " '................................ .........  25 \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '4 Step 13.  Assessing and confirm agreements made  '\n",
      " '................................ ...................  25 \\n'\n",
      " 'Publicatio n ................................ '\n",
      " '................................ ................................ '\n",
      " '.........................  25 \\n'\n",
      " 'Step 14.  Publishing specification in both human and machine readable format '\n",
      " '...... 26 \\n'\n",
      " 'Step 15.  Publishing reusable elements that project teams can make use of  '\n",
      " '............  26 \\n'\n",
      " 'Step 16.  Publishing a conformity test suite  '\n",
      " '................................ ...............................  26 \\n'\n",
      " 'Change Management  ................................ '\n",
      " '................................ ................................ ........  '\n",
      " '26 \\n'\n",
      " 'Step 17.  Receiving feedback  ................................ '\n",
      " '................................ ......................  27 \\n'\n",
      " 'Step 18.  Processing changes  ................................ '\n",
      " '................................ ......................  28 \\n'\n",
      " 'Step 19.  Publication of a new version  ................................ '\n",
      " '................................ ........  28 \\n'\n",
      " 'Phasing out an ontology  ................................ '\n",
      " '................................ ................................ .... 29 \\n'\n",
      " 'Step 20.  Proposal for phasing out an ontology '\n",
      " '................................ ..........................  29 \\n'\n",
      " 'Step 21.  Assessment of the proposal, announcement and implementation of a '\n",
      " 'public \\n'\n",
      " 'review period  ................................ '\n",
      " '................................ ................................ '\n",
      " '.................  29 \\n'\n",
      " 'Step 22.  Ratification of the decision to phase out an ontology  '\n",
      " '................................  29 \\n'\n",
      " 'Step 23.  Publication of the phased -out ontology  '\n",
      " '................................ ......................  30 \\n'\n",
      " 'Method  ................................ ................................ '\n",
      " '................................ .............................  30 \\n'\n",
      " 'Setting up a working group charter  ................................ '\n",
      " '................................ ..................  30 \\n'\n",
      " 'Organising and facilitating working group meetings  '\n",
      " '................................ ........................  31 \\n'\n",
      " 'Developing a domain model  ................................ '\n",
      " '................................ .............................  32 \\n'\n",
      " 'Supporting transparency during development  ................................ '\n",
      " '................................ . 33 \\n'\n",
      " 'Generation of the data specification and documentation  '\n",
      " '................................ ...............  34 \\n'\n",
      " 'Toolchain  ................................ ................................ '\n",
      " '................................ ........................  37 \\n'\n",
      " 'Management of issues and errors  ................................ '\n",
      " '................................ ....................  36 \\n'\n",
      " 'Lifecycle of an ontology  ................................ '\n",
      " '................................ ................................ .... 36 \\n'\n",
      " 'Criteria for promotion to candidate ontology  '\n",
      " '................................ ................................ .. 37 \\n'\n",
      " 'Criteria for promotion to candidate ontology  '\n",
      " '................................ ................................ .. 37 \\n'\n",
      " 'Working group on data standards  ................................ .......  '\n",
      " 'Error! Bookmark not defined.  \\n'\n",
      " 'Context  ................................ ................................ '\n",
      " '................................ ..............................  39 \\n'\n",
      " 'Order description, composition, and responsibiliti es '\n",
      " '................................ .......................  39 \\n'\n",
      " 'Reporting  ................................ ................................ '\n",
      " '................................ ...........................  40 \\n'\n",
      " 'OSLO Toolchain  ................................ '\n",
      " '................................ ................................ '\n",
      " '.................  41 \\n'\n",
      " 'Linked Open Vocabularies (LOV)  ................................ '\n",
      " '................................ ......................  41 \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '5 Terminology  \\n'\n",
      " ' \\n'\n",
      " 'Terminology/Acronym  Description  \\n'\n",
      " 'Application profile   Describes which specifications can be used for '\n",
      " 'certain \\n'\n",
      " 'appli obscations of a n ontology . The ontology is taken as a basis \\n'\n",
      " 'and supplemented with context -specific additions. Examples are \\n'\n",
      " 'the application profiles developed within the OSLO² project or \\n'\n",
      " 'DCAT -AP for exchange of data between Open data portals.   \\n'\n",
      " 'Domain model   A domain model is a conceptual model of a certain domain '\n",
      " 'that \\n'\n",
      " 'represents  both behavior and data. It is a formal representatio n \\n'\n",
      " 'of knowledge domain with entities, relationships, data types, etc.   \\n'\n",
      " 'Controlled vocabulary  Umbrella term for code lists, taxonomies and '\n",
      " 'thesauri, among \\n'\n",
      " 'others. Controlled vocabularies, on the other hand, are used for \\n'\n",
      " 'filling in specific data attributes with standardized values. An \\n'\n",
      " 'example of a controlled vocabulary is the Language Named \\n'\n",
      " 'Authorit y List of the Publications Office of the European Union \\n'\n",
      " '(OP).  \\n'\n",
      " 'High level domain model   A high level domain model describes the relevant '\n",
      " 'entities in a \\n'\n",
      " 'domain with a high level of abstraction. It is the result of a first \\n'\n",
      " 'step towards the formalization of d omain knowledge and the \\n'\n",
      " 'analysis of information needs.   \\n'\n",
      " 'Declaration of Intent   A declaration of intent describes the domain and '\n",
      " 'purpose of the \\n'\n",
      " 'ontology  to be developed and is communicated to various \\n'\n",
      " 'relevant stakeholders at the start of the process.   \\n'\n",
      " 'JSON-LD  JavaScript Object Notation  for Linked Data is a way to represent \\n'\n",
      " 'Linked Data  in JSON.   \\n'\n",
      " 'SHACL   Shapes Constraint Language is a way to describe and validate \\n'\n",
      " 'data graphs (in RDF).  \\n'\n",
      " 'Specification   A specification is a technical document that gives substance '\n",
      " 'to \\n'\n",
      " 'the ontology . Specifications can be adjusted based on advancing \\n'\n",
      " 'insight without changing the corresponding ontology .  \\n'\n",
      " 'Ontology  An ontology is a conceptual framework, developed through \\n'\n",
      " 'collaborative efforts and consensus among various interested \\n'\n",
      " 'parties and stakeholders, that provides a structured \\n'\n",
      " 'representation of knowledge and defines the relationships \\n'\n",
      " 'between different entities. It is designed to describe a consistent \\n'\n",
      " 'and reproducible  way of organizing and categorizing information, \\n'\n",
      " 'enabling the classification, retrieval, and interpretation of data \\n'\n",
      " 'within a specific domain or subject area.    \\n'\n",
      " 'UML class diagram   A static diagram that describes the structure of a '\n",
      " 'system based \\n'\n",
      " 'on classes, attributes, relationships, and operations.   \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '6 Terminology/Acronym  Description  \\n'\n",
      " 'Graffoo  Graffoo is a visualization technique used in drafting ontologies, \\n'\n",
      " 'providing a concise and graphical representation of complex \\n'\n",
      " 'concepts, relationships, and hierarchies. It helps ontology \\n'\n",
      " 'designers to gain  a better understanding of their models and \\n'\n",
      " 'facilitates communication and collaboration among stakeholders \\n'\n",
      " 'in the ontology development process.  \\n'\n",
      " 'Vocabulary  \\n'\n",
      " ' Describes a shared conceptual framework for certain concepts \\n'\n",
      " 'with a focus on data exchange  \\n'\n",
      " 'Workin g Group Charter   Documents the expectations on the outcomes of the '\n",
      " 'work of the \\n'\n",
      " 'thematic working group and describes how the working group \\n'\n",
      " 'will be organized. This document serves as a starting point for \\n'\n",
      " 'starting the development process.   \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '7 Executive Summary  \\n'\n",
      " 'This executive summary provides an overview of the guidelines on process and '\n",
      " 'methodology \\n'\n",
      " 'for interoperability, focusing on the creation of knowledge graphs and the '\n",
      " 'management of \\n'\n",
      " 'related semantic assets. The guidelines are based on the OSLO framework and  '\n",
      " 'are the first \\n'\n",
      " 'step towards establish ing collaborative governance between Belgium and '\n",
      " 'Italy.  The \\n'\n",
      " 'document outlines the processes and guidelines necessary for achieving '\n",
      " 'organisational \\n'\n",
      " 'interoperability in the creation of ontologie s, from the establishment  of '\n",
      " 'a formal governance \\n'\n",
      " 'structure to community building and the development of a semantic '\n",
      " 'specification for \\n'\n",
      " 'publication.  \\n'\n",
      " ' \\n'\n",
      " 'The guidelines emphasize the importance of collaborative governance in '\n",
      " 'promoting \\n'\n",
      " 'interoperability between organizations. By establishi ng a formal governance '\n",
      " 'structure, \\n'\n",
      " 'stakeholders from both Belgium and Italy can actively participate in '\n",
      " 'decision -making \\n'\n",
      " 'processes and contribute their expertise to ensure the effective development '\n",
      " 'and \\n'\n",
      " 'maintenance of knowledge graphs and semantic assets. Com munity building '\n",
      " 'also plays a \\n'\n",
      " 'crucial role in the successful implementation of the guidelines. The '\n",
      " 'document highlights the \\n'\n",
      " 'need to engage relevant stakeholders, including domain experts, data '\n",
      " 'custodians, and \\n'\n",
      " 'technology providers, in the development and adop tion of ontologies .  \\n'\n",
      " ' \\n'\n",
      " 'The document concludes by identifying challenges that need to be addressed '\n",
      " 'to fully realize \\n'\n",
      " 'the goals outlined in the guidelines. These challenges , at the end of this '\n",
      " 'deliverable, serve as \\n'\n",
      " 'a roadmap for future actions  towards D2.2 , including overcoming technical '\n",
      " 'challenges, \\n'\n",
      " 'addressing cultural and organizational barriers, and promoting awareness and '\n",
      " 'understanding \\n'\n",
      " 'of the benefits of organisational interoperability.  The goal of this '\n",
      " 'deliverable is identifying the \\n'\n",
      " 'challenges that arise wh en trying to establish a collaborative governance '\n",
      " 'between Belgium \\n'\n",
      " 'and Italy, while the next deliverable will propose sol utions on how to '\n",
      " 'solve them.  \\n'\n",
      " ' \\n'\n",
      " 'In summary, the guidelines on process and methodology for organisational '\n",
      " 'interoperability \\n'\n",
      " 'presented in this document provide a comprehensive framework for creating '\n",
      " 'knowledge \\n'\n",
      " 'graphs, including the management of the related semantic assets . By '\n",
      " 'following thes e \\n'\n",
      " 'guidelines, organizations can enhance data interoperability, promote cross '\n",
      " '-border \\n'\n",
      " 'cooperation, and contribute to the advancement of knowledge and '\n",
      " 'innovation.    Commented [A2]: To be adapted for D2.2  \\n'\n",
      " 'Commented [A3]: I think we should make clear that this \\n'\n",
      " 'deliverable is simply OSLO with some challenges at the end we \\n'\n",
      " 'identified in the first 6 months of the project.. We cannot say that \\n'\n",
      " 'this document represen ts the guidelines for achieving organisational \\n'\n",
      " 'interoperability for Belgium and Italy simply because, as said in \\n'\n",
      " 'different WP2 meetings,  this type of process is currently not really \\n'\n",
      " 'applied in Italy. There is a variant of this process that is not reported  \\n'\n",
      " 'in this deliverable that we apply in Italy. So currently this document \\n'\n",
      " 'does not represent the Italian situation. I think we can say this, \\n'\n",
      " 'targeting the goal of WP2 in the upcoming deliverable.  \\n'\n",
      " 'Commented [A4R3]: I have rewritten the text a bit so that it is \\n'\n",
      " 'clearer that th is is just the oslo process and methodology with \\n'\n",
      " 'identified challenges towards a collaborative governance for italy \\n'\n",
      " 'and belgium  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '8 1 About  \\n'\n",
      " 'MareGraph aims to bring together and collaborate with highly diverse '\n",
      " 'national and region al \\n'\n",
      " 'Research Infrastructure (RIs) in Europe. To maximize the impact of MareGraph '\n",
      " 'and its results, \\n'\n",
      " 'the project implements effective communication, dissemination and knowledge '\n",
      " 'transfer \\n'\n",
      " 'methodology and strategies. Having a defined process and methodology for o '\n",
      " 'rganizational \\n'\n",
      " 'interoperability is crucial as it ensures seamless communication, data '\n",
      " 'exchange, and \\n'\n",
      " 'collaboration between different departments, systems, and organizations, '\n",
      " 'thereby enhancing \\n'\n",
      " 'efficiency, reducing errors, and enabling effective decision -makin g. \\n'\n",
      " 'This document stems from the ICEG process and methodology, which in turn was '\n",
      " 'the English \\n'\n",
      " 'translation of the OSLO process and methodology. The process focuses on the '\n",
      " 'harmonization \\n'\n",
      " 'and alignment of initiatives with the goal of developing ontologies  across  '\n",
      " 'borders in the EU.  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '9 2 Retrospective at D2.1  \\n'\n",
      " 'What did we do in D2.1?    \\n'\n",
      " 'For the D2.1 deliverable of the MareGraph project, a  first version of a '\n",
      " 'governance framework  \\n'\n",
      " 'was made  that would enable effective collaborative decision -making and '\n",
      " 'encourage the \\n'\n",
      " 'development of a stakeholder community integral to the success and broad '\n",
      " 'adoption and \\n'\n",
      " 'reuse of ontologies. This deliver able was characterized by applying the '\n",
      " 'OSLO Process and \\n'\n",
      " 'Meth od and assisting our partner, CNR, in going through th is process  of '\n",
      " 'creating  semantic \\n'\n",
      " 'specification s within  the MareGraph project . By doing this we aim to  '\n",
      " 'establish guidelines on \\n'\n",
      " 'ontology creation to reach a  widespread consensus and promote '\n",
      " 'interoperability between \\n'\n",
      " 'member states in the European Union .    \\n'\n",
      " ' \\n'\n",
      " 'To promote transparency and ensure clarity in our approach, extensive '\n",
      " 'documentation was \\n'\n",
      " 'prepared, outlining the methodologies and processes used during this period  '\n",
      " 'as well as the \\n'\n",
      " 'related technical and organizational challenges identified . These findings '\n",
      " 'from D2.1 are now \\n'\n",
      " 'shaping the direction of the subsequent D2.2 phase, as we attempt to outline '\n",
      " 'and address \\n'\n",
      " 'the challenges identified.  \\n'\n",
      " '   \\n'\n",
      " 'The establ ishment of guidelines on process and methodology for '\n",
      " 'organizational \\n'\n",
      " 'interoperability has been a hallmark of D2.1, providing a framework for '\n",
      " 'organizations to \\n'\n",
      " 'improve data interoperability and support cross -border cooperation. During '\n",
      " 'the phase, \\n'\n",
      " 'several key c hallenges were pinpointed , such as technical constraints and '\n",
      " 'organizational \\n'\n",
      " 'hurdles, and the need to elevate awareness of the benefits of '\n",
      " 'interoperability.   \\n'\n",
      " '    \\n'\n",
      " 'Building upon the groundwork established in D2.1, the MareGraph project has '\n",
      " 'developed a \\n'\n",
      " 'roadma p for future actions, clearly identifying the specific challenges to '\n",
      " 'be addressed in the \\n'\n",
      " \"D2.2 phase. This roadmap is critical in steering the project's forward \"\n",
      " 'momentum, ensuring \\n'\n",
      " 'that all forthcoming efforts are concentrated and effective in addressing '\n",
      " 'the  challenges laid \\n'\n",
      " 'out. With this structured approach and the guidelines in place, the '\n",
      " 'MareGraph project is \\n'\n",
      " 'strategically positioned to address the complexities of organizational '\n",
      " 'interoperability, \\n'\n",
      " 'advancing towards its ultimate aim of enabling seamless data exchange and '\n",
      " 'collaboration \\n'\n",
      " 'across European borders.  \\n'\n",
      " ' \\n'\n",
      " 'Identified challenges  \\n'\n",
      " 'The collaborative initiative between Belgium and Italy within MareGraph is '\n",
      " 'dedicated to \\n'\n",
      " 'establishing a unified methodology rooted in the OSLO framework, with the '\n",
      " 'goal of creating \\n'\n",
      " 'knowledge graphs including the management of the related semantic assets. '\n",
      " 'The focus lies \\n'\n",
      " 'on defining processes and guidelines to achieve organizational '\n",
      " 'interoperability in ontologies. \\n'\n",
      " 'Throughout this process, various challenges have been encountered. During '\n",
      " 'deliverable D2.1, \\n'\n",
      " 'the following challenges were identified regarding the OSLO Process and '\n",
      " 'Methodology.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '10 Topic   Description  \\n'\n",
      " 'Working group on data \\n'\n",
      " 'standards  The establishment of a working group on data standards \\n'\n",
      " 'plays a crucial role in the central coordination and \\n'\n",
      " 'oversight of information standardization efforts. \\n'\n",
      " 'Currently, there still is a need to find a European -level \\n'\n",
      " 'solution for this body, which ideally comprises experts \\n'\n",
      " 'from various member states.  \\n'\n",
      " 'Endorsement group  Ratification of the endorsed ontologies should be '\n",
      " 'carried \\n'\n",
      " 'out by a designated body in the member state where the \\n'\n",
      " 'contracting party originates from. However, the challenge \\n'\n",
      " 'lies in verifying the existence of a relevant party or body \\n'\n",
      " 'within each member state that can fulfill this endorsement \\n'\n",
      " 'role. It is important to note that, to the best of our \\n'\n",
      " 'knowledge, there is currently no equivalent European -\\n'\n",
      " 'level body which can be responsible for endorsing \\n'\n",
      " 'ontologies.  \\n'\n",
      " 'OSLO process  In the process of developing ontologie s, OSLO aims to \\n'\n",
      " 'reach a point where the ontology can be registered in their \\n'\n",
      " 'own standards registry. However, determining the \\n'\n",
      " 'appropriate timing to initiate this inclusion and addressing \\n'\n",
      " 'any discrepancies that may arise, such as variations in the \\n'\n",
      " 'developmen t process and tooling or the required use of \\n'\n",
      " 'UML models for OSLO, become crucial considerations.  \\n'\n",
      " 'Scalability  Scalability is a key objective in developing a process and \\n'\n",
      " 'methodology for ontologies that can be effectively reused \\n'\n",
      " 'by other member states.  Addressing this challenge is \\n'\n",
      " 'closely intertwined with the issues surrounding the \\n'\n",
      " 'working group on data standards and the endorsement \\n'\n",
      " 'group. The ultimate goal is to establish a document that is \\n'\n",
      " 'as generic as possible, allowing for easy adoption by other \\n'\n",
      " 'member states.  \\n'\n",
      " 'Publication of the ontology \\n'\n",
      " 'and documentation  The challenge of where to publish the ontology and the \\n'\n",
      " 'accompanying documentation is closely linked to both the \\n'\n",
      " \"'OSLO Process' and 'Scalability' topics mentioned above. \\n\"\n",
      " 'To overcome this challenge,  it is crucial to establish a clear \\n'\n",
      " 'process for determining the appropriate platform or Commented [A5]: I do not '\n",
      " 'think this is a problem of naming but \\n'\n",
      " 'a problem of the overall development of the process.  \\n'\n",
      " 'Commented [A6R5]: I agree, I changed it to process  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '11 repository for publication. Avoiding scattered repositories \\n'\n",
      " 'and ensuring easy accessibility and discoverability of the \\n'\n",
      " 'models and documentation are key objectives. By d efining \\n'\n",
      " 'clear guidelines and identifying a centralized and easily \\n'\n",
      " 'accessible platform for publication (e.g. LOV), \\n'\n",
      " 'stakeholders can locate and access the different standards \\n'\n",
      " 'and ontologies and their documentation more efficiently.  \\n'\n",
      " 'Maintenance of recognized \\n'\n",
      " 'ontologies  Maintenance of recognized ontologies is essential once \\n'\n",
      " 'they have been adopted. Establishing the right \\n'\n",
      " 'responsibilities for conducting regular reviews, engaging \\n'\n",
      " 'stakeholders when needed, documenting changes, and \\n'\n",
      " 'fostering user feedback  are key aspects of ensuring the \\n'\n",
      " 'ongoing relevance and effectiveness of recognized \\n'\n",
      " 'ontologies.  \\n'\n",
      " ' \\n'\n",
      " 'Moving forward  \\n'\n",
      " 'In D2.2 we aim to further improve the guidelines set out below to promote '\n",
      " 'collaboration in \\n'\n",
      " 'ontology creation throughout the European Union , doing this by further '\n",
      " 'addressing the \\n'\n",
      " 'identified challenges and where possible coming up  with proposed solutions. '\n",
      " 'One part of the \\n'\n",
      " 'proposed solution is a vision for collaboration and a shared repository on '\n",
      " 'European level, this \\n'\n",
      " 'vision is  described in the se ction below, ‘ 3 Vision for co -creation of '\n",
      " 'ontologies ’. \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '12 3 Vision for co -creation of ontologies  \\n'\n",
      " 'Given the obstacles discussed in ‘ Retrospective at D2.1 ’, striving towards '\n",
      " 'a vision where \\n'\n",
      " 'collaboration is promoted instead of a shared governance is favorable given '\n",
      " 'the diverse \\n'\n",
      " 'needs,  political landscape, and way of working in public administrations of '\n",
      " 'different European \\n'\n",
      " 'countries. The focus should therefore be on identifying common goals, share '\n",
      " 'best practices \\n'\n",
      " 'and develop flexible solutions that can be adapted to individual contexts  '\n",
      " 'while maintaining a \\n'\n",
      " 'transparent process to promote reuse and credibility.  \\n'\n",
      " ' \\n'\n",
      " 'To establish common standards, we first need a platform where standards, '\n",
      " 'vocabularies, \\n'\n",
      " 'datasets, best practices, and solutions can be shared and accessed by all '\n",
      " 'member states that \\n'\n",
      " 'are part of the project. For this, we propose using LOV as a repository and '\n",
      " 'Joinup  for the \\n'\n",
      " 'community .  \\n'\n",
      " ' \\n'\n",
      " \"Joinup , the collaborative platform established by the European Commission's \"\n",
      " 'Interoperability \\n'\n",
      " 'Solutions for Public Administrations Programme (ISA Programme), plays a '\n",
      " 'crucial role in \\n'\n",
      " 'enabling the sharing of vocabularies and application profiles. To further '\n",
      " 'enhance  semantic \\n'\n",
      " 'interoperability, a significant advancement could involve the development of '\n",
      " 'a framework \\n'\n",
      " 'focused on leveraging semantic assets for inspiration, adoption, and '\n",
      " 'contribution. This section \\n'\n",
      " 'explores the potential of such a framework and its key compon ents.  \\n'\n",
      " ' \\n'\n",
      " 'Inspiration  \\n'\n",
      " 'To promote the reuse of existing assets and assess their quality, an EU '\n",
      " '-wide registry for \\n'\n",
      " 'vocabularies and application profiles could be established. This registry '\n",
      " 'would provide a \\n'\n",
      " 'comprehensive overview of available resources, allowing f or immediate '\n",
      " 'assessment. A \\n'\n",
      " 'valuable resource in this regard is the Linked Open Vocabularies (LOV) '\n",
      " 'platform, which offers \\n'\n",
      " \"a visual interface to gauge the extent of reuse and the vocabulary's \"\n",
      " 'existing employment in \\n'\n",
      " 'other contexts  and explore a compilation of  vocabularies published using '\n",
      " 'open standards, \\n'\n",
      " 'with interconnections to other web resources.  \\n'\n",
      " ' \\n'\n",
      " 'Adopt  \\n'\n",
      " 'To facilitate the smooth contextualization of vocabularies within local '\n",
      " 'settings  and promote \\n'\n",
      " 'a decentralized approach in ontology creation , a standardized process for '\n",
      " 'collaboration and \\n'\n",
      " 'a method for modelling  and dissemination could be developed. This '\n",
      " 'collaborative workbench \\n'\n",
      " 'would provide a structured environment for stakeholders to co ntribute their '\n",
      " 'expertise and \\n'\n",
      " 'align the vocabulary with specific requirements. By following a s hared '\n",
      " 'process and method \\n'\n",
      " 'based on best practices, see ‘Guidelines in creating ontologies’ , the '\n",
      " 'adoption of vocabularies \\n'\n",
      " 'becomes more efficient and consistent across different contexts.  \\n'\n",
      " ' \\n'\n",
      " 'Contribute  \\n'\n",
      " 'Building upon the established process and method, the framework should '\n",
      " 'include an \\n'\n",
      " 'automated mechanism to publish the finalized vocabulary in the standard '\n",
      " 'registry on Joinup. \\n'\n",
      " 'This ensures that the vocabulary becomes readily available for others to '\n",
      " 'engage with and \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '13 apply effectively. By streamlining the publication process, the framework '\n",
      " 'encourages active \\n'\n",
      " 'participation and contribution from a wider community of stakeholders.  \\n'\n",
      " ' \\n'\n",
      " 'Conclusion  \\n'\n",
      " 'The proposed framework for collaborative vocabulary sharing on Joinup '\n",
      " 'presents a significant \\n'\n",
      " 'opportunity to advance semantic interoperability. By inspiring, adopting, '\n",
      " 'and contributing to \\n'\n",
      " 'vocabularies and application profiles, public administrations can enhance '\n",
      " 'their ability to \\n'\n",
      " 'communicate and exchange data effectively. The establishment of an EU -wide '\n",
      " 'registry, a \\n'\n",
      " 'collaborative workbench, and an automated publication process would provide '\n",
      " 'th e necessary \\n'\n",
      " 'infrastructure to support this framework. Ultimately, this initiative would '\n",
      " 'contribute to the \\n'\n",
      " 'harmonization of semantic assets and foster greater interoperability across '\n",
      " 'European public \\n'\n",
      " 'administrations.  \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '14 4 Guidelines in creating ontologies  \\n'\n",
      " 'Overv iew \\n'\n",
      " 'In th e first section , we aim to give an overview of what the process and '\n",
      " 'methodology looks \\n'\n",
      " 'like before exploring these topics  deeper in the subsequent sections . For '\n",
      " 'applying the vision \\n'\n",
      " 'through the process and method for ontology creation we have defined 4 high '\n",
      " 'level steps , \\n'\n",
      " 'based on the paper of Raf Buyle ‘ Raising semantic and technical '\n",
      " 'interoperability in the public \\n'\n",
      " 'sector ‘1. Our technique for improving interoperability incorporates the '\n",
      " 'procedure of \\n'\n",
      " 'achieving both technical and semantic consensus , along with an end -to-end '\n",
      " 'method that \\n'\n",
      " 'follows the principles of Linked Data. This method ensures the maintenance '\n",
      " 'of semantic \\n'\n",
      " 'agreements within a functional public sector context. When this approach is '\n",
      " 'applied across \\n'\n",
      " 'the different member states within the EU to the development of semantic '\n",
      " 'assets, we can \\n'\n",
      " 'collaborate and reuse more effectively as semantic assets have undergone a '\n",
      " 'process that \\n'\n",
      " 'includes  the necessary alignments with relevant stakeholders and sufficient '\n",
      " 'harmonization \\n'\n",
      " 'with other standards and onto logies.  The application of this approach can '\n",
      " 'be broken down \\n'\n",
      " 'into four steps:   \\n'\n",
      " '    \\n'\n",
      " '1. Establish a formal governance  locally : The standardization process '\n",
      " 'should be anchored \\n'\n",
      " 'at an existing governance body or initiate a new one. This step is critical '\n",
      " 'as it builds \\n'\n",
      " 'trust among various stakeholders and influences the adoption of data '\n",
      " 'standards. This \\n'\n",
      " 'governance body is described furt her in this document as the peer review '\n",
      " 'community, \\n'\n",
      " 'they consist  of experts and stakeholders tasked with reviewing and '\n",
      " 'providing \\n'\n",
      " 'constructive feedback on the development of ontologies, standards, and '\n",
      " 'semantic \\n'\n",
      " 'agreements.  \\n'\n",
      " ' \\n'\n",
      " '2. Create a clear process for achievin g semantic and technical agreements: '\n",
      " 'The process \\n'\n",
      " 'should define the roles of different actors and describe how a consensus can '\n",
      " 'be \\n'\n",
      " 'achieved among stakeholders. This process has been successfully implemented '\n",
      " 'and \\n'\n",
      " 'documented in Flanders and at the Belgian inter federal level.  This process '\n",
      " 'has also \\n'\n",
      " 'been used by CNR during the MareGraph project, during which the process '\n",
      " 'and \\n'\n",
      " 'method has been adapted to fit the different contexts within governments in '\n",
      " 'the EU.  \\n'\n",
      " ' \\n'\n",
      " '3. Implement an end -to-end method based on Linked Data principles: This '\n",
      " 'means that \\n'\n",
      " 'all decision records, discussions, and models should be publicly accessible, '\n",
      " 'with the \\n'\n",
      " 'latter documented using a formal language based on RDF. The method should '\n",
      " 'include \\n'\n",
      " 'an imple mentation framework that ensures the traceability and alignment '\n",
      " 'of \\n'\n",
      " 'semantic agreements to suit various stakeholders such as policy makers, '\n",
      " 'domain \\n'\n",
      " 'experts, analysts, and developers. This process has been applied and '\n",
      " 'documented in \\n'\n",
      " 'Flanders and at the Belgian  interfederal level.   \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '1 https://biblio.ugent.be/publication/8712631  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '15 4. Co-create data standards: The semantic agreements should be achieved '\n",
      " 'in open \\n'\n",
      " 'thematic working groups that include domain experts from the public sector, '\n",
      " 'private \\n'\n",
      " 'sector, and academia. These groups should follow the process and method  '\n",
      " 'within a \\n'\n",
      " 'formal governance framework.  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '16 Introduction  \\n'\n",
      " 'This section provides a  more in depth  description  of the guidelines for '\n",
      " 'ontology creation  as \\n'\n",
      " 'described in the overview above , drawing upon the best practices derived '\n",
      " 'from the OSLO \\n'\n",
      " 'Process and Method i n Flanders. These guidelines have been further refined '\n",
      " 'and adapted \\n'\n",
      " 'based on the valuable insights gained during the MareGraph project. By '\n",
      " 'following these \\n'\n",
      " 'guidelines, organizations can ensure a systematic and effective approach to '\n",
      " 'ontology \\n'\n",
      " 'development, fost ering interoperability and facilitating knowledge '\n",
      " 'sharing.  \\n'\n",
      " ' \\n'\n",
      " 'Context  \\n'\n",
      " 'Governments at local, regional, inter -federal and European level often have '\n",
      " 'to  cooperate in \\n'\n",
      " 'the context of their services. In practice, a great deal of data must '\n",
      " 'therefore be exchanged \\n'\n",
      " 'between the various administrations. This data comes from different systems, '\n",
      " 'may not be \\n'\n",
      " 'available in the same technical format, and does not necessar ily follow the '\n",
      " 'same semantics. \\n'\n",
      " 'High quality data exchange becomes extremely difficult without making '\n",
      " 'agreements. These \\n'\n",
      " 'agreements must be anchored as broadly as possible and, where relevant, lead '\n",
      " 'to a n ontology  \\n'\n",
      " \"with a voluntary, 'comply or explain' or mand atory nature, in order to \"\n",
      " 'avoid unnecessary costs \\n'\n",
      " 'for data exchange.  \\n'\n",
      " ' \\n'\n",
      " 'When ontologies  are developed by governments, it is important that the '\n",
      " 'goals of the various \\n'\n",
      " 'stakeholders are aligned, as well as inside the hierarchy of an '\n",
      " 'organization. All parties invo lved \\n'\n",
      " 'must be aware of the benefits entailed by effective and efficient use of the '\n",
      " 'ontologie s. The \\n'\n",
      " 'stakeholders must be convinced of the usefulness of the ontologies , '\n",
      " 'whether  it benefits them \\n'\n",
      " 'directly. The development process set out in this document is bas ed on '\n",
      " 'international \\n'\n",
      " 'standards  and ontologies , guarantees sufficient support among '\n",
      " 'stakeholders, and provides \\n'\n",
      " 'for coordination with experts both within their own organization and from '\n",
      " 'the professional \\n'\n",
      " 'field.  \\n'\n",
      " ' \\n'\n",
      " 'The process and method are based on principles o f openness and '\n",
      " 'transparency, the \\n'\n",
      " 'stimulation of high involvement, and offering the necessary guarantees in '\n",
      " 'terms of stability, \\n'\n",
      " 'quality and applicability. Moreover, standards  and ontologies  exist in a '\n",
      " 'changing \\n'\n",
      " 'environment, so there must be room for managing  changes and maintenance of '\n",
      " 'agreements \\n'\n",
      " 'and standards.  \\n'\n",
      " ' \\n'\n",
      " 'Scope  \\n'\n",
      " 'This document describes a scalable process and method for developing and '\n",
      " 'modifying \\n'\n",
      " 'ontologies , as well as managing the ir life cycle. Th is process and '\n",
      " 'method  are largely based on \\n'\n",
      " 'the OSLO process and method, which in turn are based  on international best '\n",
      " 'practices from \\n'\n",
      " 'ISA2, W3C3 and OpenStand4, among others. This process is aimed at building '\n",
      " 'consensus \\n'\n",
      " ' \\n'\n",
      " '2 https://joinup.ec.europa.eu/document/process -and-methodology -developing '\n",
      " '-semantic -agreements  \\n'\n",
      " '3 https://www.w3.org/2017/Process -20170301/  \\n'\n",
      " '4 https://open -stand.org/  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '17 between different public administrations, as well as facilitating '\n",
      " 'semantic, syntactic, and \\n'\n",
      " 'technical interoperability. How this process can be organized is supported '\n",
      " 'by means of a \\n'\n",
      " 'method. This method describes a way of working to ensure clear '\n",
      " 'communication  and clear \\n'\n",
      " 'documentation throughout the process, so that the ontology can be '\n",
      " 'implemented by all \\n'\n",
      " 'stakeholders such as project managers, business analysts, developers, '\n",
      " 'etc.  \\n'\n",
      " 'The process and method described in this document form the basis for the '\n",
      " 'development of a \\n'\n",
      " 'new ontology , adoption and modification of existing  ontologies , and the '\n",
      " 'possible phasing out \\n'\n",
      " 'of those ontologies . In particular, this document is aimed at ontologies  '\n",
      " 'for which a recognition \\n'\n",
      " 'procedure is intended.  \\n'\n",
      " ' \\n'\n",
      " 'Figure 1: What must and what can be delivered in the context of the '\n",
      " 'development of a n ontology  \\n'\n",
      " '  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '18 Principles  \\n'\n",
      " 'The process and method explained in the following chapters follow a number '\n",
      " 'of  fundamental \\n'\n",
      " 'principles for the development of ontologies , which are based on the '\n",
      " 'principles for standards \\n'\n",
      " 'development of OpenStand5. These principles apply as best practices and have '\n",
      " 'already been \\n'\n",
      " 'endorsed by, among others, W3C, IEEE, IETF, IAB and Intern et Society.  \\n'\n",
      " ' \\n'\n",
      " '1. The ontology is developed in collaboration with all stakeholders  and '\n",
      " 'respecting \\n'\n",
      " \"everyone's autonomy, integrity, processes, and intellectual property. \"\n",
      " 'Moreover, \\n'\n",
      " 'participation stands  free to all interested and informed parties.  \\n'\n",
      " '2. The process is aimed at finding a broad consensus . Decisions are made in '\n",
      " 'a fair and \\n'\n",
      " 'transparen t way . Mechanisms are provided for appealing against decisions, '\n",
      " 'as well as \\n'\n",
      " 'for a periodic assessment of the ontologie s. Furthermore, all decisions and '\n",
      " 'relevant \\n'\n",
      " 'documentation are made publicly available . \\n'\n",
      " '3. The ontologies  being developed strive for technical merit, '\n",
      " 'interoperability and \\n'\n",
      " 'scalability . \\n'\n",
      " '4. Ontologies  together with their relevant documentation are made available '\n",
      " 'for \\n'\n",
      " 'implementation  by all parties. Specifications are being developed that '\n",
      " 'allow \\n'\n",
      " 'implementation in a rea sonable manner.  \\n'\n",
      " ' \\n'\n",
      " 'Process  \\n'\n",
      " 'The process for developing and maintaining ontologies is divided into three '\n",
      " 'high -level phases. \\n'\n",
      " 'These phases are further explained in sections 4.2, 4.3 and 4.4. First '\n",
      " 'attention is drawn to the \\n'\n",
      " 'various actors and their responsibiliti es (4.1). The change management is '\n",
      " 'explained in section \\n'\n",
      " '4.5. Finally, section 4.6 provides an explanation of the phasing out of a n '\n",
      " 'ontology . How the \\n'\n",
      " 'processes explained in this chapter, in combination with the methods from '\n",
      " 'chapter 5, are \\n'\n",
      " 'used throughout th e lifecycle of a n ontology , is summarized in chapter 6: '\n",
      " 'the lifecycle of a n \\n'\n",
      " 'ontology . \\n'\n",
      " ' \\n'\n",
      " '5 https://open -stand.org/about -us/principles/  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '19  \\n'\n",
      " 'Figure 2: High -level overview of the different processes  \\n'\n",
      " ' \\n'\n",
      " 'Actors and responsibilities  \\n'\n",
      " 'The table below provides an overview of the actors participating in the '\n",
      " 'process and their \\n'\n",
      " 'responsibilities. Each of these actors has an equivalent in the ISA '\n",
      " 'methodology for developing \\n'\n",
      " 'semantic agreements6. \\n'\n",
      " 'Actor  Responsibilities  \\n'\n",
      " 'Thematical Working \\n'\n",
      " 'group(s)7 This group of people with knowledge about the topic and/or \\n'\n",
      " 'existing data models and implementations is responsible for the \\n'\n",
      " 'development of the domain model.  \\n'\n",
      " 'Editors of thematical \\n'\n",
      " 'working group(s)8 They are responsible for facilitating the working groups '\n",
      " 'and the \\n'\n",
      " 'technical elaboration of the domain model in the form of \\n'\n",
      " 'diagrams and specifications.  \\n'\n",
      " 'Peer Review Community  A best practice for the standardization of '\n",
      " 'information is to \\n'\n",
      " 'establish a community of peers within the field of ontology \\n'\n",
      " 'creation. This community, comprising sufficient actors to \\n'\n",
      " 'represent the whole community, plays a crucial role in \\n'\n",
      " 'assessing new ontologi es to be created and evaluating \\n'\n",
      " 'ontologies that are ready for publication. They are  responsible \\n'\n",
      " 'for the review of work with regard to the standardization of \\n'\n",
      " 'information. This group should have sufficient knowledge \\n'\n",
      " 'about  mutual consistency (system operation) in the \\n'\n",
      " 'recognition of new ontologies , international standards and \\n'\n",
      " ' \\n'\n",
      " '6 https://joinup.ec.europa.eu/sites/default/files/document/2015 -\\n'\n",
      " '03/Process%20and%20methodology%20for%20developing%20semantic%20agreements.pdf  \\n'\n",
      " '7 ISA: Domain Model Working Group  \\n'\n",
      " '8 ISA: Expert Pool  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '20 ontologies that have an impact on local governments and \\n'\n",
      " 'advises on  the generic development and change process. Th is \\n'\n",
      " 'role is based on the OSLO working g roup ‘data standards’9 \\n'\n",
      " 'Product owners  Product owners are responsible for managing a n ontology '\n",
      " 'after \\n'\n",
      " 'its development. In concrete terms, they monitor problems or \\n'\n",
      " 'questions that are asked with regard to the ontology,  call the \\n'\n",
      " 'working group together in func tion of the questions asked, and \\n'\n",
      " 'are responsible for the further development of ontologie s in the \\n'\n",
      " 'context of new use cases or changes in underlying standards  or \\n'\n",
      " 'ontologies  (dependencies).  \\n'\n",
      " 'Project Management \\n'\n",
      " 'Data Standards10 Responsible for organizing work ing groups and inviting '\n",
      " 'experts, \\n'\n",
      " 'as well as communication with various stakeholders.  \\n'\n",
      " ' \\n'\n",
      " 'Announce a n ontology  \\n'\n",
      " 'In line with the basic principles for ontologies  development, it is best '\n",
      " 'practice to report \\n'\n",
      " 'ontologies  to the Ontology Peer Review Community  in time and to reach a '\n",
      " 'broad consensus.  \\n'\n",
      " 'To ensure a widely supported ontology , early involvement of the business is '\n",
      " 'needed. Their \\n'\n",
      " 'knowledge makes it possible to map existing processes - together with the '\n",
      " 'terminology used \\n'\n",
      " '- and formulate use cases  for the ontology  to be developed. Moreover, a '\n",
      " 'first High Level \\n'\n",
      " 'Domain Model can be drawn up together with the business. This information '\n",
      " 'forms the basis \\n'\n",
      " 'for recording semantic agreements and already provides an insight into the '\n",
      " 'relevance of the \\n'\n",
      " 'ontology  in the initial phase.  \\n'\n",
      " ' \\n'\n",
      " '9 ISA: Review Group  \\n'\n",
      " '10 ISA: Secretariat & Activity Leader  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '21  \\n'\n",
      " 'Figure 3: Process for the announcement of a n ontology  \\n'\n",
      " ' \\n'\n",
      " 'Step 1.  Develop and communicate a declaration of intent that describes the '\n",
      " 'scope of \\n'\n",
      " 'the to -be-developed ontology  \\n'\n",
      " 'The purpose of the declaration of intent is to answer a number of basic '\n",
      " 'questions:  \\n'\n",
      " '● Why is it important to develop this ontology ? What is the added value?  \\n'\n",
      " '● What is the interface with existing standards and ontologies at a '\n",
      " 'national11, European \\n'\n",
      " 'or global l evel?  \\n'\n",
      " '● Which standards , ontologies  and other sources already exist in this '\n",
      " 'domain?  \\n'\n",
      " '● Who are the stakeholders that need to be involved and why do they need to '\n",
      " 'be \\n'\n",
      " 'involved?  \\n'\n",
      " 'The declaration of intent is prepared by the project management. As an '\n",
      " 'example, we refer to \\n'\n",
      " 'the project charter12 of Maregraph and the registration form13 regarding the '\n",
      " 'business \\n'\n",
      " 'workshop for MareGraph.  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " \"11 By 'national level,' we refer to the country where the contracting party \"\n",
      " 'originates from, as well as the country \\n'\n",
      " 'where the party in charge of developing the data standard originates '\n",
      " 'from.  \\n'\n",
      " '12 https://www.maregraph.eu/files/Charter_Maregraph_OSLO.pdf  \\n'\n",
      " '13 Registration form business workshop  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '22 Step 2.  Invite relevant and interested business stakeholders to a '\n",
      " 'workshop meeting to \\n'\n",
      " 'identify processes and use cases  \\n'\n",
      " 'The declaration of intent  forms the basis for a first meeting with an '\n",
      " 'initial group of \\n'\n",
      " 'stakeholders to identify different use cases14 to which this ontology c an '\n",
      " 'serve, starting from \\n'\n",
      " 'the processes. This session is organized by the project management and the '\n",
      " 'editors and \\n'\n",
      " 'serves as pr eparation for the further development of the process for '\n",
      " 'ontology  development, \\n'\n",
      " 'on the basis of which an official Working Group Charter is elaborated in the '\n",
      " 'next step. If a \\n'\n",
      " 'thematic working group has already been established, the members of this '\n",
      " 'group can a lso be \\n'\n",
      " 'invited to this workshop.  \\n'\n",
      " 'Step 3.  Further develop declaration of intent into a Working Group Charter '\n",
      " 'by adding \\n'\n",
      " 'requirements and conditions based on input from the business  \\n'\n",
      " 'The Working Group Charter sets the expectations for the deliverables that '\n",
      " 'the themati c \\n'\n",
      " 'working group will produce. It allows the Peer Review Community  data '\n",
      " 'standards to evaluate \\n'\n",
      " 'the relevance and applicability of the ontology  to be developed. For '\n",
      " 'practical guidelines \\n'\n",
      " 'regarding the preparation of a Working Group Charter, see “5.1. Drawing up a '\n",
      " 'Working Group \\n'\n",
      " 'Charter”.  \\n'\n",
      " 'Step 4.  Present the Working Group Charter to the Pe er Review Community  '\n",
      " 'for \\n'\n",
      " 'approval for starting a thematic working grou p \\n'\n",
      " 'The charter is submitted to the p eer review community  for approval before '\n",
      " 'the public \\n'\n",
      " 'working groups can start working on the development of a specification . '\n",
      " 'Once this has been \\n'\n",
      " 'appr oved by both bodies, the registration of the ontology  is successful,  '\n",
      " 'and the  ontology is \\n'\n",
      " 'entered in the relevant registry with the status “under development”. As '\n",
      " 'part of the \\n'\n",
      " 'treatment of the charter, it is decided in consultation with the thematic '\n",
      " 'working group \\n'\n",
      " 'whether the ontology to be developed aims for a voluntary, \"comply or '\n",
      " 'explain\", or \\n'\n",
      " 'mandatory nature.  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '14 See for example the process workshops that were held in the context of '\n",
      " 'the project of “Lokale Besluiten als \\n'\n",
      " 'Linked Open Data”. The report on this workshop can be found on:  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '23 Announce a n ontology  \\n'\n",
      " 'A specification is a technical document that gives substance to the ontology '\n",
      " '. In practice it is \\n'\n",
      " 'often difficult to distinguish the specification from the ontology  itself. '\n",
      " 'Typical examples in this \\n'\n",
      " 'regard are PDF -A, DCAT and RDF. In some cases, multiple specifications are '\n",
      " 'part of a n \\n'\n",
      " 'ontology . These specifications then each give a domain -specific '\n",
      " 'interpretation to the \\n'\n",
      " 'ontology . An example of this is the INSPIRE Data S pecifications15, which '\n",
      " 'provide a domain -\\n'\n",
      " 'specific interpretation of the \"INSPIRE Implementing Rules\" (the standard) '\n",
      " 'for each of the \\n'\n",
      " 'INSPIRE themes.  \\n'\n",
      " 'The process for developing a specification is based on the process for the '\n",
      " 'ISA process for \\n'\n",
      " 'developing semanti c agreements16. This process must be followed for the '\n",
      " 'development of a \\n'\n",
      " 'specification for ontologies  such as domain models and controlled vocabular '\n",
      " 'ies.\\n'\n",
      " ' \\n'\n",
      " 'Figure 4: process for the development of a n ontology  \\n'\n",
      " 'Step 5.  Set up the working group a nd environment  \\n'\n",
      " 'In this step, the practical side of the organization of the working group is '\n",
      " 'set up. This means \\n'\n",
      " 'that a project environment is set up, the members of the working group are '\n",
      " 'invited,  and the \\n'\n",
      " 'composition and assignment of roles is recorded. The ontology to be created '\n",
      " 'is now in the \"in \\n'\n",
      " 'development\" phase. Furthermore, the planning for organizing the working '\n",
      " 'group meetings, \\n'\n",
      " 'the public review and finalization is created.  \\n'\n",
      " ' \\n'\n",
      " 'Step 6.  Creating an initial draft  \\n'\n",
      " 'Based on the knowledge at the start of the process, for example based on '\n",
      " 'available project \\n'\n",
      " 'documentation, wireframes, process descriptions, elaborated use cases and '\n",
      " 'existing models \\n'\n",
      " 'and standards, a specification version is prepared. Questions and any probl '\n",
      " 'ems that arise \\n'\n",
      " ' \\n'\n",
      " '15 http://inspire.ec.europa.eu/data -specificat ions/2892  \\n'\n",
      " '16https://joinup.ec.europa.eu/sites/default/files/document/2015 -\\n'\n",
      " '03/Process%20and%20methodology%20for%20developing%20semantic%20agreements.pdf  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '24 from the analysis are listed in an action list. A proposal for a solution '\n",
      " 'is created for as many \\n'\n",
      " 'listed action points as possible. This will serve as a starting point for '\n",
      " 'discussions in the working \\n'\n",
      " 'group meetings.  \\n'\n",
      " ' \\n'\n",
      " 'Formalizing semantic agree ments  \\n'\n",
      " 'Formalizing semantic agreements is a specialization of the process for '\n",
      " 'developing a \\n'\n",
      " 'specification. When creating an initial draft, the method for developing a '\n",
      " 'domain model as \\n'\n",
      " 'described in section 5.3 is used.  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'Step 7.  Organizing the working groups  \\n'\n",
      " 'The project manager and editor together prepare the agenda for the working '\n",
      " 'group meeting, \\n'\n",
      " 'based on open points that arise from the analysis and/or the previous '\n",
      " 'working group meeting. \\n'\n",
      " 'During the working group meetings, the members of the working group go th '\n",
      " 'rough the initial \\n'\n",
      " 'or intermediate draft of the specification, and through the various items on '\n",
      " 'the agenda that \\n'\n",
      " 'are listed in the action list and  try to reach a consensus.  \\n'\n",
      " ' \\n'\n",
      " 'Step 8.  Elaborate interim draft specification  \\n'\n",
      " 'The conclusions of the working group meeting a re processed in a new interim '\n",
      " 'draft. Any new \\n'\n",
      " 'points that were identified during the working group or during the '\n",
      " 'development of a new \\n'\n",
      " 'draft are added to the action list and serve as input for creating the '\n",
      " 'agenda for the next \\n'\n",
      " 'working group meeting.  \\n'\n",
      " ' \\n'\n",
      " 'Formaliz ing semantic agreements  \\n'\n",
      " 'Formalizing semantic agreements is a specialization of the process for '\n",
      " 'developing a \\n'\n",
      " 'specification. When creating an interim draft, the method for developing a '\n",
      " 'domain model as \\n'\n",
      " 'described in secti on 5.3  is used.  \\n'\n",
      " ' \\n'\n",
      " 'Step 9.  Mid-term evaluation by the Peer Review Community  \\n'\n",
      " 'A stable interim draft specification is proposed to the Peer Review '\n",
      " 'Community , together with \\n'\n",
      " 'an overview of the organized working group sessions and the p arties '\n",
      " 'involved. The Peer \\n'\n",
      " 'Review Community  decides whether the specification is sufficiently mature '\n",
      " 'to switch to a \\n'\n",
      " 'public review period and uses the criteria for promotion to a proposed '\n",
      " 'ontology for this. The \\n'\n",
      " 'duration of the public review period is determined in consultation between '\n",
      " 'the thematic \\n'\n",
      " 'working group and the Peer Review Community . \\n'\n",
      " ' \\n'\n",
      " 'Step 10.  Organizing a public review  \\n'\n",
      " 'After completing various iterations of steps 4 and 5, and once there is '\n",
      " 'sufficien t consensus \\n'\n",
      " 'around the specification, a public review period is organized, in which the '\n",
      " 'general public is \\n'\n",
      " 'asked to provide feedback. This public review can be accompanied by the '\n",
      " 'organization of \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '25 extra public workshops to capture feedback. Based on the feedb ack '\n",
      " 'received, there are two \\n'\n",
      " 'options:  \\n'\n",
      " '1. The feedback received is editorial or results in minor semantic changes '\n",
      " '(see 4.5. Change \\n'\n",
      " 'management  related to receiving and classifying feedback): The final '\n",
      " 'version of the model can \\n'\n",
      " 'be prepared and publication can be made, provided that a short validation is '\n",
      " 'possible by the \\n'\n",
      " 'thematic working group.  \\n'\n",
      " '2. The feedback receive d includes proposals for major semantic changes: one '\n",
      " 'or more additional \\n'\n",
      " 'working group meetings are needed to clarify the new actions and reach '\n",
      " 'consensus again. If \\n'\n",
      " 'this is deemed necessary by the working group, a new public review can be '\n",
      " 'organized again.  \\n'\n",
      " 'The ontology  ends up in the \"pending\" phase at the start of the public '\n",
      " 'review period and \\n'\n",
      " 'receives a publication status of \"proposed ontology \". Before this phase can '\n",
      " 'be started, the \\n'\n",
      " 'project management together with the editors of the working group and the '\n",
      " 'wor king group \\n'\n",
      " 'must test ontologies  to see whether all criteria for promotion to proposed '\n",
      " 'ontology  have \\n'\n",
      " 'been met (see step 5). The public review period is ideal for creating and '\n",
      " 'evaluating proof -of-\\n'\n",
      " 'concept implementations of the specification. These proof -of-concepts can '\n",
      " 'be carried out by \\n'\n",
      " 'members of the thematic working group or by external interested parties.  \\n'\n",
      " ' \\n'\n",
      " 'Step 11.   Finalizing the specification  \\n'\n",
      " 'The editors process, when necessary,  in consultation with the thematic '\n",
      " 'working group, all \\n'\n",
      " 'feedback received. This results in a final, stable version of the '\n",
      " 'specification and accompanying \\n'\n",
      " 'documentation.  \\n'\n",
      " ' \\n'\n",
      " 'Step 12.  Quality contr ol by the Peer Review Community   \\n'\n",
      " 'The Peer Review Community  performs a quality check to ensure that the '\n",
      " 'process has been \\n'\n",
      " 'followed correctly and whether the objectives described in the Working Group '\n",
      " 'Charter have \\n'\n",
      " 'been achieved. If the work is assessed positiv ely, it can be published to '\n",
      " 'the proper channels , \\n'\n",
      " 'otherwise the thematic working group may be asked to go through (part of) '\n",
      " 'the process again. \\n'\n",
      " 'The Peer Review Community should  use the criteria for promotion to a '\n",
      " 'recognized  ontology  \\n'\n",
      " 'for this quality control.  \\n'\n",
      " ' \\n'\n",
      " 'Step 13.  Assessing and confirm agreements made  \\n'\n",
      " 'After the assessment , the domain model can be promoted to a recognized '\n",
      " 'ontology (see \\n'\n",
      " 'criteria for promotion to a recognized ontology ), the ontolog y is then in '\n",
      " 'the “in use” phase , \\n'\n",
      " 'or the thematic working group  can go through (part of) the process '\n",
      " 'again.  \\n'\n",
      " ' \\n'\n",
      " 'Publication  \\n'\n",
      " 'To promote the adoption of the ontology , it is necessary to provide '\n",
      " 'technolo gy as an aid to \\n'\n",
      " 'start using it. Therefore, following the development of a specification, at '\n",
      " 'least the following \\n'\n",
      " 'steps are taken that are aimed at providing developers, information '\n",
      " 'architects and other \\n'\n",
      " 'stakeholders with the necessary documentation and resou rces to implement '\n",
      " 'the ontology : \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '26  \\n'\n",
      " 'Figure 5: process for the publication of an ontology  \\n'\n",
      " ' \\n'\n",
      " 'Step 1.  Publishing specification in both human and machine readable '\n",
      " 'format  \\n'\n",
      " 'The data specification allows developers and information architects to '\n",
      " 'estimate the impact \\n'\n",
      " 'on existing and new applications. It provides insight into how a n ontology  '\n",
      " 'can be used. Finally, \\n'\n",
      " 'a machine allows readable data specification to automate certain aspects of '\n",
      " 'the adoption. \\n'\n",
      " 'The ontology  is included in the registry  with the status “in use”, with a '\n",
      " 'reference to the \\n'\n",
      " 'specification that is published.  \\n'\n",
      " 'Step 2.   Publishing reusable elements that project teams can make use of  \\n'\n",
      " 'Reusable elements, such as a JSON -LD context file in which a data '\n",
      " 'specification (eg in the case \\n'\n",
      " 'of OSL O a vocabulary) is translated into a list of terms, along with their '\n",
      " 'identifier, that can be \\n'\n",
      " 'used to create a compliant JSON payload 17. Other examples are the \"subject '\n",
      " 'pages18\" that \\n'\n",
      " 'are made available as standard to support the URI.  \\n'\n",
      " 'Step 14.  Publishing a conformit y test suite  \\n'\n",
      " 'A conformity test suite allows you to validate implementations and ensures '\n",
      " 'correct adoption \\n'\n",
      " 'of standards  and ontologies . Examples are the SHACL 19validator for OSLO '\n",
      " 'and the \"INSPIRE \\n'\n",
      " 'Validator\" of the European Commission20. \\n'\n",
      " ' \\n'\n",
      " 'Change Management  \\n'\n",
      " 'An ontology  no matter in what of its lifecycle, can be subject to feedback '\n",
      " 'and necessary \\n'\n",
      " 'changes. It is important that this feedback is captured and evaluated in a '\n",
      " 'structured way, and \\n'\n",
      " 'a clear, repeatable and transparent process to deal with it.  \\n'\n",
      " 'Change mana gement ensures that there is the necessary guarantee that '\n",
      " 'changes, if \\n'\n",
      " 'necessary, are coordinated with the necessary stakeholders and that the '\n",
      " 'impact of changes \\n'\n",
      " 'is taken into account.  \\n'\n",
      " ' \\n'\n",
      " '17 http://data.vlaanderen.be/doc/applicatieprofiel/persoon#jsonld  \\n'\n",
      " '18 When a data URI is entered into a brow ser, a subject page can be '\n",
      " 'displayed that displays a description of the \\n'\n",
      " 'data resource in man and machine readable format.  \\n'\n",
      " '19 https://www.w3.org/TR/shacl/  \\n'\n",
      " '20 http://inspire -sandbox.jrc.ec.europa.eu/validator/  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '27 The change management process is aligned with the corresponding process21 '\n",
      " 'developed by \\n'\n",
      " 'the ISA Program, and is based on the following principles:  \\n'\n",
      " '● Openness : Openness means that feedback can be given on the ontologie s '\n",
      " 'and their \\n'\n",
      " 'underlying specifications by anyone and that logging, analysis and decisions '\n",
      " 'are  done in \\n'\n",
      " 'complete transparency.  \\n'\n",
      " '● Controlled change : Changes must be step -by-step and traceable, taking '\n",
      " 'into account  the \\n'\n",
      " 'possible impact for those parties who have already implemented the ontology '\n",
      " '. \\n'\n",
      " 'Change management applies to those phases of the lifecycle where the '\n",
      " 'ontology  is \"stable\":  \\n'\n",
      " '● Candidate ontology  \\n'\n",
      " '● Recognized ontology  \\n'\n",
      " '● Candidate revised ontology  \\n'\n",
      " ' \\n'\n",
      " 'Feedback can be given at any time, and is evaluated, logged and treated '\n",
      " 'according to the \\n'\n",
      " 'process described below. Feedback while the ontology is \"under development\" '\n",
      " 'or \"under \\n'\n",
      " 'review\" is immediately taken into account during the (re) definition '\n",
      " 'according to  the process \\n'\n",
      " 'described in the section \" developing a specification \", unless the Peer '\n",
      " 'review community  \\n'\n",
      " 'decides to park it and to include it in a next release. We also refer to the '\n",
      " 'method for managing \\n'\n",
      " 'issues and errata . \\n'\n",
      " 'The change management process consists of the following major steps or sub '\n",
      " '-processes:  \\n'\n",
      " 'Figure 6: Process for change manage ment  \\n'\n",
      " 'Step 1.  Receiving feedback  \\n'\n",
      " 'In this step, the feedback received is captured and evaluated for relevance. '\n",
      " 'This is the \\n'\n",
      " 'responsibility of the product owner. If the feedback is assessed as '\n",
      " 'relevant, it is logged. If not, \\n'\n",
      " 'the relevant stakeholder will be notified and the feedback will not be '\n",
      " 'logged. The feedback \\n'\n",
      " 'can come from, among others, people or organizations that implement the '\n",
      " 'ontology  in their \\n'\n",
      " 'applications, see conflicts with other standards /ontologies  or provide new '\n",
      " 'use cases that the \\n'\n",
      " 'ontology must  accommodate.  \\n'\n",
      " 'The logged feedback is then subjected to an evaluation to determine further '\n",
      " 'processing. In \\n'\n",
      " 'particular, an evaluation is made of the type of change that may be required '\n",
      " 'to the ontology \\n'\n",
      " 'and its underlying specifications:  \\n'\n",
      " ' \\n'\n",
      " '21 https://joinup.ec.europa.eu/document/description -change -management '\n",
      " '-release -and-publication -process -\\n'\n",
      " 'structural -metadata  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '28 ● Editorial changes and e rrors : These are changes that have no impact '\n",
      " 'on the \\n'\n",
      " 'applications that have implemented the ontology , for example additional '\n",
      " 'clarification, \\n'\n",
      " 'typing errors, etc.  \\n'\n",
      " '● Minor substantive changes : Examples of minor substantive changes in the '\n",
      " 'context of  \\n'\n",
      " 'ontologies or  semantic data standards such as OSLO are: the addition of a '\n",
      " 'property \\n'\n",
      " 'and making certain restrictions stricter or less strict. These changes have '\n",
      " 'a (possible) \\n'\n",
      " 'impact on implementations, but a small impact.  \\n'\n",
      " '● Major substantive changes : These changes impact fun damental matters in '\n",
      " 'the \\n'\n",
      " 'specification and underlying specifications, for example by changing a '\n",
      " 'definition, \\n'\n",
      " 'adding classes, removing properties or fundamentally changing audited '\n",
      " 'vocabulary. \\n'\n",
      " 'Existing implementations will be forced to analyse  the impact and, where '\n",
      " 'necessary, \\n'\n",
      " 'make changes in order to remain in conformity with the (new version of the) '\n",
      " 'ontology . \\n'\n",
      " 'Step 2.  Processing changes  \\n'\n",
      " 'The processing of changes depends on the type of change listed above:  \\n'\n",
      " ' \\n'\n",
      " '● Editorial changes and errors:  These changes can simply be implemented. A '\n",
      " 'new \\n'\n",
      " 'version does not necessarily have to be published and, for example. erratum '\n",
      " 'to be \\n'\n",
      " 'published.  \\n'\n",
      " '● Minor substantive changes:  For these changes, the process for developing '\n",
      " 'a \\n'\n",
      " 'specification must be followed. However, for minor changes this can be a '\n",
      " 'shortened \\n'\n",
      " 'procedure, in which the thematic working group is convened to discuss the '\n",
      " 'issues and \\n'\n",
      " 'then implement the changes in a new ver sion of the specification. When it '\n",
      " 'comes to \\n'\n",
      " 'an ontology  that is already \"in use\" (cf. lifecycle of a n ontology ), a '\n",
      " 'period of public \\n'\n",
      " 'review is started and the specification receives the publication status '\n",
      " '\"Candidate \\n'\n",
      " 'Revised ontology \". \\n'\n",
      " '● Major substantive chan ges: For these changes, the entire process for '\n",
      " 'developing a \\n'\n",
      " 'specification must be run through, including a new public review period, '\n",
      " 'regardless of \\n'\n",
      " 'the lifecycle phase in which the ontology is located.  \\n'\n",
      " 'It is important to note that logged changes should not be treated one by '\n",
      " 'one. Once logged, \\n'\n",
      " 'these can be bundled and included in the specification according to a '\n",
      " 'predefined release \\n'\n",
      " 'cycle. The frequency or the criteria with which a new release is carried out '\n",
      " 'must be laid down \\n'\n",
      " 'in the Working Group Charter.  \\n'\n",
      " 'When i t is decided to process the feedback received in a new version of the '\n",
      " 'specification (in \\n'\n",
      " 'the case of small or large substantive changes), the lifecycle phase starts '\n",
      " '\"in revision\". The \\n'\n",
      " 'feedback can also trigger the phasing out of a n ontology,  for example whe '\n",
      " 'n it appears that it \\n'\n",
      " 'has been completely surpassed by technological changes. We refer to the '\n",
      " 'process for phasing \\n'\n",
      " 'out of a n ontology . \\n'\n",
      " 'Step 15.  Publication of a new version  \\n'\n",
      " 'After analysing  and implementing the changes, according to the processes '\n",
      " 'required according \\n'\n",
      " 'to the type of change, a new version of the ontology,  the underlying '\n",
      " 'specifications and the \\n'\n",
      " 'supporting documentation must be prepared and finally published. Older '\n",
      " 'versions of the \\n'\n",
      " 'ontology  and the underlying specification remain available and contain '\n",
      " 'referenc es to the \\n'\n",
      " 'most recent version. The version is determined by the publication date and '\n",
      " 'not by \\n'\n",
      " 'incremental version numbers.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '29 Phasing out a n ontology  \\n'\n",
      " 'An ontology can be phased out, for example when it is outdated by '\n",
      " 'technological evolution \\n'\n",
      " 'or when significant errors are found in the specification.  \\n'\n",
      " ' \\n'\n",
      " 'Figure 7: Process for phasing out a n ontology  \\n'\n",
      " 'Step 1.  Proposal for phasing out a n ontology  \\n'\n",
      " 'When feedback received from stakeholders shows that a n ontology  is '\n",
      " 'obsolete, or when \\n'\n",
      " 'significan t errors are found, the product owner of the ontology  can decide '\n",
      " 'to submit a \\n'\n",
      " 'proposal to the Peer Review Community , in consultation with the editors and '\n",
      " 'the members \\n'\n",
      " 'of the thematic working, for phasing out the ontology . \\n'\n",
      " 'Step 2.  Assessment of the proposal, announce ment and implementation of a '\n",
      " 'public \\n'\n",
      " 'review period  \\n'\n",
      " 'The Peer Review Community  evaluates the proposal and, if admissible, '\n",
      " 'announces a public \\n'\n",
      " 'review period, during which all interested stakeholders can provide feedback '\n",
      " 'on the proposal \\n'\n",
      " 'to phase out the ontology . This public review period lasts at least four '\n",
      " 'weeks and is also \\n'\n",
      " 'intended as a transitional period, during which the ontology  is still in '\n",
      " 'use.  \\n'\n",
      " 'Step 16.  Ratification of the decision to phase out a n ontology  \\n'\n",
      " 'If no valid objections were raised during the public review period, the '\n",
      " 'ontology can be  phase d \\n'\n",
      " 'out. \\n'\n",
      " '  \\n'\n",
      " '\\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '30 Step 17.  Publication of the phased -out ontology  \\n'\n",
      " 'The product owner, editors and thematic working groups publish a version of '\n",
      " 'the specification \\n'\n",
      " 'with the publication status “phased -out ontology ”. This publication also '\n",
      " 'includes the reason \\n'\n",
      " 'for phasing out the ontology . \\n'\n",
      " ' \\n'\n",
      " 'Method  \\n'\n",
      " 'The method describes how the process can be set up based on a number of  '\n",
      " '(technical) \\n'\n",
      " 'documents to ultimately result in a n ontology . First the method is '\n",
      " 'explained to arrive at a \\n'\n",
      " 'domain model. It is then explained how the transparency of the process can '\n",
      " 'be guaranteed \\n'\n",
      " 'by producing relevant documentation. The following chapter pro vides an '\n",
      " 'overview of the \\n'\n",
      " 'tools that can be used to generate the documents listed.  \\n'\n",
      " ' \\n'\n",
      " 'Setting up a working group charter  \\n'\n",
      " 'The Working Group Charter is based on an artifact from the W3C '\n",
      " 'Standardization process22. \\n'\n",
      " 'This document is created in the first phase of the development process of a '\n",
      " 'n ontology  and \\n'\n",
      " 'sets expectations for the deliverables that the thematic working group will '\n",
      " 'produce. The \\n'\n",
      " 'charter contains the following information:  \\n'\n",
      " '● The objective and scope of the thematic working group (eg the development '\n",
      " 'of a n \\n'\n",
      " 'ontology  for domain X).   \\n'\n",
      " '● The evaluation criteria that are used during the development process.  For '\n",
      " 'example, \\n'\n",
      " 'whether and how many implementations have to exist before the ontology can '\n",
      " 'be \\n'\n",
      " 'approved and the nature of these implementations (proof  -of- concepts  or '\n",
      " 'production \\n'\n",
      " 'implementations).   \\n'\n",
      " '● The duration of the working group ( e.g. 6  months).   \\n'\n",
      " '● The type of deliverables (eg specification document, software '\n",
      " 'component).   \\n'\n",
      " '● Expected milestones (dates), when known.   \\n'\n",
      " '● The internal process of the thematic working group for approving '\n",
      " 'deliverables (for \\n'\n",
      " 'example, unanimity, or unanimity minus one).         \\n'\n",
      " '● Dependencies between these and other thematic working groups.         \\n'\n",
      " '● Modalities for the working group meetings such as location and '\n",
      " 'frequency.         \\n'\n",
      " '● If available, the date of the first face - to- face meeting.         \\n'\n",
      " '● Communication mechanisms ( eg GitHub repository, mailing list , Google '\n",
      " 'Drive folder, \\n'\n",
      " 'etc.)         \\n'\n",
      " '● Information regarding intellectual property and licenses.  \\n'\n",
      " '● The frequency that the criteria base d on which issues after the '\n",
      " 'publication of a n ontology  \\n'\n",
      " 'will be dealt with and new releases will be prepared. In other words, how '\n",
      " 'are change \\n'\n",
      " 'management and release management arranged in a practical way?         \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '22 https://www.w3.org/2017/Process -20170301/#WGCharter  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '31 Organising and facilitating working group me etings  \\n'\n",
      " 'The working group consists of a collection of domain experts and '\n",
      " 'stakeholders with \\n'\n",
      " 'knowledge of existing use cases and implementations. Invitations to working '\n",
      " 'group meetings \\n'\n",
      " 'are issued by the project management which have a view of relevant stakehol '\n",
      " 'ders based on \\n'\n",
      " 'previous experiences, existing contacts and a relevant stakeholder list '\n",
      " 'provided by the \\n'\n",
      " 'contracting party.  \\n'\n",
      " 'A typical development process will require at least three working group '\n",
      " 'meetings, which can \\n'\n",
      " 'be structured as follows:  \\n'\n",
      " 'Working group meeting 1.  Become familiar with use cases and existing '\n",
      " 'standards  \\n'\n",
      " '● Explain the working group structure and used tooling for communication and '\n",
      " 'follow -up. \\n'\n",
      " '● Explaining existing use cases, e.g. based  on a few guest speakers.  \\n'\n",
      " '● Brainstorming session (possibly in subgroups) around other relevant use '\n",
      " 'cases and \\n'\n",
      " 'information needs.  \\n'\n",
      " ' \\n'\n",
      " 'Working group meeting 2.  Substantive discussions concerning the thematic '\n",
      " 'domain  \\n'\n",
      " '● Discussing draft specification  \\n'\n",
      " '● Discussing open issues  \\n'\n",
      " '● Preparation of action and discussion points  \\n'\n",
      " ' \\n'\n",
      " 'Working group meeting 3.  Finishing and concluding specificat ion \\n'\n",
      " '● Discussing remaining discussion points  \\n'\n",
      " '● Discussing final specification  \\n'\n",
      " '● Testing of specification against use cases   \\n'\n",
      " ' \\n'\n",
      " 'Additionally, extra working group meetings can be scheduled for substantive '\n",
      " 'discussions, with \\n'\n",
      " 'the entire working group or with a subset of this group to discuss specific '\n",
      " 'topics. It is the role \\n'\n",
      " 'of the editors of the working group to prepare and moderate the meetings, '\n",
      " 'their tasks \\n'\n",
      " 'include:  \\n'\n",
      " '● Preparing agenda items  \\n'\n",
      " '● Timekeeping during working group meeting  \\n'\n",
      " '● Taking minutes of the working gro up meeting  \\n'\n",
      " '● Facilitating discussions  \\n'\n",
      " 'Prior to each working group meeting, the following documents are forwarded '\n",
      " 'to the \\n'\n",
      " 'participants in preparation:  \\n'\n",
      " '● Latest version of the domain model with a summary of any changes.  \\n'\n",
      " '● Up-to-date overview of action and discussio n points (consolidation of '\n",
      " 'previous \\n'\n",
      " 'working group session + online discussions between the working group '\n",
      " 'sessions)  \\n'\n",
      " '● Report from previous workgroup session  \\n'\n",
      " '● Practical information and agenda for the next working group  \\n'\n",
      " 'Following each working group session, the following information is sent to '\n",
      " 'the participants:  \\n'\n",
      " '● Report of the meeting including links to the documents that were used (eg '\n",
      " 'draft \\n'\n",
      " 'specification)  \\n'\n",
      " 'Invitation for participants to continue discussions via GitHub.  \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '32 Developing a domain model  \\n'\n",
      " 'The development of the domain model takes place in thematic working groups '\n",
      " 'and requires \\n'\n",
      " 'input from various stakeholders. The figure below provides an overview of '\n",
      " 'the various steps \\n'\n",
      " 'for developing a domain model. This method is based on the ISA process and m '\n",
      " 'ethod for \\n'\n",
      " 'recording semantic agreements23 and the W3C Process Document24.  \\n'\n",
      " ' \\n'\n",
      " 'Figure 8: Developing a domain model  \\n'\n",
      " 'Step 1.  Based on a first workshop with Business Stakeholders and the '\n",
      " 'information in \\n'\n",
      " 'the working group charter, use cases and competency questions are created '\n",
      " 'to \\n'\n",
      " 'accommodate the data as a n ontology . These can be documented in a '\n",
      " 'separate \\n'\n",
      " 'document or later contained in the specification of the domain model or '\n",
      " 'the \\n'\n",
      " 'definitions and description of the data entitie s.  \\n'\n",
      " ' \\n'\n",
      " 'Step 2.  Requirements are distilled from the use cases with which the data '\n",
      " 'must comply \\n'\n",
      " 'as an ontology . For example, based on Use Case X we can deduce that the '\n",
      " 'following \\n'\n",
      " 'data entities, attributes and relationships are needed . \\n'\n",
      " '    \\n'\n",
      " 'Step 3.  The Use Cases and Requirements make it possible to make an overview '\n",
      " 'of the \\n'\n",
      " 'information needs (data entities, attributes and relationships) that are '\n",
      " 'required in the \\n'\n",
      " 'domain model.              \\n'\n",
      " ' \\n'\n",
      " 'Step 4.  The working group identifies and analyses  existing models (and '\n",
      " 'data \\n'\n",
      " 'standards), both at the level of individual business applications and '\n",
      " 'applicable \\n'\n",
      " 'international standards (W3C, ISA, IETF, etc.)  \\n'\n",
      " ' \\n'\n",
      " '23 https://joinup.ec.europa.eu/document/process -and-methodology -developing '\n",
      " '-semantic -agreements  \\n'\n",
      " '24 https://www.w3.org/2017/Process -20170301/  \\n'\n",
      " 'Commented [A7]: we also identified competency questions. We \\n'\n",
      " 'should consider this in the upcoming deliverable  \\n'\n",
      " 'Commented [A8]: this sentence seems not finished.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '33 Step 5.  A mapping table is prepared to compare the information needs '\n",
      " 'with existing \\n'\n",
      " 'models and data standards.  This is done based on the SKOS matching '\n",
      " 'principles25 . An \\n'\n",
      " 'example and template of such a mapping table can be found on the OSLO Google '\n",
      " 'Drive  \\n'\n",
      " 'or on the follow ing link where an example of a mapping can be found for '\n",
      " 'OSLO: \\n'\n",
      " 'Mobiliteit: T rips en Aanbod .  \\n'\n",
      " ' \\n'\n",
      " 'Step 6.  The mapping table from the previous step makes it possible to '\n",
      " 'select core data \\n'\n",
      " 'entities, attributes and relations for the domain model.  Where possible, '\n",
      " 'existing \\n'\n",
      " 'models and data standards are reused,  and sufficient attention is paid to '\n",
      " 'the \\n'\n",
      " 'elaboration of the new elements.  \\n'\n",
      " ' \\n'\n",
      " 'Step 7.  A draft domain model is created and documented. This leads to (1) a '\n",
      " 'graphical \\n'\n",
      " 'representation ( e.g. UML class diagram, graf foo diagram , …) of the domain '\n",
      " 'model and \\n'\n",
      " '(2) a data specification in the form of a vocabulary in both human  and '\n",
      " 'machin e-\\n'\n",
      " 'readable formats . Examples of this can be found on https://purl.eu/ , '\n",
      " 'section 4.4 \\n'\n",
      " 'explains which tools can be used to generate these artifacts.  \\n'\n",
      " ' \\n'\n",
      " 'Step 8.  Controlled vocabularies (code lists, taxonomies, thesauri, etc.) '\n",
      " 'are harmonized \\n'\n",
      " 'and recorded.   \\n'\n",
      " ' \\n'\n",
      " 'Step 9.  The domain model is finalized. Furthermore, controlled '\n",
      " 'vocabularies, along \\n'\n",
      " 'with any other restrictions such as cardinalities,  can also be included in '\n",
      " 'the \\n'\n",
      " 'specification. This leads to a new version of (1 ) the  graphical '\n",
      " 'representation,  (2) the \\n'\n",
      " 'vocabulary document and (3), if controlled vocabularies and other '\n",
      " 'restrictions were \\n'\n",
      " 'added, an application profile. Examples of application profiles can also be '\n",
      " 'found on \\n'\n",
      " 'https://purl.eu/ , the relevant tooling is explained in section 4.4.  \\n'\n",
      " ' \\n'\n",
      " 'Step 10.  Finally, a conformity clause must be determined and documented. '\n",
      " 'This \\n'\n",
      " 'determines what demands an implementation of the ontology  must meet in '\n",
      " 'order to \\n'\n",
      " 'conform to the data specification. Examples of this can b e found in the '\n",
      " 'vocabulary and \\n'\n",
      " 'application profiles at https://purl.eu/ .         \\n'\n",
      " ' \\n'\n",
      " 'Supporting transparency during development  \\n'\n",
      " 'To support transparency of the development process of the ontology , the '\n",
      " 'following documents or \\n'\n",
      " 'resources are made publicly accessible:  \\n'\n",
      " '● The Working Group Charter will be published on the registry on a dedicated '\n",
      " 'online \\n'\n",
      " 'repository (e.g.  GitHub) as well as on relevant standards registers such '\n",
      " 'as \\n'\n",
      " 'https://purl.eu/ .  \\n'\n",
      " '● Reports of meetings held by the working group are made publicly available '\n",
      " 'in HTML format \\n'\n",
      " 'on the dedicated online repository as well as on relevant standards '\n",
      " 'registers such as \\n'\n",
      " 'https://purl.eu/ .  \\n'\n",
      " ' \\n'\n",
      " '25 https://www.w3.org/TR/skos -primer/  Commented [A9]: we definitely do not '\n",
      " 'follow such a \\n'\n",
      " 'methodology when we create ontologies :)  \\n'\n",
      " 'Commented [A10R9]: Good remark, that seems like something \\n'\n",
      " 'that should be discussed towards D2.2. I will however leave it as it is \\n'\n",
      " 'for now.  \\n'\n",
      " 'Commented [A11]: what we reported about the reuse of \\n'\n",
      " 'existing standards should be properly reported in the deliverable. \\n'\n",
      " 'd2.2  \\n'\n",
      " 'Commented [A12R11]: @giorgia.lodi@cnr.it can you please \\n'\n",
      " 'provide us with a link on wher e we can find this?  \\n'\n",
      " 'Commented [A13]: what is a vocabulary document?  \\n'\n",
      " 'Commented [A14R13]: please refer to the glossary, I will \\n'\n",
      " 'change this to vocabulary  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '34 ● Design documents (draft domain model, design data specificatio n, etc.) '\n",
      " 'are published with \\n'\n",
      " 'each new version on the dedicated online repository as well as on relevant '\n",
      " 'standards \\n'\n",
      " 'registers such as https://purl.eu/ . The latter  always refers to the most '\n",
      " 'recent version.   \\n'\n",
      " '● Final domain models, in the case of standard semantic  data, are included '\n",
      " 'in the dedicated \\n'\n",
      " 'online  repository.   \\n'\n",
      " '● All interested parties can provide feedback on the ontology and the '\n",
      " 'developed specifications. \\n'\n",
      " 'This can be done via an easy -to-use and publicly accessible mailing list '\n",
      " 'and / or issue log, which \\n'\n",
      " 'is kep t in a GitHub repository.   \\n'\n",
      " '● Publish design documents for each new version on the dedicated online '\n",
      " 'repository.  \\n'\n",
      " ' \\n'\n",
      " 'Generation of the data specification and documentation  \\n'\n",
      " 'A specification is a technical document that gives substance to the ontology '\n",
      " '. Specification s \\n'\n",
      " 'can be adjusted based on advanced insight without changing the corresponding '\n",
      " 'ontology . It \\n'\n",
      " 'is often difficult to distinguish a specification from the ontology  itself. '\n",
      " 'Typical examples in this \\n'\n",
      " 'regard are PDF -A, DCAT and RDF. In some cases, multiple specifications are '\n",
      " 'part of a n \\n'\n",
      " 'ontology . These specifications then each give a domain -specific '\n",
      " 'interpretation to the \\n'\n",
      " 'ontology . An example of this are the INSPIRE Data Specifications, which '\n",
      " 'provide a domain -\\n'\n",
      " \"specific interpretation of the 'INSPIRE Implementing Rules' (the standard) \"\n",
      " 'for each of the \\n'\n",
      " 'INSPIRE themes.  \\n'\n",
      " 'To give an example on how to generate data specification and documentation, '\n",
      " 'the following \\n'\n",
      " 'method and toolchain  were developed in OSLO  and can be reused26. This '\n",
      " 'method uses the \\n'\n",
      " 'Resource Description Frame (RDF) as the underlying data model but  can also '\n",
      " 'be serialized to \\n'\n",
      " 'a traditional XML.  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '26 Related to the OSLO Toolchain there are some identified obstacles, these '\n",
      " 'are further \\n'\n",
      " 'described in ’Identified Challenges and proposed solutions’  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '35 All models are modeled in  UML. All models are managed in a central \\n'\n",
      " 'repository to expose dependencies and perform quality control. You can \\n'\n",
      " 'find the most recent version on GitHub.   \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " 'The UML models are transformed into an ontology in Turtle27 syntax, using \\n'\n",
      " 'RDF, the EA tool to -RDF. This is the machine -readable version of a \\n'\n",
      " 'vocabulary. In addition, this tool can export a UML model of an '\n",
      " 'application \\n'\n",
      " 'profile to a .TSV file for further processing. More information about the '\n",
      " 'too l \\n'\n",
      " 'and the configuration used for the models can be found on GitHub. \\n'\n",
      " 'Furthermore, the output is easy to convert to other formats such as XML or \\n'\n",
      " 'JSON.   \\n'\n",
      " ' \\n'\n",
      " 'Based on this machine -interpretable version of the vocabulary, a number \\n'\n",
      " 'of artifacts are then generated w ith a Specification Generator  tool. \\n'\n",
      " 'Successively with this tool the following documents are generated: HTML \\n'\n",
      " 'version of the vocabulary, HTML version of the application profile and the \\n'\n",
      " 'JSON -LD context definition. An XML schema can also be defined from '\n",
      " 'this.   \\n'\n",
      " ' \\n'\n",
      " 'The output is subjected to a review using the Ontology Pitfall Scanner28, \\n'\n",
      " 'Turtle syntax validator  29and JSON -LD validator30. The models together '\n",
      " 'with \\n'\n",
      " 'their documentation are then published on the Vocabularies GitHub \\n'\n",
      " 'repository, after which they are also made available automatic ally via '\n",
      " 'the \\n'\n",
      " 'dedicated GitHub repository.   \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '27 https://www.w3.org/TR/turtle/  \\n'\n",
      " '28 http://oops.linkeddata.es/response.jsp#  \\n'\n",
      " '29 http://ttl.summerofcode.be/  \\n'\n",
      " '30 https://json -ld.org/playground/  \\n'\n",
      " ' \\n'\n",
      " 'Modelling  \\n'\n",
      " 'Transformation  \\n'\n",
      " 'Generating \\n'\n",
      " 'specifications \\n'\n",
      " 'and artifacts  \\n'\n",
      " 'Publication  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '36 Management of issues and errors  \\n'\n",
      " 'All interested parties must be given the opportunity to log issues related '\n",
      " 'to the ontology  and the \\n'\n",
      " 'specification. This must be done in an open and t ransparent manner.   \\n'\n",
      " 'The product owner of the ontology monitors the issues. This means he is '\n",
      " 'responsible for answering \\n'\n",
      " 'questions and, where necessary, calling in experts to answer specific '\n",
      " 'questions. Furthermore, it is also \\n'\n",
      " 'the product owner who, based on the frequency and / or the criteria with '\n",
      " 'regard to new releases of \\n'\n",
      " 'the ontology , as stated in the Working Group Charter, convenes the members '\n",
      " 'of the thematic working \\n'\n",
      " 'groups to discuss the issues and changes to the ontology  and to prepare its '\n",
      " 'specification (see change \\n'\n",
      " 'management).  \\n'\n",
      " 'Changes to the ontology must be documented on a webpage that was provided '\n",
      " 'for this task. The \\n'\n",
      " 'minimum information per release includes:   \\n'\n",
      " '●        The date of the release.   \\n'\n",
      " '●        A textual description of the change.   \\n'\n",
      " '●        Where possible, references to the issues that were dealt with and '\n",
      " 'processed as part of the \\n'\n",
      " 'release.   \\n'\n",
      " ' \\n'\n",
      " 'Lifecycle of a n ontology  \\n'\n",
      " 'The life cycle of a n ontology , and the status that the  ontology has in '\n",
      " 'the registry is based on the W3C \\n'\n",
      " 'Recommendation Track31. The table below provides an overview of the life '\n",
      " 'cycle of a n ontology  and \\n'\n",
      " 'the link with process and method.   \\n'\n",
      " 'Lifecycle phase  Publication status   Process   Method   \\n'\n",
      " 'N / A   Working Group Charter32 Registration of a n \\n'\n",
      " 'ontology  Setting up a Working Group \\n'\n",
      " 'Charter  \\n'\n",
      " 'In development   Draft document33 Development of a \\n'\n",
      " 'specification  Development of a domain \\n'\n",
      " 'model  \\n'\n",
      " 'In treatment   Candidate ontology34   \\n'\n",
      " 'Change Management   \\n'\n",
      " 'Generate data specification \\n'\n",
      " 'and documentation  \\n'\n",
      " 'In use   Recognized o ntology  \\n'\n",
      " '(+ Errors)  35 \\n'\n",
      " 'In revision   Draft document   Development of a \\n'\n",
      " 'specification  Management of issues and \\n'\n",
      " 'errors  \\n'\n",
      " ' \\n'\n",
      " '31 https://www.w3.org/2017/Process -20170301/  \\n'\n",
      " '32 Analog to the W3C Working Group Charter  \\n'\n",
      " '33 Analog to the W3C Working Draft  \\n'\n",
      " '34 Analog to the W3C Candidate Recommendation  \\n'\n",
      " '35 Analog to the W3C Recommendation  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '37 Candidate Revised Ontology36 \\n'\n",
      " 'Phased out   Phased -out ontology37 Phasing out a n \\n'\n",
      " 'ontology   N / A  \\n'\n",
      " 'Working group \\n'\n",
      " 'ended (work \\n'\n",
      " 'stopped before \\n'\n",
      " 'the \"in use\" phase \\n'\n",
      " 'was reached)   Working group Report   N / A  N / A  \\n'\n",
      " ' \\n'\n",
      " 'Criteria for promotion to candidate ontology  \\n'\n",
      " '● All documented use cases and requirements in the Workgroup Charter at the '\n",
      " 'start of the work \\n'\n",
      " 'have been met.   \\n'\n",
      " '● Any changes in dependencies were documented.   \\n'\n",
      " '● The criteria for evaluating the implementation experience were defined and '\n",
      " 'approved by the \\n'\n",
      " 'Peer Review Community  (eg, minimum two implementations or proof -or- '\n",
      " 'concepts ).   \\n'\n",
      " '● A deadline for giving feedback must be specified.   \\n'\n",
      " '● Demonstrate that the specificati on has already been assessed by a wide '\n",
      " 'audience based on \\n'\n",
      " 'those involved in the working group and receive feedback via the mailing '\n",
      " 'list and / or issue \\n'\n",
      " 'log.  \\n'\n",
      " '● Certain data entities may be labeled as \"at risk\". These may be removed '\n",
      " 'before the candidate \\n'\n",
      " 'ontolog y is promoted to ontology .  \\n'\n",
      " 'Criteria for promotion to candidate ontology  \\n'\n",
      " '● All issues that have been documented must be processed.   \\n'\n",
      " \"● There must have been 'sufficient' implementation experience during the \"\n",
      " 'public review period.   \\n'\n",
      " '● The final specification may not contain significant differences in '\n",
      " 'relation to the candidate \\n'\n",
      " 'ontology . \\n'\n",
      " '● The Peer Review Community  has approved the promotion to ontology .  \\n'\n",
      " '● A place (eg  GitHub) is specified to keep track of errors and issues after '\n",
      " 'publication as a n \\n'\n",
      " 'ontology .  \\n'\n",
      " '● A product owner has been specified who is responsible for change '\n",
      " 'management.   \\n'\n",
      " ' \\n'\n",
      " 'Toolchain  \\n'\n",
      " 'The  toolchain used under  the OSLO project  toolchain in itself is '\n",
      " 'primarily open -source, thus providing \\n'\n",
      " 'a flexible framework for software development. However, it incorporates a '\n",
      " 'commercial tool, \\n'\n",
      " 'Enterprise Architect (EA), which , although  is in itself  a closed system . '\n",
      " 'The reason that the toolchain i s \\n'\n",
      " 'based on Enterprise Architect , is because it is recognized for its cost '\n",
      " '-effectiveness in comparison to \\n'\n",
      " 'other commercial tools available in the market  and allows to provide '\n",
      " 'semantic assets to the ontology, \\n'\n",
      " ' \\n'\n",
      " '36 Analog to the W3C Revising a Recommendation  \\n'\n",
      " '37 Analog to the W3C Obsoleted or Rescinded Recommendation  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '38 something that usually is not possible within open-source  programs which '\n",
      " 'are often primarily used \\n'\n",
      " 'for drawing and semantic information has to be defined separately.  \\n'\n",
      " 'The current implementation of the toolchain has many strengths. It is '\n",
      " 'capable of managing a \\n'\n",
      " 'decentralized knowledge base, caters to  a diver se group of editors with '\n",
      " 'varying expertise , and allows \\n'\n",
      " 'the collective creation and  discussion of a vast volume of documents. '\n",
      " 'Furthermore, it is integrated \\n'\n",
      " 'with a publication platform governed by a stringent flow, ensuring the '\n",
      " 'coherence and integrity of the  \\n'\n",
      " 'created content.  \\n'\n",
      " 'However, the toolchain does face ch allenges. The diverse contributions can '\n",
      " 'lead to conflicts, making \\n'\n",
      " 'it difficult to achieve a consistent version of the OSLO graph. '\n",
      " 'Additionally, while standardizing an \\n'\n",
      " 'intermediary format is possible, it could require significant modifications '\n",
      " 'to existing tools, raising \\n'\n",
      " 'concerns about their adoption. Despite these challenges, the toolchain is '\n",
      " 'noted for its simplicity, \\n'\n",
      " 'stability, and clarity . These characteristics  distinguish it from other '\n",
      " 'standards like LinkML.  \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '39 Peer review community  \\n'\n",
      " 'Context  \\n'\n",
      " 'Citizens and businesses across Europe expect consistent and efficient '\n",
      " 'services from their \\n'\n",
      " 'respective governments, similar  to the standards  and ontologies  followed '\n",
      " 'in the private \\n'\n",
      " 'sector. Governments throughout Europe provide a wide range of public '\n",
      " 'services, supported \\n'\n",
      " 'by various specialized applications from different software suppliers. '\n",
      " 'However, the data \\n'\n",
      " 'within these applications is often modelled  from specific perspectives, '\n",
      " 'hindering its reuse \\n'\n",
      " 'across different applica tions and processes. Consequently, transforming the '\n",
      " 'data for reuse \\n'\n",
      " 'incurs high costs. This results in citizens and businesses having to '\n",
      " 'repeatedly provide \\n'\n",
      " 'information, leading to duplicated investments, errors, and frustrations. '\n",
      " 'The objective is to \\n'\n",
      " 'establi sh greater coherence in system operations, improved semantic '\n",
      " 'understandability, and \\n'\n",
      " 'enhanced metadata findability, enabling easier access to data. Furthermore, '\n",
      " 'the use of \\n'\n",
      " 'technical standards for information exchange (APIs) helps avoid redundant '\n",
      " 'technical \\n'\n",
      " 'investments.  \\n'\n",
      " ' \\n'\n",
      " 'Order description, composition, and responsibilities  \\n'\n",
      " 'The peer  review community  can be an  existing governance body or a new '\n",
      " 'one  responsible for \\n'\n",
      " 'the central coordination and oversight of efforts related to standardizing '\n",
      " 'information across \\n'\n",
      " 'Europe . The activities involve standardizing the meaning (semantics), '\n",
      " 'syntax (grammar), \\n'\n",
      " 'technical standards  and ontologies  for information exchange, and metadata '\n",
      " '(\"data on data\"). \\n'\n",
      " 'To ensure stability and mutual consistency of ontologies , a generic '\n",
      " 'development an d change \\n'\n",
      " 'process is employed.  \\n'\n",
      " 'The development process should be  based on international standards, '\n",
      " 'guarantee sufficient \\n'\n",
      " 'support from stakeholders and provide for coordination with experts , both '\n",
      " 'within their own \\n'\n",
      " 'organization and from the professional field. All European governments have '\n",
      " 'the option to \\n'\n",
      " 'participate in the development process.  \\n'\n",
      " 'It is also advised for a formal process to be set up to change ontologies  '\n",
      " 'maintained at federal \\n'\n",
      " 'or regi onal entities , or local authorities. Changes can have a major '\n",
      " 'impact on existing \\n'\n",
      " 'information systems and must therefore be carefully evaluated. A r egistry  '\n",
      " 'should  be \\n'\n",
      " 'established, overseen by agreements made within the endorsement group '\n",
      " 'regarding its \\n'\n",
      " 'managem ent. In addition, the working group is responsible for monitoring '\n",
      " 'international \\n'\n",
      " 'standards that have an impact on European governments and monitoring the '\n",
      " 'generic \\n'\n",
      " 'development and change process.  \\n'\n",
      " 'The various ontologies  (such as MareGraph) are developed in su b-working '\n",
      " 'groups that are of \\n'\n",
      " 'a temporary nature. In addition, the following actions are carried out in '\n",
      " 'temporary sub -\\n'\n",
      " 'working groups: (1) drawing up a generic development and change process for '\n",
      " 'ontologies  \\n'\n",
      " 'managed by federal or regional entities and local au thorities and '\n",
      " 'submitting them to the \\n'\n",
      " 'endorsement group for approval, (2) draw up a procedure for the recognition '\n",
      " 'of ontologies  \\n'\n",
      " 'and submit it for approval and (3) define and set up a registry .  \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '40 Each participant within the peer review community who is resp onsible for '\n",
      " 'one or more sub -\\n'\n",
      " 'working groups is responsible for the coordination, follow -up and '\n",
      " 'implementation of the \\n'\n",
      " 'instructions and agreements of the data standards working group within his / '\n",
      " 'her sub -working \\n'\n",
      " 'group.  \\n'\n",
      " 'Reporting  \\n'\n",
      " 'The review community  provides regular reports on the progress of '\n",
      " 'ontologies  development \\n'\n",
      " 'to the relevant European oversight committee at their meetings.  \\n'\n",
      " '  \\n'\n",
      " 'Assignment description of the peer review community   \\n'\n",
      " '  \\n'\n",
      " 'The peer review community in ontology development  will focus  on how ensur '\n",
      " 'ing the quality, \\n'\n",
      " 'relevance, and applicability of ontologies through rigorous quality '\n",
      " 'assurance measures, \\n'\n",
      " 'consensus -building, and alignment with working group objectives. It is also '\n",
      " 'involved in \\n'\n",
      " 'handling appeals, conducting periodic a ssessments, and endorsing the final '\n",
      " 'version of \\n'\n",
      " 'ontologies for publication. In general, they will play an important role in  '\n",
      " 'the governance of \\n'\n",
      " 'ontology development and the management of semantic assets . \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '41 Exploring the Landscape: Perceptions and Views on E U-\\n'\n",
      " 'wide Ontology Creation  \\n'\n",
      " ' \\n'\n",
      " 'OSLO Toolchain  \\n'\n",
      " 'The toolchain, while robust and flexible, does have certain limitations. The '\n",
      " 'inclusion of the \\n'\n",
      " 'commercial tool, Enterprise Architect (EA), can be a hurdle for some users, '\n",
      " 'particularly those \\n'\n",
      " \"who prefer open -source alt ernatives. EA's nature as a closed system can \"\n",
      " 'limit adaptability and \\n'\n",
      " 'customization, contrasting the otherwise open -source nature of the '\n",
      " 'toolchain.  \\n'\n",
      " 'Another limitation lies in the management of a decentralized knowledge base. '\n",
      " 'While this \\n'\n",
      " 'setup allows for dive rse input and collaboration, it can lead to potential '\n",
      " 'conflicts due to the \\n'\n",
      " 'patchwork nature of document creation, impeding the formation of a unified '\n",
      " 'version of the \\n'\n",
      " 'OSLO graph.  \\n'\n",
      " \"Standardization also presents a challenge. While it's possible to \"\n",
      " 'standardize a n intermediary \\n'\n",
      " 'format, it may require significant adjustments to the existing tools. This '\n",
      " 'could lead to \\n'\n",
      " \"adoption hesitations among users. Lastly, the toolchain's focus on semantics \"\n",
      " 'can be limiting \\n'\n",
      " 'for users who are more oriented towards implementation, as business analysts '\n",
      " 'typically are \\n'\n",
      " 'not trained to look at semantics.  \\n'\n",
      " 'Linked Open Vocabularies (LOV)  \\n'\n",
      " 'Linked Open Vocabularies (LOV) come with a range of limitations that can '\n",
      " 'affect their efficacy \\n'\n",
      " 'and user experience. One of the most notable challenges is the q uality and '\n",
      " 'consistency of the \\n'\n",
      " 'vocabularies. Given that there are no universally accepted standards for '\n",
      " 'creating or \\n'\n",
      " \"implementing these vocabularies, it's common to find errors, \"\n",
      " 'inconsistencies, or lack of \\n'\n",
      " 'necessary detail in some vocabularies. This lack of uniformity also impacts '\n",
      " 'interoperability, a \\n'\n",
      " 'key aim of LOV. The differing standards and structures of vocabularies can '\n",
      " 'make successful \\n'\n",
      " 'interoperability a challenge.  \\n'\n",
      " 'Another key limitation of LOV is its complexity. The framework is built on '\n",
      " 'Semantic Web \\n'\n",
      " 'technologies, which can present a steep learning curve for those not '\n",
      " 'familiar with them. This \\n'\n",
      " 'complexity can act as a deterrent, hindering widespread adoption of LOV. '\n",
      " 'Additionally, LOV \\n'\n",
      " 'presents usability challenges, particularly for non -English speakers, as m '\n",
      " 'ost vocabularies are \\n'\n",
      " 'predominantly in English. Furthermore, the metadata provided by these '\n",
      " 'vocabularies is often \\n'\n",
      " 'incomplete, which can affect the discoverability and usability of the '\n",
      " 'vocabularies.  \\n'\n",
      " 'LOV also faces challenges in terms of scalability and maintenance. As the '\n",
      " 'amount of data \\n'\n",
      " 'within the ecosystem increases, managing, updating, and querying this data '\n",
      " 'can become \\n'\n",
      " 'increasingly challenging. Some vocabularies are not regularly updated or '\n",
      " 'maintained, leading \\n'\n",
      " 'to outdated or irrelevant information. Moreover, the LOV ecosystem heavily '\n",
      " 'depends on the \\n'\n",
      " 'community for development and maintenance, which may not always be reliable '\n",
      " 'or \\n'\n",
      " 'consistent. Lastly, ensuring privacy and security of data within this open '\n",
      " 'ecosystem can be \\n'\n",
      " 'challenging, adding t o the list of limitations of LOV.  \\n'\n",
      " '  \\n'\n",
      " 'D2.1 Guidelines on process and methodology for \\n'\n",
      " 'organisational interoperability (Version 1)  \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " ' \\n'\n",
      " '  \\n'\n",
      " ' \\n'\n",
      " '42  \\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"07d376b4bbce4baf8a746f8be5437d0d\"\n",
    "ENDPOINT = \"https://lucasaieastus2servicesopen.cognitiveservices.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        api_key=KEY,\n",
    "        api_version=\"2023-12-01-preview\",\n",
    "        azure_endpoint=ENDPOINT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"0:00\n",
    "When Russia launched its February 2022 invasion, one of its stated goals was the demilitarisation of Ukraine.\n",
    "0:05\n",
    "And indeed by June 2023, Russian state media ran a headline suggesting that goal had mostly been accomplished.\n",
    "0:12\n",
    "Shockingly enough however, more than a year after that headline the Russian military appears to still be locked in a grinding war\n",
    "0:18\n",
    "of attrition against the Ukrainian armed forces. Continuously applying firepower, manpower and materiel against Ukrainian defences\n",
    "0:25\n",
    "seemingly in order to slowly stretch, corrode and grind forward. It is an approach that has almost certainly destroyed far more Russian than Ukrainian equipment.\n",
    "0:33\n",
    "But despite the disparity, Ukrainian equipment losses have still been immense. And Russia certainly began the invasion with more equipment in hand.\n",
    "0:40\n",
    "It's a situation that raises a number of questions that I want to start looking at today. What have Ukrainian losses looked like? What sort of resupply have they received?\n",
    "0:48\n",
    "How is the force changing and is the equipment picture improving or degrading over time? I'll start by looking at the potential impact of equipment quality,\n",
    "What Am I Talking About?\n",
    "0:55\n",
    "especially as Ukraine goes over to using more and more NATO equipment. And then start diving straight into questions of quantity.\n",
    "1:02\n",
    "That'll mean looking at both the big picture of how Ukrainian losses have generally compared to Russian ones so far, and what the wider resupply picture looks like.\n",
    "1:09\n",
    "But also some deep dives into specific Ukrainian equipment categories: tanks, infantry fighting vehicles, APCs and aircraft.\n",
    "1:16\n",
    "Note that in this episode I won't be focusing either on air defence or artillery systems because they are likely to get their own coverage in the future.\n",
    "1:22\n",
    "But I will close out with some broader observations about the way the Ukrainian army's equipment might be changing over time.\n",
    "1:28\n",
    "And perhaps more importantly, whether it has enough of it. But before I start hitting you with an array of graphs and stats a word from a sponsor.\n",
    "1:36\n",
    "And today I get to welcome back long-term channel sponsor and my VPN of choice, Private Internet Access.\n",
    "1:41\n",
    "Whether you're absorbing YouTube content or trawling through satellite imagery, our modern online environment certainly has a lot to offer.\n",
    "1:48\n",
    "But it can also feel like an environment that is steadily becoming more fragmented, and one with some potentially very rough edges\n",
    "1:53\n",
    "when it comes to things like personal privacy and data breaches. When trying to look out for your privacy in that sort of environment\n",
    "1:59\n",
    "you might want to make sure you have the right combination of tools and practices. Much like the seat belts and safety systems on an airplane,\n",
    "2:05\n",
    "much of the time you might be fine without them. But if something strange does happen you are probably going to be happy they are there.\n",
    "2:11\n",
    "In an online environment that might mean things like watching where you browse, keeping strong passwords and maintaining a healthy distrust of things like links and attachments.\n",
    "2:19\n",
    "And then if you're like me, you'll also want a VPN. With a click what a VPN allows you to do is reroute your internet traffic,\n",
    "2:25\n",
    "change your publicly perceived IP address, while also potentially changing how your traffic is logged. Noting that Private Internet Access has a no logs policy\n",
    "2:33\n",
    "that's reportedly been both audited and tested in court. They offer servers in 91 different countries,\n",
    "2:39\n",
    "plus if you want a little extra precision there are also servers in 50 US states. All in the context of software which is advertised as being open source,\n",
    "2:47\n",
    "available cross platform and compatible with a range of streaming services. Plus you probably won't have to worry about juggling multiple accounts,\n",
    "2:53\n",
    "because a single subscription will cover an unlimited number of devices. So if you are interested, there'll be a link in the description that will give you access\n",
    "3:00\n",
    "to an 83% discount on a 2 year subscription, plus 4 bonus months, covering an unlimited number of devices and with a 30 day money-back guarantee.\n",
    "3:08\n",
    "So with my genuine thanks to a long running sponsor, let's get back to it. OK, so a couple of weeks ago (at time of recording) we looked at some of the\n",
    "Assessing Ukraine & Russia\n",
    "3:15\n",
    "available evidence for Russian heavy equipment losses in Ukraine, and tried to get an understanding of how deep their remaining reserves might go.\n",
    "3:21\n",
    "But in order to better understand where the balance of this war of attrition might drift long term, we probably have to repeat that exercise for the Ukrainians.\n",
    "3:29\n",
    "To a great extent, a lot of the harsh arithmetic of war here is going to be very similar. How much did the Ukrainians start with, how quickly\n",
    "3:35\n",
    "are they losing it compared to the invading Russians? And what does the pipeline look like in terms of both the quality and quantity of replacements?\n",
    "3:42\n",
    "But compared to that work on Russia, I think there's a couple of differences in Ukraine's situation that we have to try and reflect.\n",
    "3:47\n",
    "For example, comparatively speaking stored equipment is less of a concern for the Ukrainians than the Russians.\n",
    "3:52\n",
    "Compared to just about any other European state, Ukraine did begin the war with an immense amount of equipment in storage.\n",
    "3:58\n",
    "We are talking hundreds of tanks, airframes that could be reactivated, retired ammunition, etc.\n",
    "4:03\n",
    "And as you can see on the right there from that image (from I believe 2018) Ukrainian storage standards for some of that reserved equipment were almost Russian.\n",
    "4:12\n",
    "We've seen plenty of evidence that some of this equipment is being reactivated, both in terms of what we're seeing appear on the battlefields,\n",
    "4:18\n",
    "the announcements of the Ukrainian government, and also programs like the one you see on the right there. Where the Czech government announced that their industry would be assisting\n",
    "4:25\n",
    "the Ukrainians with reactivating old T-64s. If you want a very rough guesstimate for how many reserved main battle tanks\n",
    "4:32\n",
    "the Ukrainians have repaired or reactivated since February 2022, I'd put it in the low triple digits.\n",
    "4:37\n",
    "But compared to the Russian Federation, Ukraine has nowhere near the same breadth of stored equipment, nor the same reliance on it.\n",
    "4:43\n",
    "Ukraine doesn't have anything like Russia's stockpiles of old Soviet artillery systems for example.\n",
    "4:48\n",
    "Which means when we are modelling force endurance another factor has to come into play. Instead in Ukraine's case we have to put a much greater weighting on the importance of allied resupply.\n",
    "4:58\n",
    "This is obviously a factor for Russia too, especially when you are talking about things like drones from Iran or ammunition from North Korea.\n",
    "5:04\n",
    "But in Ukraine's case, what equipment is being donated and what equipment we expect them to receive in the future is a much more important part of the analysis.\n",
    "5:11\n",
    "For the purpose of today's episode, I am generally going to be counting any heavy equipment that has either been delivered or explicitly pledged as equipment that is supplied.\n",
    "5:20\n",
    "We are generally going to proceed on the assumption that Ukraine is going to receive everything that's been promised and nothing more.\n",
    "5:25\n",
    "I also won't be analysing NATO's equipment levels or storages this episode. Because frankly, with a couple of niche exceptions, the limitation on resupply to Ukraine\n",
    "5:34\n",
    "is almost always going to be a question of politics and priority more than raw availability. Across basically every equipment category, Ukraine's allies have access\n",
    "5:42\n",
    "to far, far more heavy equipment than Russia does. Colloquially we might refer to Russia and Ukraine as being low on equipment\n",
    "5:48\n",
    "once they are a couple of steps away from shovels and Molotov cocktails. Elsewhere, being \"short\" might mean not having a spare army or two's\n",
    "5:54\n",
    "worth of armoured vehicles parked in the desert like the United States. Or having some of the largest armoured forces in Europe\n",
    "6:00\n",
    "pointed at each other, as in the case of Greece and Türkiye. For Ukraine, often the question is not is there equipment out there,\n",
    "6:06\n",
    "it's whether they'll be given access to it. The other thing that sets Ukraine's equipment position apart is the unique role\n",
    "6:13\n",
    "played by arguably its greatest equipment donor, the Russian Federation. Captured equipment plays a much, much greater role for the Ukrainians than it does for the Russians.\n",
    "6:22\n",
    "That's true both because Ukraine had less equipment to begin with, and is also believed to have captured far more of it.\n",
    "6:27\n",
    "At time of recording, the Ukrainians are visually confirmed to have captured about 3,000 pieces of heavy equipment.\n",
    "6:33\n",
    "The number is about 1,000 going in the other direction. And over the course of the war, we've seen entire Ukrainian units\n",
    "6:38\n",
    "sometimes re-equipped with stuff of Russian origin. So whereas this was mostly an afterthought for our Russia analysis,\n",
    "6:44\n",
    "here it's going to be much more front and centre. Now before we start jumping into aggregate numbers around losses and resupply,\n",
    "The Quality Question\n",
    "6:50\n",
    "there's another side of the coin I wanted to explore first: quality. When we talked about Russian losses and resupply 2 weeks ago this question was really important\n",
    "6:58\n",
    "because over time Russia was using more and more older equipment. We had to understand the potential difference that using a T-62 or D-20 howitzer might make\n",
    "7:06\n",
    "over the much more modern designs that dominated the pre-invasion Russian force. A one-to-one quantitative replacement in those cases\n",
    "7:13\n",
    "may not mean a one-to-one replacement in combat power. With Ukraine the question is also important, but sometimes runs in the other direction.\n",
    "7:20\n",
    "What's the impact of going from a Soviet howitzer to a PzH 2000, or from a BMP to a Bradley?\n",
    "7:25\n",
    "It's been said that quantity can have a quality all of its own, but quality can also have a quality all of its own.\n",
    "7:31\n",
    "The basic question then is to what extent does that actually matter, or is a tank a tank, and a gun a gun?\n",
    "7:37\n",
    "Intuitively in some cases the answer is obviously going to be \"yes\", and in others \"not so much\". The impact might be intuitive when the high quality piece of equipment\n",
    "7:45\n",
    "allows you to do something that is significantly different to what the lower quality piece enabled. In a war with very slow moving front lines for example, range is an obvious example here.\n",
    "7:54\n",
    "If you need to hit targets 60 kilometres behind the line for example, no amount of systems with 40 kilometre ranges are going to allow you to service that target.\n",
    "8:02\n",
    "Longer-ranged artillery might have a massive advantage in counter-battery engagements, and longer ranged anti-aircraft systems might actually be able to hit\n",
    "8:09\n",
    "aircraft using stand-off weapons like glide bombs. In other cases, the impact of a quality differential might be much more narrow.\n",
    "8:15\n",
    "All of that however is just based on anecdotes and what is intuitive. If you are watching this channel however, you are probably interested in what the data\n",
    "8:22\n",
    "(as imperfect and incomplete as it might be) may be able to tell us about some of the impacts equipment quality can have for Ukrainian forces.\n",
    "8:29\n",
    "If you watch interviews or talk to some of the Ukrainian personnel that use some of the better NATO equipment, one thing they'll often seem to say\n",
    "8:36\n",
    "is that it's better designed from a survivability perspective. Yes, your vehicle might still be knocked out by a mine, an ATGM or a loitering munition,\n",
    "8:43\n",
    "but even if the vehicle's done, the crew might survive to fight another day. When you are talking about modern IFVs or tanks, that sort of feedback\n",
    "8:50\n",
    "might make a degree of sense from a design perspective, as Soviet engineers were often just working with a different design philosophy\n",
    "8:55\n",
    "and different design priorities to their NATO counterparts. And while we don't really have data from both sides on how many crew on average\n",
    "9:02\n",
    "survive whenever a given vehicle type is hit, we can kind of hunt for clues using the visually-confirmed loss data.\n",
    "9:08\n",
    "The best clue for us here on how likely a crew was to survive comes down to what a vehicle looks like when it becomes a loss.\n",
    "9:15\n",
    "Differentiating between vehicles that have been thoroughly destroyed as compared to those that have merely been damaged and knocked out\n",
    "9:21\n",
    "is one of the most subjective and difficult things when it comes to operating these VC databases. It's also always possible that a vehicle was initially lightly damaged,\n",
    "9:29\n",
    "and then later thoroughly destroyed by for example a follow-up drone attack. But all else being equal, I think it's a fairly safe assumption to make\n",
    "9:36\n",
    "that more crew and passengers will survive a vehicle being knocked out and looking like this afterwards, something that would be classified as damaged, abandoned and captured in the database.\n",
    "9:45\n",
    "As compared to a destroyed vehicle like this one where the burnt out husk of the hull is in Ukraine, and the turret may be exploring a new career as an anti-satellite weapon.\n",
    "9:53\n",
    "And while it's generally possible for any vehicle to end up anywhere on that spectrum from lightly damaged all the way through to catastrophically destroyed,\n",
    "10:00\n",
    "the data we have suggests that in Ukraine they don't do so at regular rates. What you're looking at on screen there now is a chart\n",
    "10:07\n",
    "showing Ukrainian visually confirmed losses for 3 infantry fighting vehicles, the BMP-1, BMP-2 and the American Bradley.\n",
    "10:15\n",
    "I chose these three vehicles because the BMP-1 and BMP-2 are the most common infantry fighting vehicles in Ukrainian service,\n",
    "10:22\n",
    "with the BMP-2 obviously being the somewhat better vehicle. While the Bradley was selected because it's the only Western infantry fighting vehicle\n",
    "10:28\n",
    "where the number of visually-confirmed losses (which is approaching 100 at this point) gives us a sample size large enough for this exercise to potentially be worth doing.\n",
    "10:36\n",
    "The colour coding on the bar splits those losses into three types. Blue are those that appear to have been at least lightly damaged.\n",
    "10:43\n",
    "Orange are those that have been damaged and then either abandoned or captured. Abandoned vehicles may or may not be recovered by either side.\n",
    "10:49\n",
    "But the key point is the crew may have had more chance to survive. While the red represents those that are classified as destroyed.\n",
    "10:56\n",
    "Just in case that previous graph was a little hard to read because more BMP-1s have been lost than the other two types,\n",
    "11:01\n",
    "this is it expressed as a percentage of losses for each platform. Immediately you'll see something interesting, the distribution for the BMP-1 and BMP-2\n",
    "11:09\n",
    "(so more than 600 vehicles combined across those two categories) are pretty close. In both cases the blue bar is relatively small and the destroyed percentage is around 80%.\n",
    "11:20\n",
    "Meanwhile for the Bradley you see a lot less red and a lot more blue. And while the difference between blue and orange might matter if you're concerned\n",
    "11:26\n",
    "with whether or not the side permanently lost the vehicle as a result of the engagement, from a crew survivability perspective, it might be useful\n",
    "11:33\n",
    "to group them together as non-catastrophic losses. And that's exactly what you can see here with all those damaged,\n",
    "11:39\n",
    "abandoned and captured categories grouped together into the green bar, while the destroyed vehicles are marked in red.\n",
    "11:44\n",
    "I need to stress this is in no way a perfect proxy for crew survivability, but the pattern there is fairly stark.\n",
    "11:50\n",
    "If you are in a Ukrainian BMP-1 or BMP-2, and you become a visually-confirmed loss,\n",
    "11:56\n",
    "there's about an 80% chance that you are in a vehicle that is going to be thoroughly destroyed. If you are in a Bradley, then statistically the destroyed percentage drops below 50%.\n",
    "12:05\n",
    "That means that if this was a perfect proxy and everyone in a damaged vehicle tended to survive while everyone in a destroyed vehicle tended to not,\n",
    "12:12\n",
    "your odds of survival for those experienced IFV operators or the infantry carried would be more than 2.5 times higher in an American Bradley than it would be in a BMP-1 or 2.\n",
    "12:23\n",
    "And that's before we factor in things like the fact we often see infantry riding on the top of the BMP-1 or 2 rather than in the infantry compartment.\n",
    "12:30\n",
    "Which means you absolutely will see some drone footage where the infantry riding on top become casualties without the vehicle being knocked out.\n",
    "12:37\n",
    "The problem with trying to do this sort of analysis for other NATO vehicles that Ukraine has received is that often the sample sizes are just too low.\n",
    "12:44\n",
    "And that leaves the data vulnerable to being skewed by a range of factors that might have nothing to do with the basic survivability of the vehicle.\n",
    "12:51\n",
    "But because those factors can also be interesting, let me quickly tell you a story of three tanks. Over the course of the war Ukraine has received hundreds of tanks from its allies,\n",
    "13:00\n",
    "including relatively small numbers of some of NATO's best. These include the British Challenger 2, the American M1A1 Abrams,\n",
    "13:07\n",
    "and some later versions of the German Leopard, including the 2A6 from Germany itself, and the 122 from Sweden.\n",
    "13:14\n",
    "You could occupy internet experts for days at a time with debates over the comparative virtues of these vehicles.\n",
    "13:20\n",
    "To give a non-tanker's opinion, they are all capable, established MBT designs. But if you expect age to correspond with survivability,\n",
    "13:27\n",
    "you'd probably give the advantage to the German Leopard 2A6, as well as the Swedish version which is related to the earlier 2A5\n",
    "13:33\n",
    "with a major focus on vehicle and crew survivability. Then you might look at the visually confirmed loss data and quickly conclude\n",
    "13:39\n",
    "that those Leopard 2A6s and Strv 122s are obviously death traps. With 18 examples appearing in the loss data against about 31 supplied.\n",
    "13:48\n",
    "The loss rate for the M1A1 Abrams is much lower at around 30%. And the British Challenger 2 looks absolutely incredible,\n",
    "13:56\n",
    "with more than 90% of the fleet still undamaged. So does this mean we should all start yelling Rule Britannia,\n",
    "14:03\n",
    "and standardise on the Challenger 3 as NATO's new main battle tank? Well, before you get too excited and start trying to figure out where the Poms will fit\n",
    "14:09\n",
    "their new tank mega-factory, we can flip the graph into one categorising the type of loss. It's suddenly the Swedish Strv 122 which is by far the most survivable vehicle in our set.\n",
    "14:20\n",
    "While there are probably VBIEDs out there with better graphs than the Challenger 2, with 100% of Challenger 2 losses being catastrophically destroyed rather than damaged.\n",
    "14:29\n",
    "If you fed that figure to an AI with no other context, it might conclude that the Challenger 2 is a less survivable vehicle than the BMP-1.\n",
    "14:36\n",
    "So what's going on here? Issue one is that the bar says 100% because only one\n",
    "14:42\n",
    "Challenger 2 has been lost in Ukraine, and it looked like this. Problem two is that when only a few vehicles are involved rather than hundreds or thousands,\n",
    "14:50\n",
    "they tend to be concentrated in specific units and potentially used in different ways. Ukraine is believed to have received 14 Challenger 2s from the United Kingdom,\n",
    "14:57\n",
    "and they appear to have gone to the 82nd Air Assault Brigade. There have been a couple of interviews with tankers from the 82nd\n",
    "15:03\n",
    "who have described how they have generally used the Challenger 2 and how they evaluate it compared to other tanks.\n",
    "15:09\n",
    "Most them praise the combination of accuracy and protection on the Challenger 2, and described using it for relatively long-range engagements.\n",
    "15:15\n",
    "The one on the right there for example, describes engaging Russian defensive positions from ranges of between 2.6 and 2.8 kilometres.\n",
    "15:22\n",
    "That is a long way from maximum range for potential tank engagements, but it's also not exactly urban warfare.\n",
    "15:28\n",
    "Another interview I found talked about preferring the lighter T-80 for close-in assault work due to a combination of factors like the Challenger's weight on one hand\n",
    "15:36\n",
    "and its comparative accuracy on the other. Whatever the case may be, it sounds like you have the entire tank force\n",
    "15:42\n",
    "concentrated in a single veteran unit that, at least anecdotally, talks about using them in a relatively conservative fashion.\n",
    "15:48\n",
    "And there might be some threats that you are comparatively less likely to encounter if you're mostly doing long-range work with a vehicle, think anti-tank mines.\n",
    "15:55\n",
    "Meanwhile, a lot of the Leopard 2A6s found themselves being used as the spearhead of the Ukrainian offensive in June 2023.\n",
    "16:02\n",
    "They were largely handed over to mechanised units, and used for some of the most difficult jobs a unit could be assigned: breaching prepared enemy defences under fire.\n",
    "16:10\n",
    "In a sense you could argue this represented just another example of a phenomenon we've seen many times before on both the Russian and Ukrainian sides.\n",
    "16:17\n",
    "Some of the better available equipment being allocated to do the most dangerous and difficult of jobs. But without sufficient engineering assets to clear the minefields,\n",
    "16:25\n",
    "no ATACMS to suppress Russian attack helicopters, and all of the other factors that went into limiting the amount of progress that Ukrainian offensive made,\n",
    "16:32\n",
    "a fair number of these very expensive tanks ended up being knocked out or destroyed relatively quickly.\n",
    "16:38\n",
    "Finally the loss picture for the M1 Abrams has been different again, and can best be described as bumpy.\n",
    "16:44\n",
    "The vehicle avoided taking any losses during the breaching phase of the Ukrainian 2023 offensive, because it hadn't yet arrived.\n",
    "16:50\n",
    "And even once they did arrive, the losses we've seen have mostly been very clustered. 10 Abrams have so far appeared on the visually confirmed loss list, compared to 32 supplied.\n",
    "16:59\n",
    "But most those losses appear to have been clustered in a relatively short span of time. When it appears they may have been committed to blunt a Russian offensive.\n",
    "17:07\n",
    "The reason I bring this up is because a) it's interesting. And b) because I think it serves as a warning against jumping to conclusions based on incomplete data.\n",
    "17:15\n",
    "And to suggest we should probably think twice before replacing all the modern main battle tanks in NATO inventories with vehicles\n",
    "17:21\n",
    "the data suggests are obviously much more survivable, like you know, the T-55. You can't always blindly trust the data.\n",
    "17:29\n",
    "So where does all that leave us if we are trying to assess what impact quality might have on Ukrainian combat power going forward?\n",
    "17:34\n",
    "Without a whole bunch of additional information, the safest conclusion is probably that it matters, but we can't be sure how much it matters.\n",
    "17:41\n",
    "Although I imagine that for Ukrainian troops who end up surviving because they were in the back of a Bradley rather than a BMP,\n",
    "17:46\n",
    "the answer is already, at least at a personal level, all the difference in the world. But now it's probably time to start talking about the quantity side of the resupply and loss picture.\n",
    "The Question Marks: Domestic & Captured\n",
    "17:55\n",
    "And in order to do that there's a couple of assumptions I need to advance up front. All of which relate to some of the biggest unknowns when it comes to equipment resupply.\n",
    "18:03\n",
    "Firstly, we generally don't have very good public estimates of how much equipment the Ukrainians themselves are producing or reactivating.\n",
    "18:09\n",
    "We know that budget is being allocated to it and that it is happening, but other than a couple of cases where I'll try and reverse engineer out\n",
    "18:16\n",
    "a rough estimate of Ukrainian production, for the most part most of the figures you are about to see assume the Ukrainian\n",
    "18:22\n",
    "defence industrial base has been on extended vacation since February 2022. Assumption two relates to how we count captured Russian equipment.\n",
    "18:29\n",
    "Especially in the earlier stages of the war, this may have been Ukraine's most significant source of heavy equipment resupply.\n",
    "18:35\n",
    "And with nearly 3,000 pieces of equipment marked as \"captured\" in the visually-confirmed loss databases at time of recording, we can't exactly ignore it.\n",
    "18:43\n",
    "At the same time, I think it would be massively over-optimistic to assume that every piece of equipment captured can be repaired,\n",
    "18:49\n",
    "put back into service and then maintained in service. So as in the past, when I am calculating inflows I am going to divide\n",
    "18:55\n",
    "the number of visually-confirmed captured vehicles by 3. Basically assuming that if you capture three T-80s for example,\n",
    "19:01\n",
    "you're probably going to be able to cobble together enough parts to operate at least one of them. Finally, there are often a whole range of ambiguities around\n",
    "19:08\n",
    "foreign military assistance and equipment deliveries. Some states provide military assistance, but never announce what equipment is in the packages.\n",
    "19:15\n",
    "Others might announce equipment but with ambiguous timelines, and it never being entirely clear what has or hasn't been delivered.\n",
    "19:21\n",
    "Unless otherwise noted, because we don't have any data to go on we are going to assume that secret deliveries don't exist.\n",
    "19:27\n",
    "We are also often going to be bundling together equipment that has been pledged with equipment that has already been delivered.\n",
    "19:32\n",
    "We'll break things out occasionally, but for the most part we are interested in the replacement pipeline, not just what's already arrived.\n",
    "Losses\n",
    "19:38\n",
    "OK, so with that discussion around quality complete, let's get into the much more data-friendly issue of quantities.\n",
    "19:44\n",
    "And the way I thought we'd do this is start with the big picture around Ukrainian losses, then the question of inputs, and then put it all together\n",
    "19:50\n",
    "for a more detailed look at specific equipment categories. Of course until the AI apocalypse plays out, all of that equipment is going to need people to operate it.\n",
    "19:58\n",
    "So I think it's worth putting a note up front about the much harder topic to assess: manpower losses and replacements.\n",
    "20:04\n",
    "We can be fairly confident that Ukraine has taken significant casualties over the course of this war. And earlier this year I pointed to some of the evidence that serious manpower shortages were in play.\n",
    "20:13\n",
    "Actions to address that manpower shortage were arguably significantly delayed, although eventually we did see changes to Ukrainian mobilisation rules.\n",
    "20:21\n",
    "They finally passed in Ukraine a couple of months ago at this point, and reports indicate that induction rates have now increased significantly.\n",
    "20:27\n",
    "Any determination of whether or not these changes are likely to be enough to meet Ukraine's manpower requirements probably needs to wait a few more months.\n",
    "20:34\n",
    "But I will note that in amongst all this we have seen some interesting initiatives, including the formation of what has often been called a \"Polish Legion\".\n",
    "20:41\n",
    "The name there is arguably a little bit of a misnomer, because this is intended to be a volunteer unit formed of Ukrainian men currently living in Poland.\n",
    "20:49\n",
    "Basically what the Polish government has said, to paraphrase Foreign Minister Sikorski, is that many of these Ukrainian men do want to serve, do want to help rotate their compatriots.\n",
    "20:57\n",
    "But many have also expressed concerns about things like training standards, or their ability to eventually re-join their families.\n",
    "21:03\n",
    "So to answer those concerns, the Poles have set up a volunteer unit that will be trained by Polish forces first and then deployed to Ukraine as a cohesive unit.\n",
    "21:11\n",
    "The recruits won't be broken up to be sent as individual replacements to various other units of the Ukrainian military, they'll stick together and fight as a unit.\n",
    "21:19\n",
    "And reportedly within a couple of days of the initiative being announced several thousand had already registered.\n",
    "21:24\n",
    "I think it'll be interesting to see going forward both whether or not the Polish model works, and whether other European states adopt it. To quote the Polish Foreign Minister,\n",
    "21:32\n",
    "\"If every European country did that, Ukraine would have several brigades.\" And one imagines that given these are volunteer units\n",
    "21:39\n",
    "that are meant to be properly trained before being deployed, they may have relatively good combat power compared to other green units.\n",
    "21:45\n",
    "But even if Ukraine is able to address some of its manpower shortages, materiel concerns are likely to remain acute.\n",
    "21:51\n",
    "The country began the war with less heavy equipment than the Russian Federation, significantly increased the size of its military without\n",
    "21:58\n",
    "a commensurate increase in the amount of heavy equipment available, and importantly for this part of the analysis, has also lost a lot of heavy equipment in nearly 2.5 years of fighting.\n",
    "22:06\n",
    "The first question then is: how much? What you're seeing on screen here are Ukrainian losses across\n",
    "22:11\n",
    "different equipment categories according to two very different sources. Those orange bars on the right are visually-confirmed losses, including vehicles\n",
    "22:19\n",
    "that were only damaged, which I think serves well as a kind of soft floor to the estimate. The red bars on the left represent an impossibly high upper-end estimate,\n",
    "22:27\n",
    "namely the claims of the Russian Ministry of Defence. And as you can see, there is a slight difference between them.\n",
    "22:34\n",
    "But I bring them up to illustrate a point around potential over-claiming. Both sides are likely to be over-claiming, that's pretty much\n",
    "22:40\n",
    "just the historical default for anyone fighting a war. But the data seems to suggest that the scale at which they are doing so is very different.\n",
    "22:48\n",
    "What you're seeing on screen there is a chart showing the relationship between the claims by both sides and the visual evidence for them.\n",
    "22:54\n",
    "That 15% for Russian aircraft claims for example, means that for every 100 Ukrainian aircraft that Russia\n",
    "23:00\n",
    "claims to have destroyed, we have evidence for about 15. On the other side, for every 100 Russian aircraft Ukraine claims\n",
    "23:06\n",
    "to have permanently grounded, we have evidence for about 33. The difference there is between over-claiming the evidence by about 3:1 and roughly 7:1.\n",
    "23:14\n",
    "For every 100 Russian rocket artillery systems the Ukrainians claim to destroy we have visual evidence for about 35.\n",
    "23:20\n",
    "An over-claim rate (or perhaps a better term here is actually \"evidence ratio\") of less than 3:1.\n",
    "23:25\n",
    "For the Russians, every 100 Ukrainian systems claimed will come with evidence of about 6, an evidence ratio of closer to 17:1.\n",
    "23:35\n",
    "Going forward, I'm mostly going to be using visually confirmed loss data for reasons I've discussed extensively before.\n",
    "23:40\n",
    "But I thought that discrepancy was interesting enough to be worth surfacing. Very generally speaking, Ukrainian claims tend to be likely high, but at least possible.\n",
    "23:50\n",
    "While if the Russian claims were true, the Ukrainians would have been defending the streets of their capital with thrown bricks and harsh language probably back in 2023.\n",
    "23:57\n",
    "What you're seeing on screen now then are the visually confirmed losses for both Russia and Ukraine across different equipment categories.\n",
    "24:03\n",
    "And I think the graph serves to highlight at least two observations. The first is that, leaving Russian losses aside for a moment,\n",
    "24:09\n",
    "this war has undoubtedly destroyed a massive amount of Ukrainian equipment. Nearly 1,000 infantry fighting vehicles, more than 860 tanks,\n",
    "24:17\n",
    "around 600 artillery systems and 500 APCs to name just a few. The second observation is that, while throughout this war\n",
    "24:25\n",
    "Russia has consistently lost heavy equipment at a much higher rate than the Ukrainians, the difference between the two isn't consistent across the categories.\n",
    "24:33\n",
    "You'll often hear that Russia has very generally had about 3 times the visually confirmed equipment losses as the Ukrainians over the course of the invasion.\n",
    "24:40\n",
    "But to an extent they haven't been losing the same stuff. For a lot of the real heavy-metal categories, think tanks, infantry fighting vehicles and AFVs,\n",
    "24:48\n",
    "the ratio is often closer to 4:1, for multiple rocket launchers around 7:1. And in the command and communication category nearly 15:1, 283 to 19.\n",
    "25:00\n",
    "But there are also categories, quite numerically significant categories, where Ukraine has been losing more vehicles than the Russians have.\n",
    "25:07\n",
    "The thing is, these tend to generally be comparatively lighter categories of equipment that NATO has been able to supply in large numbers.\n",
    "25:14\n",
    "For example, the Ukrainians have lost more than 480 infantry mobility vehicles (think things like Humvees) compared to the Russian 265, a ratio of 1.8:1.\n",
    "25:23\n",
    "The ratio for MRAPs, wheeled mine-resistant vehicles (think things like the MaxxPro or the Bushmaster) is closer to 4:1.\n",
    "25:30\n",
    "On one hand, as we'll discuss later, this might signal that Ukraine is comparatively short on heavy equipment and has to use more light stuff instead.\n",
    "25:37\n",
    "On the other hand, it also suggests that wider 3:1 ratio across all categories might not truly represent the scale of the difference we've seen so far.\n",
    "25:45\n",
    "If you are trying to win a long-term war of attrition, you probably don't want to be trading tanks and infantry fighting vehicles for Humvees and M113s.\n",
    "25:54\n",
    "But whatever the disparity between the two sides, the fact is, especially once you account for non-visually confirmed or non-combat losses,\n",
    "26:01\n",
    "those graphs show a huge amount of Ukrainian equipment that needed replacement. This brings us to the question of inputs that Ukraine has turned to to balance out their losses.\n",
    "Inputs\n",
    "26:09\n",
    "Overall, the most important source has almost certainly been deliveries from abroad. At this point dozens of nations have supported Ukraine with various financial and military resources.\n",
    "26:18\n",
    "Some have done so very loudly, giving us specific numbers and delivery timelines whenever new aid is forthcoming.\n",
    "26:24\n",
    "Others like Finland have been forthright about the fact that aid is being sent, but not about what's in each potentially explosive shipment.\n",
    "26:31\n",
    "And then somewhere in between there are those that appear to have tried to be transparent, but sometimes seem to have a little bit of trouble with the accounting.\n",
    "26:38\n",
    "With the US recently announcing that they'd found another roughly 2 billion US dollar over-count in the amount of equipment that has been sent to Ukraine,\n",
    "26:45\n",
    "because once again equipment appears to been valued based on how much it would cost to replace it with new equipment,\n",
    "26:50\n",
    "as opposed to how much the old stuff was actually worth. Understanding those ambiguities as best we can, the chart on screen\n",
    "26:57\n",
    "shows some rough estimates for how much equipment across various equipment categories is estimated to have been pledged or delivered to Ukraine from foreign sources.\n",
    "27:05\n",
    "As you can see, Ukraine has been receiving serious numbers of some lighter systems (like MRAPs, APCs) and infantry mobility vehicles (things like Humvees).\n",
    "27:13\n",
    "But also a fair amount of metal on those heavier line items. Overall the pledges amount to about 900 tanks, more than 1,000 infantry fighting vehicles,\n",
    "27:21\n",
    "3,500 armoured personnel carriers, and more than 5,000 infantry mobility vehicles.\n",
    "27:26\n",
    "Including 3,000+ Humvees. That might sound like a lot of Humvees, but we have to remember the US reportedly left the Taliban with about 12,000 of them.\n",
    "27:34\n",
    "The other key input we are concerned with capturing today is equipment the Ukrainians have managed to get from the Russians.\n",
    "27:40\n",
    "Even before the first heavy equipment deliveries began from NATO states, the tractor brigades were at it in Ukraine, and a significant\n",
    "27:47\n",
    "number of Russian vehicles found their colours turned. At time of recording, the estimated more than 2,900 pieces of captured equipment\n",
    "27:54\n",
    "include more than 500 tanks, 271 AFVs, 616 IFVs,\n",
    "28:00\n",
    "and an embarrassingly large number of command vehicles. But while I think it's important to model the fact that this mode of equipment acquisition\n",
    "28:08\n",
    "was no doubt significant for the Ukrainians, those figures don't include the divide by 3 adjustment that I would generally suggest.\n",
    "28:14\n",
    "And I think the data suggests that significant aid packages from the Russians may be less common going forward.\n",
    "28:20\n",
    "What you can see on screen here is the change in the number of tanks, IFVs and AFVs captured by the Ukrainians across three distinct periods.\n",
    "28:27\n",
    "What you can see on screen here are massive numbers for captured vehicles during that February to May 2022 period,\n",
    "28:33\n",
    "even larger numbers between May '22 and May 2023, and then low, or in some cases even negative numbers for May 2023 through '24.\n",
    "28:43\n",
    "I think the most likely explanation here is that these three time periods reflect very different stages of the war and different major events.\n",
    "28:49\n",
    "February to May 2022 is the era of the Kyiv Goodwill Gesture, where the Russians apparently demonstrated the scale of their goodwill\n",
    "28:56\n",
    "by leaving hundreds of armoured vehicles behind as they pulled out of Ukraine's north. The year between May 2022 and May 2023\n",
    "29:03\n",
    "includes such minor events as the Kharkiv Counter-Offensive. That saw Russian defence lines collapse, a number of major bases overrun,\n",
    "29:11\n",
    "with huge amounts of equipment left behind or vacuumed up in the process. Both of those represent periods of relatively rapid movement.\n",
    "29:18\n",
    "A time when vehicles might be left behind or overrun, and thus more likely to be captured. By contrast, that final 12 month period has been\n",
    "29:26\n",
    "overwhelmingly dominated by relatively static fighting. Vehicles being lost might be lost in the grey zone\n",
    "29:32\n",
    "or behind enemy lines, making them very difficult to recover. And during that period they were also probably more likely to be subjected\n",
    "29:38\n",
    "to follow-up attack by drop drones or FPVs, which we've often seen used to completely destroy vehicles in order to prevent them being recovered.\n",
    "29:46\n",
    "The collapse in the rate of equipment captured is so great, that with tanks you actually see a negative number, meaning the list got smaller over those 12 months.\n",
    "29:54\n",
    "This reflects the fact that over time you are constantly seeing efforts to try and find and remove duplicate entries from the databases.\n",
    "30:00\n",
    "And that where visual evidence emerges of what was presumed to be a captured vehicle being destroyed, for example by a follow-up FPV strike,\n",
    "30:07\n",
    "that vehicle will then if possible be reclassified from captured to destroyed. Normally that process is all but invisible in the data\n",
    "30:14\n",
    "because new losses being reported tend to be more than enough to compensate. But as capture rates dived over those 12 months, those corrections and changes became very visible.\n",
    "30:23\n",
    "The key takeaway is that Ukraine is probably even more dependent now on domestic production and allied deliveries than it was in 2022 or 2023.\n",
    "30:31\n",
    "The Russian military may have fronted the Ukrainians a \"Build Your Own Mechanised Force\" starter pack or two,\n",
    "30:36\n",
    "but it's hard to imagine capture rates climbing to those sort of levels again unless the lines become much more dynamic.\n",
    "30:41\n",
    "But now with some of that big picture around equipment losses and inflows in place, I want to start weaving all those items together and try to better understand\n",
    "30:49\n",
    "or at least guesstimate, how things might be netting out for different parts of the Ukrainian force. So to jump into some specific equipment categories,\n",
    "MBTs\n",
    "30:56\n",
    "if we are leaving artillery and air defence systems for a future episode I figured we could go for something of a heavy metal opening. So let's talk tanks.\n",
    "31:03\n",
    "At a big picture level, tanks have had a rather interesting war in Ukraine so far, especially from a PR perspective.\n",
    "31:08\n",
    "There was the early panic over whether this war signalled the end of the tank. And in 2024, we've seen time and time again that heavy armour\n",
    "31:15\n",
    "can be vulnerable to a range of systems including landmines, ATGMs and drones. I've heard testimony from people who have fought in Ukraine\n",
    "31:22\n",
    "that they as infantry don't want to be anywhere near the friendly tanks, because the sheer amount of shit they tend to attract.\n",
    "31:28\n",
    "But despite those drawbacks, both sides also continue to swear by them. We've seen them used defensively to stage armoured ambushes, offensively to lead assaults,\n",
    "31:37\n",
    "deployed as essentially very heavily armoured artillery systems, or used to pick apart defensive positions, fortifications and structures\n",
    "31:44\n",
    "before infantry move in for assault actions. In other words tanks in Ukraine have proven to be vulnerable,\n",
    "31:50\n",
    "but then again, so too is basically everything. And the combination of a big gun and hopefully decent sensors\n",
    "31:56\n",
    "on a mobile and well protected platform still seems to be something that the armies can appreciate. While there have obviously been some peaks and troughs,\n",
    "32:03\n",
    "overall Ukrainian tank loss figures show a degree of stability. What you can see on screen there are the approximate visually-confirmed losses\n",
    "32:09\n",
    "for the period May to June 2022, May to June 2023, and May to June 2024.\n",
    "32:15\n",
    "I've chosen these time periods to try and give a somewhat like-to-like comparison between the 3 years, while leaving aside the somewhat dramatic outlier\n",
    "32:22\n",
    "that was the very earliest stages of the invasion. What you can arguably see is that while there is an up-tick in the middle there,\n",
    "32:28\n",
    "which has probably corresponded with the beginning of Ukraine's 2023 offensive, the burn rate between the three periods doesn't appear to have swung that wildly.\n",
    "32:36\n",
    "What that might suggest to us (even though it's clearly not enough information in and of itself) is that unless the loss rates of the tanks that are active have been changing significantly over time,\n",
    "32:46\n",
    "the total size of the fleet has probably remained relatively stable. That would broadly align with a February assessment by the IISS that at the time of that report's writing,\n",
    "32:55\n",
    "\"We estimate that 2 years after the full-scale invasion, the number of MBTs in service with the Ukrainian armed forces remains near pre-war levels.\"\n",
    "33:03\n",
    "If that's true, the whole demilitarisation thing is clearly going super well. As we saw from those loss charts earlier, a stable Ukrainian MBT inventory\n",
    "33:11\n",
    "can't be because they're not losing tanks, they are, and they've lost a lot of them. Roughly 870 destroyed, damaged or captured as at time of recording.\n",
    "33:20\n",
    "But against those estimated losses there have also been significant inflows. About 900 main battle tanks have been pledged to Ukraine by its various allies,\n",
    "33:28\n",
    "of which about 2/3 are believed to have been delivered. For the most part, you can think of those pledges as falling into three very broad waves.\n",
    "33:36\n",
    "There was a massive initial surge of Soviet and post-Soviet designs that began quite early in 2022.\n",
    "33:42\n",
    "Then a push leading up to the 2023 offensive to get Ukraine at least some modern Western main battle tanks to augment its force.\n",
    "33:48\n",
    "And then what appears to basically be a numbers-making exercise to get Ukraine some older NATO tanks, specifically the Leopard 1.\n",
    "33:55\n",
    "Although a vast majority of those are being reactivated from storage and so haven't made it to the battlefield yet.\n",
    "34:00\n",
    "I also note there's sometimes a tendency in coverage to dismiss the importance of smaller announcements that we do see from time to time,\n",
    "34:06\n",
    "such as the recent report that The Netherlands and Denmark will be sending another 14 Leopard 2A4s to Ukraine this summer.\n",
    "34:12\n",
    "We are now at the point that over 100 Leopards 2s, or related designs like the Strv 122, have been pledged or delivered to Ukraine.\n",
    "34:19\n",
    "And for the most part that figure has been built up one small announcement at a time. While there have been a huge array of countries involved\n",
    "34:25\n",
    "in trying to get Ukraine more battle tanks, there are a couple of efforts that appear to have really moved the needle from a supply perspective.\n",
    "34:31\n",
    "The largest single line item for allied tank supply to Ukraine probably remains Poland's 250+ T-72s and 60 PT-91s.\n",
    "34:40\n",
    "The problem from the Ukrainian perspective is that, much like Russia's donations in 2022 and 2023,\n",
    "34:45\n",
    "it seems very unlikely we'll be seeing those sort of donated numbers from Poland again in the near future, as the country is now left waiting for deliveries of new main battle tanks such as its K2s from Korea.\n",
    "34:55\n",
    "The most common single type of NATO tank to be delivered by contrast, is likely to be the Leopard 1 with about 155 either in the pipeline or already delivered.\n",
    "35:04\n",
    "And of course we can't forget Moscow's generous contributions that we discussed earlier, although as per our stated assumptions,\n",
    "35:10\n",
    "I'm only going to assume about a third of them can be put into action. Put that all together and you get the chart you see here.\n",
    "35:16\n",
    "What I've done is broken up the visually confirmed losses and potential inflows, things like captured vehicles, domestic production or foreign support,\n",
    "35:23\n",
    "and divided those into Soviet, post-Soviet and related vehicles on the top and NATO-type vehicles on the bottom.\n",
    "35:28\n",
    "In part based on the data work we did back in the Russian losses episode, for illustrative purposes I very roughly estimated the number of Ukrainian tanks\n",
    "35:35\n",
    "reactivated from storage at about 200 since February 2022. Net that all together and you end up with Ukraine being up by about 470 tanks overall,\n",
    "35:44\n",
    "270 if you leave out the reactivation component. Adjust down for approximately 300 of the pledged tanks\n",
    "35:51\n",
    "being shown here not having been delivered yet, and you end up with the Ukrainian fleet being slightly negative if you don't account for reactivation, to slightly positive if you do.\n",
    "35:59\n",
    "In other words, you are at the sort of point where potential errors in every direction, be it losses that don't become visually confirmed,\n",
    "36:05\n",
    "or damaged vehicles that end up being repaired rather than written off, could fairly easily move the result either side of zero.\n",
    "36:12\n",
    "To me, what that suggests (and we could very easily be wrong) is that after nearly 2.5 years of hard attritional fighting\n",
    "36:19\n",
    "the active Ukrainian tank fleet is probably pretty close to where it began. With the biggest changes being the shift in the composition of the force, not the size of it.\n",
    "IFVS & APCS\n",
    "36:27\n",
    "Moving on, let's look at a core component of many mechanised forces in Ukraine, the infantry fighting vehicle.\n",
    "36:33\n",
    "Perhaps even more so than tanks, infantry fighting vehicles (especially the old BMP series) have been absolutely ubiquitous in both Ukrainian and Russian service.\n",
    "36:41\n",
    "They are intended to combine the ability to transport (or attempt to transport) infantry safely across a very dangerous battlefield\n",
    "36:47\n",
    "and pack a useful amount of firepower, often in the form of an auto-cannon and/or missile armament. By comparison, armoured personnel carriers like the American M113\n",
    "36:56\n",
    "would normally be expected to lean much more into the sort of battle taxi role. Providing some protection to the troops they carry,\n",
    "37:02\n",
    "but often having a significantly lesser armament. Both types of vehicles can serve to protect infantry from things like shrapnel.\n",
    "37:08\n",
    "But you probably don't want to be using them in the same sort of way that you would use the more heavily armed IFVs.\n",
    "37:13\n",
    "As you can see on screen there, Ukraine's loss patterns for the two vehicle types have changed in different ways over time. In both cases Ukrainian losses\n",
    "37:21\n",
    "in May to June 2023 leading into the Ukrainian counter offensive were significantly higher than in May to June 2022.\n",
    "37:27\n",
    "But the rate of increase was significantly different, APC losses more than tripled, while IFV losses increased by about 75%.\n",
    "37:34\n",
    "Then in May to June 2024 with Ukraine now back on the defensive, IFV losses would drop back a bit while APC losses would continue to grow.\n",
    "37:42\n",
    "Adjusting for the very different state of the war across those three time periods, what this would seem to suggest is that the Ukrainian IFV fleet\n",
    "37:49\n",
    "has probably been broadly stable between 2023 and 2024, assuming you are likely to lose fewer vehicles on the defence than the attack.\n",
    "37:57\n",
    "While all else being equal, you'd probably guess that the APC fleet has grown. And to test that idea a bit, we can start to pick apart\n",
    "38:03\n",
    "the visually confirmed losses and the estimated resupply. As with the tanks, what I've done on screen here is split up the visually-confirmed losses,\n",
    "38:10\n",
    "announced pledges or purchases, and an estimate of vehicles captured, into the Soviet/Russian/Ukrainian category on the top\n",
    "38:17\n",
    "and the NATO vehicle category on the bottom. What it shows is Ukraine falling short on replacing its losses\n",
    "38:23\n",
    "of Soviet infantry fighting vehicles while running a surplus on NATO ones. Ukraine is visually confirmed to have lost about 830 Soviet IFVs,\n",
    "38:30\n",
    "for an inflow of about 670. For NATO infantry fighting vehicles it's a loss of 137 against an inflow of 729.\n",
    "38:39\n",
    "I'd also note that as you see on screen there, the Ukrainian BMP fleet at least on paper, is only avoiding being in an even worse state,\n",
    "38:45\n",
    "because of an inflow of an estimated 205 recovered captured vehicles. A number which is only possible because Ukraine is visually confirmed\n",
    "38:53\n",
    "to have captured a staggering 616 from the Russians at time of recording. If you net those figures for Soviet and NATO IFVs together,\n",
    "39:01\n",
    "Ukraine ends up positive by about 430 vehicles. But if you then adjust down for the fact that potentially hundreds of these vehicles\n",
    "39:08\n",
    "may not have been delivered yet, you end up with a much more humble increase. More or less what the loss data appeared to be hinting at a moment ago.\n",
    "39:15\n",
    "While there were a lot of estimates involved in that calculation, I think what is abundantly clear is that whatever the size of the IFV inventory,\n",
    "39:22\n",
    "the composition of that inventory in Ukraine is absolutely changing over time. Replacements may not be enough to keep up with BMP losses,\n",
    "39:29\n",
    "while pledges of NATO infantry fighting vehicles have so far outstripped losses. There are several NATO IFV designs we have seen become more common over time.\n",
    "39:37\n",
    "These include the German Marder, of which Ukraine has already received more than 100 and will eventually receive 140 in total I believe.\n",
    "39:44\n",
    "The US-made Bradley has been even more significant and fought a number of high-profile engagements. Ukraine is believed to have received just over 300 Bradleys at this point,\n",
    "39:53\n",
    "and there's a question over how many more might be received in the future. The Swedish CV90 has appeared in much smaller numbers,\n",
    "39:59\n",
    "but I bring it up here because this is the vehicle with potentially the longest tail. With both the Bradleys and the Marders, generally what Ukraine expects to receive\n",
    "40:06\n",
    "is older reserved or reactivated stock. With the CV90 we've actually seen plans for some\n",
    "40:11\n",
    "new-build production that might sustain the force into the future. In that respect you might describe the CV90 as being for the Ukrainians\n",
    "40:18\n",
    "what BMP-3 is for the Russians, the modern option that isn't available in anywhere near sufficient numbers now,\n",
    "40:23\n",
    "but which will continue to trickle into the force long term because it's the one in active production. Although it has to be said there's a wide gap between the number\n",
    "40:31\n",
    "of BMP-3s that Russia is believed to be producing and the rate of anticipated CV90 production for Ukraine.\n",
    "40:37\n",
    "Before we move on, I do want to flag one potential anomaly that does come through in the IFV data. For most types of IFVs in Ukraine there tends to be a relatively intuitive relationship\n",
    "40:46\n",
    "between visually confirmed losses and the number of vehicles supplied. Three times as many Bradleys have been sent as Marders,\n",
    "40:52\n",
    "and Bradley losses are about 3.5 times Marder losses. The anomaly though comes in when we are talking about the Polish Wolverine IFV.\n",
    "40:59\n",
    "It was announced back in April 2023 that Ukraine would be getting 200 of these things, including 100 in the very short term.\n",
    "41:06\n",
    "We then did start to see images of the vehicles in Ukraine being apparently operated by Ukrainian troops.\n",
    "41:12\n",
    "So by those accounts the Wolverine should be the second most numerous NATO IFV in Ukrainian service, second only to the American Bradley.\n",
    "41:19\n",
    "But while we've seen 93 Bradley losses, 20 Marders and 6 CV90s, the total visually confirmed loss figure for the Wolverine is 4.\n",
    "41:28\n",
    "Unless the Poles have secretly found a way to manufacture IFVs out of adamantium, I'm not sure I have a ready answer to this quirk in the data.\n",
    "41:35\n",
    "There are any number of things that could be driving it, deliveries could be less than anticipated, the vehicles may have been allocated to less dangerous fronts,\n",
    "41:42\n",
    "been very lucky and not destroyed as often, or when destroyed not photographed as often. It may also simply have just been used in a very different way by the Ukrainians,\n",
    "41:51\n",
    "because while it does have an IFV-type armament, the Wolverine, unlike the Bradley, Marder, CV90 and BMPs, is a wheel vehicle not a tracked one.\n",
    "41:59\n",
    "But in the absence of a definitive answer, for now I'll just leave this mystery to the audience. While Ukrainian IFV losses and resupply figures have,\n",
    "42:07\n",
    "at least on paper, remained relatively closely balanced, those same figures for armoured personnel carriers very much haven't.\n",
    "42:13\n",
    "As you can see there, the picture for Soviet, Russian and Ukrainian designs, mostly things like BTRs, has remained relatively balanced.\n",
    "42:20\n",
    "But the visually confirmed loss of fewer than 300 NATO APCs has so far been balanced by announced inflows of more than 3,300.\n",
    "42:29\n",
    "Producing a total net on paper position of positive 3,060 vehicles.\n",
    "42:35\n",
    "Accounting for vehicles that may have been pledged but not yet delivered lowers that figure to around positive 1,700.\n",
    "42:41\n",
    "But hopefully we can all agree that even by the standards of this war, that's still a lot of vehicles.\n",
    "42:46\n",
    "And a lot of that figure has been made up by various versions of an old Cold War stalwart, the American-designed M113.\n",
    "42:53\n",
    "In terms of NATO APCs, the M113 is about as simple as it gets. In its basic version the thing is basically a metal box\n",
    "43:01\n",
    "to which you add some armour plate, an engine, and some tracks. Troops can then hide inside said metal box while it drives around,\n",
    "43:08\n",
    "because marching long distances in a heavy pack sucks, and so too does shrapnel. As to which of those two things sucks more,\n",
    "43:14\n",
    "that will probably come down to the individual infantryman you talk to. If you go through public Ukrainian reviews of the M113,\n",
    "43:21\n",
    "you'll find things they generally like, like its simplicity, and some things that are viewed less favourably.\n",
    "43:26\n",
    "In an ideal world this probably wouldn't be the vehicle they'd be getting, with a focus instead on more IFVs or modern APCs.\n",
    "43:32\n",
    "But as I previously indicated I think as far back as 2022, if Ukraine's allies were ever going to try and supply anywhere close\n",
    "43:39\n",
    "to the sheer number of armoured vehicles that the country likely needs, and even more try and do it without providing 30 distinct designs\n",
    "43:46\n",
    "thus extending the torture already experienced by Ukrainian maintainers, the M113 was always going to be one of the only potential answers.\n",
    "43:53\n",
    "Because basically everyone in NATO has them, they have a lot of them, and also basically no one wants them.\n",
    "44:00\n",
    "As you can see on screen there, Ukraine is believed to have received M113s from The Netherlands, Lithuania, Australia, Denmark, Germany, Poland, Spain,\n",
    "44:07\n",
    "the United Kingdom, Italy, Belgium, Luxembourg, and a casual 900 (including 300 medical vehicles) from the United States of America.\n",
    "44:16\n",
    "That's a lot of metal boxes, but to give a sense of perspective, it's still fewer than the more than 1,000 vehicles the US supplied to the Iraqis.\n",
    "44:23\n",
    "M113 is not a particularly modern, well armed or well protected vehicle. But you can potentially fit dozens or hundreds of the things into an aid package\n",
    "44:32\n",
    "that might otherwise pay for a handful of tanks, or maybe one or two air defence systems. And so we should probably expect to see more and more of them appearing in Ukraine as time goes on.\n",
    "44:42\n",
    "Not just as basic APCs, but also as an array of specialised vehicles. Especially since, as we'll come back to, you still see plenty of cases\n",
    "44:50\n",
    "of Ukrainian units having to use unarmoured civilian vehicles to do things like evacuate wounded when a dedicated armoured option would probably be preferable.\n",
    "44:58\n",
    "Ukraine likely has far more APCs than it started with, but they probably also wouldn't say no to a couple of thousand more.\n",
    "Aircraft\n",
    "45:04\n",
    "While the Ukrainian ground forces may be treading water in some equipment categories and potentially building up in others,\n",
    "45:10\n",
    "I think the data suggests that the Ukrainian Air Force is in a much more difficult position. If you just look at the headline picture, you might get the impression things are relatively neutral.\n",
    "45:19\n",
    "97 visually-confirmed aircraft losses at time of recording, up against slightly more than 130 aircraft delivered or pledged as resupply.\n",
    "45:27\n",
    "On the surface that might seem to suggest that the Ukrainian Air Force is in better condition now than when Russia invaded in February 2022.\n",
    "45:34\n",
    "But for a variety of reasons I'd suggest that's probably not the case. The Ukrainian Air Force has been flying hard and constantly for almost 2.5 years at this point.\n",
    "45:42\n",
    "Constantly displacing between different airfields trying to evade Russian long-range strikes. As a result that 97 figure should probably be considered a floor,\n",
    "45:50\n",
    "as other aircraft have likely been worn out or lost due to other factors. And likewise that 128 figure probably doesn't include some reactivated airframes.\n",
    "45:58\n",
    "In particular, things like Sukhoi 24s. But the two most significant asterisks I think are a) delivery timelines that we'll talk about in a moment.\n",
    "46:06\n",
    "And that secondly, losses and resupply aren't impacting different parts of the force evenly. On paper for example, Ukraine's force of its most numerous fighter aircraft,\n",
    "46:14\n",
    "the MiG-29, seems to basically be treading water. 30 MiG-29s have been visually-confirmed destroyed or damaged since February 2022,\n",
    "46:22\n",
    "while 27 MiG 29s are believed to have been delivered by Poland and Slovakia, although some of those were likely intended as parts donors.\n",
    "46:29\n",
    "In recent weeks Poland has raised the prospect of potentially sending another squadron of these aircraft to Ukraine, but likely only after it receives new aircraft to replace them.\n",
    "46:38\n",
    "That pushes any potential timeline well out, so I haven't included them here. One of the many challenges for the Ukrainian MiG-29 fleet\n",
    "46:45\n",
    "is that while replacements haven't quite covered visually-confirmed losses, the number of tasks being assigned to the fleet has likely increased over time.\n",
    "46:52\n",
    "We've seen the aircraft used to do things like hunt Russian drones, or deploy HARM anti-radiation missiles against Russian air defence systems.\n",
    "46:59\n",
    "And for a force that was estimated to have fewer than 40 active MiG-29 airframes at the time of the Russian invasion,\n",
    "47:05\n",
    "even just those tasks alone would likely be a significant ask for the force, given the length of the Ukrainian front line.\n",
    "47:10\n",
    "But as hard as things appear to have been for Ukraine's MiGs, the state of its Sukhoi 24 and 25 squadrons is likely even harder.\n",
    "47:18\n",
    "Pre-invasion, the Sukhoi 25 was an important ground attack aircraft for both the Ukrainians and the Russians.\n",
    "47:23\n",
    "Ukraine was estimated to have just over 30 active airframes of the type. And there are plenty of videos of Ukrainian pilots out there flying the type\n",
    "47:30\n",
    "at altitudes so low that you might think it's an air show manoeuvre. If not for the flashing lights on the radar warning system\n",
    "47:36\n",
    "and the fact the pilots appear to be firing off volleys of rockets. As a rule, the use of live ammunition is generally discouraged during air shows.\n",
    "47:44\n",
    "Remarkably, flying right up to the front line and then attacking with relatively short-range weapon systems appears to have been a highly dangerous exercise.\n",
    "47:51\n",
    "And Ukraine has suffered the visually-confirmed loss of 20 of the type since February 2022,\n",
    "47:56\n",
    "against maybe 4 pledged replacements from abroad. In proportional terms, the visually confirmed losses have been even higher\n",
    "48:03\n",
    "for Ukraine's fleet of Sukhoi 24 Fencers. These swing-wing strike aircraft appear to have been viciously hunted by the Russians\n",
    "48:09\n",
    "because they are the ones capable of carrying Storm Shadow missiles. A weapon which has caused a number of very high-profile inconveniences\n",
    "48:16\n",
    "for Russian forces, including the addition of some nice new ventilation holes to at least one Russian submarine.\n",
    "48:21\n",
    "19 Sukhoi 24s appear in the visually-confirmed loss database, and the only replacements are believed to be the reactivation of stored Ukrainian models.\n",
    "48:30\n",
    "In other words as best we can tell, Ukraine's fixed-wing attack aviation has been worn down over time to a fraction of its pre-war strength.\n",
    "48:38\n",
    "At least in raw quantitative terms, the offset for a lot of these losses is meant to be the American F-16.\n",
    "48:44\n",
    "From relatively humble beginnings, the number of pledged F-16 airframes has steadily climbed. The figure has reached roughly 95 at time of recording,\n",
    "48:52\n",
    "which, if they all arrived tomorrow, would be enough to replace every aircraft Ukraine is visually confirmed to have lost since February 2022.\n",
    "48:59\n",
    "However, as soon as you start to dive into the fine print that 95 figure starts to come down significantly.\n",
    "49:04\n",
    "Some of the airframes pledged for example, are only ever intended to be parts donors, not operational aircraft.\n",
    "49:10\n",
    "Another major issue is timelines. Belgium has pledged about 30 airframes with the first to be delivered this year, but the final ones may not arrive until the end of 2027.\n",
    "49:19\n",
    "Meaning the final deliveries are hopefully much more relevant to Ukraine's post-war security position than the current invasion.\n",
    "49:26\n",
    "There's a couple of times on this channel where we've discussed the debate around how much of a game changer F-16 is likely to be,\n",
    "49:31\n",
    "and how its capabilities measure up against the various Soviet and Russian aircraft in play. But wherever you stand on that particular debate, I think there's one area\n",
    "49:39\n",
    "where the potential impact of this platform in Ukraine is very clear. Whether you think F-16 is a bad plane, a good plane,\n",
    "49:45\n",
    "or god's gift to fighter pilots, the fact is it is a plane. And if it wasn't for the effort to train Ukrainian pilots and ground crews,\n",
    "49:52\n",
    "source these airframes and eventually get them to Ukraine, then the medium-term projection for the Ukrainian Air Force would probably be\n",
    "49:59\n",
    "that the Russians would succeed in grinding it down to combat ineffectiveness. Realistically for Ukraine at this point it isn't a choice between a fleet of F-16s,\n",
    "50:07\n",
    "or a fleet of Gripens, or a fleet of F-18s, or a fleet of F-35s (as hilarious as that would be),\n",
    "50:12\n",
    "it's a choice between an F-16 fleet and not having a fleet. Russia has spent almost 2.5 years putting fewer\n",
    "50:19\n",
    "than 100 Ukrainian aircraft on the visually-confirmed loss list. F-16 has the potential to replace a lot of them,\n",
    "50:25\n",
    "and with a platform compatible with a wider range of NATO weapon systems. Ukrainian pilots may not have a wonder weapon, but they will have a weapon.\n",
    "50:33\n",
    "While the Russians face the prospect of basically having to start the process of destroying the entire Ukrainian Air Force all over again.\n",
    "50:40\n",
    "The Russians have had a number of recent successes flying reconnaissance drones deep into Ukrainian territory and then launching pinpoint strikes with systems like Iskander.\n",
    "50:48\n",
    "But only time will tell whether the tempo of those strikes increases, decreases or remains about the same.\n",
    "50:53\n",
    "In terms of the combat power of the Ukrainian Air Force out into 2025, I think there are three other elements we need to cover.\n",
    "50:59\n",
    "The first is the arrival of potential enablers. The kind of systems that may not be the most numerous,\n",
    "51:04\n",
    "but might help Ukraine get the best out of the aircraft it has. The biggest example I can think of here is that Sweden has pledged\n",
    "51:10\n",
    "to provide Ukraine with two light AWACS aircraft. These won't have the same capabilities as the much larger AWACS aircraft flown by forces like the USAF,\n",
    "51:19\n",
    "but because they'll be under Ukrainian control they will be able to fly in Ukrainian airspace. And if they are deployed carefully, they might be fairly useful to Ukrainian defensive efforts.\n",
    "51:28\n",
    "The second big question is whether some long-hypothesised transfers will actually eventuate. We are talking for example about Mirage 2000s from France.\n",
    "51:36\n",
    "We've been told that some will eventually be coming, but not how many or how soon. There's also been some discussion and speculation\n",
    "51:42\n",
    "around Ukraine potentially sourcing some FA-18 Hornets. And reportedly more advanced discussions around Ukraine potentially receiving\n",
    "51:49\n",
    "an aircraft almost custom designed for the war they are now fighting, the Swedish Gripen. It was first reported that Ukrainian pilots were testing and training on the type back in 2023.\n",
    "51:59\n",
    "But so far we haven't heard any announcement that the aircraft will actually be delivered. Of all the fighters we've just discussed, airframe for airframe,\n",
    "52:06\n",
    "the Gripen would arguably be the most useful. But in the absence of any announcement, it doesn't make it into our data.\n",
    "52:12\n",
    "The final question has very little to do with airframes and everything to do with munitions. All else being equal, the combat value of a fighter or multi-role aircraft\n",
    "52:21\n",
    "can vary wildly based on what's on the rails. Ukraine's MiG-29s and especially its Sukhoi 27s can have impressive performance characteristics,\n",
    "52:29\n",
    "and compared to many NATO aircraft they do. But they're also mostly stuck using a diminishing supply\n",
    "52:35\n",
    "of old Soviet air-to-air and air-to-ground munitions. With the arrival of F-16s, the compatibility should be there\n",
    "52:41\n",
    "to unlock much more of NATO's air-delivered arsenal. But we haven't heard much in the way of public announcements\n",
    "52:46\n",
    "to supply any of those potential weapon systems. An F-16 equipped with the AGM-154 Joint Stand-off Weapon\n",
    "52:53\n",
    "is going to be able to engage targets much further away than one using JDAM. And if the unlikely decision was ever made to supply them,\n",
    "52:59\n",
    "an F-16 equipped with the longest range versions of the AGM-158 could potentially target airfields or industrial targets a casual 500+ miles into Russian territory.\n",
    "53:10\n",
    "There would be nowhere in the Black Sea that Russian warships and submarines would be out of range. Nor would there be any Russian fighter air base\n",
    "53:17\n",
    "within a useful distance of Ukraine that couldn't potentially be targeted. That's very much an extreme scenario, and given the demonstrated escalation appetite\n",
    "53:24\n",
    "of the US government to date, a very, very unlikely one. But I think it's a reminder of how important it can be to talk ordnance, not just airframes.\n",
    "53:32\n",
    "In terms of a very rough, uncertain overall picture, the Ukrainian Air Force started the war with a much deeper equipment deficit\n",
    "53:39\n",
    "relative to its Russian opponents than did the Ukrainian ground forces. The technological and quantitative gap was much wider,\n",
    "53:46\n",
    "and Russia has continued to make Ukrainian air power a primary target. Between advanced Russian air-to-air missiles, air defence systems and attacks on Ukrainian air bases,\n",
    "53:55\n",
    "a significant port portion of Ukraine's pre-war airframe inventory has probably been worn down at this point.\n",
    "54:01\n",
    "Some like the MiG-29s have been broadly sustained by an infusion of new parts and airframes. While at this point the Fencer fleet, which continues to occasionally launch Storm Shadows,\n",
    "54:09\n",
    "probably only exists in any sense at this point because of repaired, reactivated or re-purposed airframes.\n",
    "54:15\n",
    "But if nothing changed, the overall trend was unambiguously towards Ukraine running out of airframes over time.\n",
    "54:22\n",
    "That's why the timing and scale of F-16 deliveries and any other Western jets that might eventually be announced are so important.\n",
    "54:28\n",
    "The Ukrainian Air Force is arguably currently in a period of great vulnerability. It's lost 2.5 year's worth of airframes, but it hasn't really received its next generation yet.\n",
    "54:38\n",
    "And while, yes, we should expect to see F-16s shot down or destroyed in Ukraine just as the MiG-29s and Sukhoi 27s have been before them, the fact is they will be there,\n",
    "54:47\n",
    "giving Ukrainian pilots something to fly and Ukrainian commanders options. But as to when we'll start seeing that transition happen in a big way, only time will tell.\n",
    "Changing Force Composition\n",
    "54:56\n",
    "So where does that arguably leave us then in terms of some big picture observations around Ukrainian equipment losses and resupply as we go further into the second half of 2024?\n",
    "55:05\n",
    "Just as with the Russian military, I think there is plenty of evidence that the Ukrainian military's equipment profile is changing.\n",
    "55:11\n",
    "The difference is that while the age and quality of a lot of Russian equipment is going backwards over time, in Ukraine the direction of travel on quality tends to be\n",
    "55:18\n",
    "much more positive, although it is still a mixed bag. If you are replacing Soviet-era tanks with a Leopard 2A6 you can confidently say that's an upgrade.\n",
    "55:26\n",
    "But change that Leopard 2 out for a Leopard 1 and it's a very different picture. If you want to move from talking quality to quantity,\n",
    "55:33\n",
    "I think there are reasons for both Kyiv and Moscow to be deeply frustrated. In terms of some of the key heavy equipment categories, like tanks and infantry fighting vehicles,\n",
    "55:41\n",
    "as we showed, Ukrainian forces are likely roughly treading water. There may have been some decline or growth in the scale of the force,\n",
    "55:48\n",
    "but generally speaking replacements and losses seem to be relatively evenly matched. And frankly that's probably not the result either side is looking for.\n",
    "55:56\n",
    "For Russia it means that after almost 2.5 years of full-scale fighting, the expenditure of vast amounts of men and materiel\n",
    "56:03\n",
    "and deep withdrawals from the old Soviet stockpiles, their invasion to \"demilitarise\" the Ukrainian military is still very much at the starting line.\n",
    "56:11\n",
    "Or in some equipment categories has walked back a couple of metres and back into the stands. They also appear to be in the process of prosecuting a war they claim\n",
    "56:19\n",
    "to partly be about keeping Ukraine away from NATO, and in the process driving the transformation of the Ukrainian military\n",
    "56:25\n",
    "from an essentially post-Soviet force into a much more NATO compatible one. If the Russian intent was to eventually win through grinding attrition,\n",
    "56:33\n",
    "from an equipment perspective they still have a very, very long way to go. On the other hand from the Ukrainian perspective, supply matching losses\n",
    "56:41\n",
    "is likely to be viewed as nowhere near sufficient. Even if the Ukrainian tank and IFV forces were exactly the same size\n",
    "56:47\n",
    "as they were in February 2022, the Ukrainian army isn't. The country has mobilised, many new brigades have been established\n",
    "56:54\n",
    "and on paper they should get equipment too. But especially when you are talking about some of the heavier and rarer stuff\n",
    "57:00\n",
    "(think infantry fighting vehicles, tanks or air defence systems) the evidence suggests that there just isn't enough to go around.\n",
    "57:06\n",
    "We've seen reporting that some of the new brigades Ukraine intends to form are going to be infantry formations, not armoured or mechanised.\n",
    "57:12\n",
    "And Ukrainian Commander Syrskyi himself has reflected on the shortage of things like MANPADS and short-range air defence missiles.\n",
    "57:18\n",
    "It's an interesting flip side to the general estimation that Russia is using more armour in Ukraine now than it was in February and March 2022.\n",
    "57:25\n",
    "Russia has been able to dial up the number of tanks and armoured vehicles deployed in Ukraine through the relatively simple expedient of deploying\n",
    "57:31\n",
    "a greater and greater percentage of its military there, as well as generating new units. Even if the equipment those generated or regenerated units\n",
    "57:38\n",
    "are being issued is getting older and older. In general I think when it comes to things like tanks, IFVs, and certain other equipment categories,\n",
    "57:45\n",
    "the best characterisation is that Ukraine has been given enough equipment to sustain it in the fight and continue to hold back and grind down Russian forces.\n",
    "57:52\n",
    "But it hasn't been given enough to enable it to build up its own strength and throw some more decisive blows in return.\n",
    "57:58\n",
    "And I think while the data suggests that might be the case, I also think it's consistent with our broader observations about the war.\n",
    "58:04\n",
    "After all, whenever a statistical model and reality disagree, reality wins, and the model has to go back to the drawing board.\n",
    "58:10\n",
    "In this case our data suggests that Ukraine started with fewer tanks, IFVs or aircraft than Russia,\n",
    "58:15\n",
    "has received considerably fewer replacements than Russia that has been able to pull things out of storage by the thousands.\n",
    "58:21\n",
    "But at the same time has consistently been able to inflict disproportionate equipment losses. That seems consistent with the kind of grinding, slow moving, attritional war that we're seeing in 2024.\n",
    "58:31\n",
    "Whereas if Russia had been inflicting more losses than the Ukrainians, or the Ukrainians had been receiving far more resupply,\n",
    "58:37\n",
    "we probably would have expected to see the momentum of the fight shift more decisively in one direction or the other.\n",
    "58:43\n",
    "There are a number of silver linings in Ukraine's equipment supply picture. Domestic production has reportedly been increasing,\n",
    "58:49\n",
    "but largely hasn't been included in this analysis. Some high quality systems have been provided, and there are some equipment categories,\n",
    "58:55\n",
    "particularly light vehicles, where resupply volumes have far exceeded visually-confirmed losses.\n",
    "59:00\n",
    "Even if pulling together tank and IFV packages has occasionally been shaky, the MRAPs, IMVs and M113s have continued to flow.\n",
    "59:08\n",
    "The unfortunate side of that however is that it means in some cases Ukrainian forces might be forced to use light vehicles\n",
    "59:14\n",
    "when ideally a heavier option would be preferable. We have plenty of evidence of Ukrainian units using light elements\n",
    "59:20\n",
    "like MRAPs or Humvees in offensive actions not necessarily because they are always the right vehicle for the job,\n",
    "59:26\n",
    "but because sometimes you just have to make do with what you've got. These systems are still obviously very useful for Ukraine in a variety of scenarios,\n",
    "59:32\n",
    "and they'll probably want supplies to continue, but in the end a Humvee is not an APC. And a metal box without a big gun is not the same as a metal box with a big gun.\n",
    "59:41\n",
    "But perhaps the biggest question if you are trying to project the future of the Ukrainian military equipment park in 2024 and 2025\n",
    "59:48\n",
    "is what happens to the pipeline from here? A lot of countries have announced deliveries for the second half of this year and budgets for next year.\n",
    "59:56\n",
    "But what we don't know is how elections and politics might see those budgets evolve in allied capitals,\n",
    "1:00:01\n",
    "and what the plans are to convert some of that budget into equipment on the ground. The Ukrainian military has already overcome a number\n",
    "1:00:08\n",
    "of logistics challenges in beginning to adopt NATO equipment. There's now better training and infrastructure in place to support\n",
    "1:00:14\n",
    "Western systems like the Bradleys or various self-propelled guns. But the future trajectory of the force and whether it can ultimately outlast\n",
    "1:00:21\n",
    "Russia's own equipment reserves will probably be shaped not just by when the already pledged equipment arrives, but by what comes next.\n",
    "1:00:28\n",
    "And so observers will probably remain laser focused on watching the pipeline as they try and assess where and if Ukraine will get the next 500 tanks, 1,000 infantry fighting vehicles,\n",
    "1:00:38\n",
    "and all of the other equipment it's going to want to have as this war carries on. And so while the Ukrainian military has clearly demonstrated\n",
    "1:00:45\n",
    "that after almost 2.5 years of fighting they are still very much capable of fighting hard defensive actions against the Russian force,\n",
    "1:00:51\n",
    "it's absolutely showing the signs of the struggle as it tries to regenerate and adapt to NATO equipment in the middle of a full-scale war.\n",
    "1:00:58\n",
    "Going forward, only time and allied political decisions [determine] whether it's the Ukrainian or Russian equipment inventory that ultimately starts to break down first.\n",
    "Channel Update\n",
    "1:01:08\n",
    "And OK, channel update to close out. Here my main notes are that this video essentially functions as a second part to my previous video on Russian equipment losses.\n",
    "1:01:16\n",
    "So if you haven't reviewed that one yet, please consider doing so. Secondly to the patrons, you should expect to see a poll going up, probably tomorrow,\n",
    "1:01:23\n",
    "where I'm going to be seeking votes and comments on what charitable causes the channel should aim to support in the second half of this year.\n",
    "1:01:30\n",
    "Finally, Perun Gaming fans I know you have been waiting a long while, but there has finally been some new content posted over there.\n",
    "1:01:36\n",
    "And I hope to return to Terra Invicta in the next couple of weeks as well. As always, my very best thanks to all of you\n",
    "1:01:41\n",
    "watching and supporting the channel, and of course to Private Internet Access for their sponsorship today. And I do hope to see you all again next week.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps extract key points out of a video transcript. \\\n",
    "           The user will describe what he wants you to find.\"},\n",
    "          {\"role\":\"user\",\"content\":f\"Tell me how long Ukraine can sustain the war based on information in this video. \\\n",
    "           Tell me what the most important bottlenecks and shortages are that Ukraine is facing.\\\n",
    "           BE SURE TO ADD NUMBERS AND PERCENTAGES TO YOUR ANSWER, as many as are POSSIBLE.\\\n",
    "           transcript of the video: {transcript}\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the video transcript, here are the key points regarding how long Russia can sustain the war, the most important bottlenecks, and shortages Russia is facing, with as many numbers and percentages as possible:\n",
      "\n",
      "### Duration Russia Can Sustain the War\n",
      "1. **Equipment Depletion**: Russia has been heavily reliant on its Soviet-era stockpiles to sustain its war effort. As of the latest data, significant portions of these stockpiles have been depleted:\n",
      "   - **T-80 Tanks**: Of the roughly 1,200 T-80Bs and BVs counted in 2021, only about 200 in decent condition remain in storage.\n",
      "   - **D-30 Artillery**: The number of D-30s in storage dropped from about 3,200 to 937.\n",
      "   - **Overall Tank Storage**: For some tank classes, up to 50% or more of the tanks in decent condition have already been removed from storage.\n",
      "\n",
      "2. **Production Rates**: Russia has scaled up production but still faces challenges:\n",
      "   - **T-90M Tanks**: Estimates range from 231 to 267 tanks produced over a little more than two years, with some sources suggesting up to 200 per annum.\n",
      "   - **Artillery Munitions**: In 2024, Russia is estimated to produce 1,325,000 152mm shells and 800,000 122mm shells. Rocket artillery production has also significantly increased.\n",
      "\n",
      "3. **Foreign Aid and Imports**: Russia has been relying on foreign sources like North Korea and Iran for ammunition and possibly other military supplies.\n",
      "\n",
      "### Bottlenecks and Shortages\n",
      "1. **Quality of Equipment**: The quality of equipment being fielded has deteriorated over time:\n",
      "   - **Modern vs. Old Tanks**: In early 2022, nearly 70% of visually-confirmed Russian tank losses were relatively modern designs. By mid-2024, this dropped to around 30%, with older models and wartime modifications making up a larger share.\n",
      "   - **Infantry Fighting Vehicles (IFVs)**: The share of old BMP-1s in losses increased from just over 10% in early 2022 to more than 40% in mid-2023, before dropping back to around 30% in mid-2024.\n",
      "\n",
      "2. **Storage Depletion**: Significant portions of Russia's stored equipment have been depleted:\n",
      "   - **T-80 Tanks**: Only about 200 out of 1,200 T-80Bs and BVs remain in decent condition.\n",
      "   - **Artillery**: More than 1,000 artillery pieces have been removed from a single storage base, reducing the number from around 1,870 to 755.\n",
      "\n",
      "3. **Industrial Bottlenecks**: Russia faces bottlenecks in upgrading and maintaining equipment:\n",
      "   - **Wartime Upgrades**: Only a small minority of tanks pulled from storage receive wartime upgrade packages. For example, only 36 out of 570 T-80BVs on the loss list had the wartime upgrade package.\n",
      "\n",
      "4. **Diverse Equipment Quality**: The disparity in equipment quality among units is significant:\n",
      "   - **T-90M vs. Older Models**: Tankers might have a 10% chance of getting a modern T-90M or end up with a tank from the 1950s or 60s.\n",
      "\n",
      "5. **Artillery Systems**: The use of older artillery systems like the D-30 and M-46 indicates a shortage of more modern systems:\n",
      "   - **D-30 Artillery**: The number of D-30s in storage has dropped significantly, and their use has increased.\n",
      "   - **M-46 Artillery**: These older systems are being reactivated, likely due to shortages in other calibers and systems.\n",
      "\n",
      "### Summary\n",
      "Russia's ability to\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=prompt,\n",
    "        temperature=0.3,\n",
    "        top_p=0.7,\n",
    "        max_tokens=800,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "        )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the video transcript, here are the key points regarding how long Ukraine can sustain the war, and the most important bottlenecks and shortages it is facing:\n",
      "\n",
      "### Sustainability of the War:\n",
      "1. **Equipment Losses and Resupply**:\n",
      "   - Ukraine has lost nearly 1,000 infantry fighting vehicles, more than 860 tanks, around 600 artillery systems, and 500 APCs.\n",
      "   - Despite these losses, Ukraine has received significant resupply from allies, including about 900 tanks, more than 1,000 infantry fighting vehicles, 3,500 armored personnel carriers, and over 5,000 infantry mobility vehicles.\n",
      "   - Captured Russian equipment has also played a significant role, with Ukraine visually confirmed to have captured about 3,000 pieces of heavy equipment, including more than 500 tanks and 616 IFVs.\n",
      "\n",
      "2. **Manpower**:\n",
      "   - Ukraine has faced significant manpower shortages, but changes to mobilization rules and initiatives like the \"Polish Legion\" have increased induction rates.\n",
      "   - The Polish Legion initiative alone saw several thousand registrations within days of its announcement.\n",
      "\n",
      "3. **Air Force**:\n",
      "   - Ukraine's Air Force has lost 97 aircraft but has received or been pledged 130 replacements.\n",
      "   - The arrival of F-16s, with 95 pledged, is crucial for sustaining air capabilities, although delivery timelines extend to 2027.\n",
      "\n",
      "### Bottlenecks and Shortages:\n",
      "1. **Heavy Equipment**:\n",
      "   - Despite resupply, Ukraine is roughly treading water in terms of tanks and infantry fighting vehicles. The country has mobilized new brigades that require equipment, but there isn't enough to go around.\n",
      "   - Ukraine has received about 900 tanks, but only about 600 have been delivered. The visually confirmed losses are roughly 870 tanks.\n",
      "   - For infantry fighting vehicles, Ukraine has lost about 830 Soviet IFVs but received 670 replacements. For NATO IFVs, losses are 137 against 729 received.\n",
      "\n",
      "2. **Aircraft**:\n",
      "   - The Ukrainian Air Force is in a vulnerable position, with significant losses in Sukhoi 24 and 25 aircraft. The F-16s are expected to replace many of these losses, but delivery timelines are long.\n",
      "   - The MiG-29 fleet is treading water with 30 losses and 27 replacements, but the number of tasks assigned to this fleet has increased.\n",
      "\n",
      "3. **Light Vehicles**:\n",
      "   - Ukraine has received a significant number of light vehicles like MRAPs and Humvees, but these are not ideal substitutes for heavier equipment in offensive operations.\n",
      "   - The visually confirmed loss of fewer than 300 NATO APCs has been balanced by inflows of more than 3,300, resulting in a net positive of 3,060 vehicles.\n",
      "\n",
      "4. **Domestic Production and Reactivation**:\n",
      "   - There is limited public data on how much equipment Ukraine is producing or reactivating domestically. However, it is happening, and budget is being allocated to it.\n",
      "\n",
      "5. **Captured Equipment**:\n",
      "   - While captured Russian equipment has been significant, the rate of capturing new equipment has decreased, making Ukraine more dependent on domestic production and allied deliveries.\n",
      "\n",
      "### Numbers and Percentages:\n",
      "- **Tanks**: 870 losses, 900 pledged, 600 delivered.\n",
      "- **IFVs**: 830 Soviet losses, 670 replacements; 137 NATO losses, 729 replacements.\n",
      "- **Aircraft**: 97 losses, 130 replacements pledged.\n",
      "- **Captured Equipment**: 3,000 pieces, including 500 tanks and 616 IFVs.\n",
      "- **Light Vehicles**: 300 NATO APC losses, 3,300 replacements, net positive 3,060.\n",
      "\n",
      "### Conclusion:\n",
      "Ukraine can sustain the war as long as the resupply from allies continues and domestic production can be ramped up. However, the most critical bottlenecks are\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=prompt,\n",
    "        temperature=0.3,\n",
    "        top_p=0.7,\n",
    "        max_tokens=800,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "        )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_russia = \"\"\"Based on the information provided in the video transcript, here are the key points regarding how long Russia can sustain the war, the most important bottlenecks, and shortages Russia is facing, with as many numbers and percentages as possible:\n",
    "\n",
    "### Duration Russia Can Sustain the War\n",
    "1. **Equipment Depletion**: Russia has been heavily reliant on its Soviet-era stockpiles to sustain its war effort. As of the latest data, significant portions of these stockpiles have been depleted:\n",
    "   - **T-80 Tanks**: Of the roughly 1,200 T-80Bs and BVs counted in 2021, only about 200 in decent condition remain in storage.\n",
    "   - **D-30 Artillery**: The number of D-30s in storage dropped from about 3,200 to 937.\n",
    "   - **Overall Tank Storage**: For some tank classes, up to 50% or more of the tanks in decent condition have already been removed from storage.\n",
    "\n",
    "2. **Production Rates**: Russia has scaled up production but still faces challenges:\n",
    "   - **T-90M Tanks**: Estimates range from 231 to 267 tanks produced over a little more than two years, with some sources suggesting up to 200 per annum.\n",
    "   - **Artillery Munitions**: In 2024, Russia is estimated to produce 1,325,000 152mm shells and 800,000 122mm shells. Rocket artillery production has also significantly increased.\n",
    "\n",
    "3. **Foreign Aid and Imports**: Russia has been relying on foreign sources like North Korea and Iran for ammunition and possibly other military supplies.\n",
    "\n",
    "### Bottlenecks and Shortages\n",
    "1. **Quality of Equipment**: The quality of equipment being fielded has deteriorated over time:\n",
    "   - **Modern vs. Old Tanks**: In early 2022, nearly 70% of visually-confirmed Russian tank losses were relatively modern designs. By mid-2024, this dropped to around 30%, with older models and wartime modifications making up a larger share.\n",
    "   - **Infantry Fighting Vehicles (IFVs)**: The share of old BMP-1s in losses increased from just over 10% in early 2022 to more than 40% in mid-2023, before dropping back to around 30% in mid-2024.\n",
    "\n",
    "2. **Storage Depletion**: Significant portions of Russia's stored equipment have been depleted:\n",
    "   - **T-80 Tanks**: Only about 200 out of 1,200 T-80Bs and BVs remain in decent condition.\n",
    "   - **Artillery**: More than 1,000 artillery pieces have been removed from a single storage base, reducing the number from around 1,870 to 755.\n",
    "\n",
    "3. **Industrial Bottlenecks**: Russia faces bottlenecks in upgrading and maintaining equipment:\n",
    "   - **Wartime Upgrades**: Only a small minority of tanks pulled from storage receive wartime upgrade packages. For example, only 36 out of 570 T-80BVs on the loss list had the wartime upgrade package.\n",
    "\n",
    "4. **Diverse Equipment Quality**: The disparity in equipment quality among units is significant:\n",
    "   - **T-90M vs. Older Models**: Tankers might have a 10% chance of getting a modern T-90M or end up with a tank from the 1950s or 60s.\n",
    "\n",
    "5. **Artillery Systems**: The use of older artillery systems like the D-30 and M-46 indicates a shortage of more modern systems:\n",
    "   - **D-30 Artillery**: The number of D-30s in storage has dropped significantly, and their use has increased.\n",
    "   - **M-46 Artillery**: These older systems are being reactivated, likely due to shortages in other calibers and systems.\n",
    "\n",
    "### Summary\n",
    "Russia's ability to\n",
    "\"\"\"\n",
    "\n",
    "response_ukraine = \"\"\"Based on the information provided in the video transcript, here are the key points regarding how long Ukraine can sustain the war, and the most important bottlenecks and shortages it is facing:\n",
    "\n",
    "### Sustainability of the War:\n",
    "1. **Equipment Losses and Resupply**:\n",
    "   - Ukraine has lost nearly 1,000 infantry fighting vehicles, more than 860 tanks, around 600 artillery systems, and 500 APCs.\n",
    "   - Despite these losses, Ukraine has received significant resupply from allies, including about 900 tanks, more than 1,000 infantry fighting vehicles, 3,500 armored personnel carriers, and over 5,000 infantry mobility vehicles.\n",
    "   - Captured Russian equipment has also played a significant role, with Ukraine visually confirmed to have captured about 3,000 pieces of heavy equipment, including more than 500 tanks and 616 IFVs.\n",
    "\n",
    "2. **Manpower**:\n",
    "   - Ukraine has faced significant manpower shortages, but changes to mobilization rules and initiatives like the \"Polish Legion\" have increased induction rates.\n",
    "   - The Polish Legion initiative alone saw several thousand registrations within days of its announcement.\n",
    "\n",
    "3. **Air Force**:\n",
    "   - Ukraine's Air Force has lost 97 aircraft but has received or been pledged 130 replacements.\n",
    "   - The arrival of F-16s, with 95 pledged, is crucial for sustaining air capabilities, although delivery timelines extend to 2027.\n",
    "\n",
    "### Bottlenecks and Shortages:\n",
    "1. **Heavy Equipment**:\n",
    "   - Despite resupply, Ukraine is roughly treading water in terms of tanks and infantry fighting vehicles. The country has mobilized new brigades that require equipment, but there isn't enough to go around.\n",
    "   - Ukraine has received about 900 tanks, but only about 600 have been delivered. The visually confirmed losses are roughly 870 tanks.\n",
    "   - For infantry fighting vehicles, Ukraine has lost about 830 Soviet IFVs but received 670 replacements. For NATO IFVs, losses are 137 against 729 received.\n",
    "\n",
    "2. **Aircraft**:\n",
    "   - The Ukrainian Air Force is in a vulnerable position, with significant losses in Sukhoi 24 and 25 aircraft. The F-16s are expected to replace many of these losses, but delivery timelines are long.\n",
    "   - The MiG-29 fleet is treading water with 30 losses and 27 replacements, but the number of tasks assigned to this fleet has increased.\n",
    "\n",
    "3. **Light Vehicles**:\n",
    "   - Ukraine has received a significant number of light vehicles like MRAPs and Humvees, but these are not ideal substitutes for heavier equipment in offensive operations.\n",
    "   - The visually confirmed loss of fewer than 300 NATO APCs has been balanced by inflows of more than 3,300, resulting in a net positive of 3,060 vehicles.\n",
    "\n",
    "4. **Domestic Production and Reactivation**:\n",
    "   - There is limited public data on how much equipment Ukraine is producing or reactivating domestically. However, it is happening, and budget is being allocated to it.\n",
    "\n",
    "5. **Captured Equipment**:\n",
    "   - While captured Russian equipment has been significant, the rate of capturing new equipment has decreased, making Ukraine more dependent on domestic production and allied deliveries.\n",
    "\n",
    "### Numbers and Percentages:\n",
    "- **Tanks**: 870 losses, 900 pledged, 600 delivered.\n",
    "- **IFVs**: 830 Soviet losses, 670 replacements; 137 NATO losses, 729 replacements.\n",
    "- **Aircraft**: 97 losses, 130 replacements pledged.\n",
    "- **Captured Equipment**: 3,000 pieces, including 500 tanks and 616 IFVs.\n",
    "- **Light Vehicles**: 300 NATO APC losses, 3,300 replacements, net positive 3,060.\n",
    "\n",
    "### Conclusion:\n",
    "Ukraine can sustain the war as long as the resupply from allies continues and domestic production can be ramped up. However, the most critical bottlenecks are\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps the user.\"},\n",
    "          {\"role\":\"user\",\"content\":f\"Compare the points in the text about Ukrainian and Russian attrition losses in the war so far. Tell me which side is better off, and maybe how long until they ecounter significant problems.\\\n",
    "           . Russia: {response_russia}; Ukraine: {response_ukraine}\\\n",
    "            Do not just repeat the information, but analyze and compare the two sides.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Comparative Analysis of Ukrainian and Russian Attrition Losses\n",
      "\n",
      "#### Equipment Depletion and Resupply\n",
      "\n",
      "**Russia:**\n",
      "- **Tanks and Artillery:** Russia has significantly depleted its Soviet-era stockpiles. For example, only about 200 out of 1,200 T-80Bs and BVs remain in decent condition. Artillery stockpiles have also seen a drastic reduction, with D-30s in storage dropping from about 3,200 to 937.\n",
      "- **Production Rates:** Russia has increased production rates but still faces challenges. For instance, T-90M tank production estimates range from 231 to 267 tanks over two years, with some sources suggesting up to 200 per annum. Artillery munitions production has also increased.\n",
      "- **Foreign Aid:** Russia is relying on foreign sources like North Korea and Iran for ammunition and possibly other military supplies.\n",
      "\n",
      "**Ukraine:**\n",
      "- **Tanks and IFVs:** Ukraine has lost nearly 1,000 infantry fighting vehicles and more than 860 tanks. However, it has received significant resupply from allies, including about 900 tanks and more than 1,000 infantry fighting vehicles. Captured Russian equipment has also played a significant role, with Ukraine visually confirmed to have captured about 3,000 pieces of heavy equipment.\n",
      "- **Aircraft:** Ukraine's Air Force has lost 97 aircraft but has received or been pledged 130 replacements, including 95 F-16s, although delivery timelines extend to 2027.\n",
      "\n",
      "**Analysis:**\n",
      "Ukraine appears to be better off in terms of resupply and equipment replacement, thanks to substantial support from allies and captured Russian equipment. Russia, on the other hand, is depleting its stockpiles faster and faces challenges in ramping up production to meet its needs.\n",
      "\n",
      "#### Quality of Equipment\n",
      "\n",
      "**Russia:**\n",
      "- **Modern vs. Old Tanks:** The quality of Russian equipment has deteriorated over time. In early 2022, nearly 70% of visually-confirmed Russian tank losses were relatively modern designs. By mid-2024, this dropped to around 30%, with older models making up a larger share.\n",
      "- **Infantry Fighting Vehicles (IFVs):** The share of old BMP-1s in losses increased from just over 10% in early 2022 to more than 40% in mid-2023, before dropping back to around 30% in mid-2024.\n",
      "\n",
      "**Ukraine:**\n",
      "- **Heavy Equipment:** Despite resupply, Ukraine is roughly treading water in terms of tanks and infantry fighting vehicles. The country has mobilized new brigades that require equipment, but there isn't enough to go around.\n",
      "- **Aircraft:** The Ukrainian Air Force is in a vulnerable position, with significant losses in Sukhoi 24 and 25 aircraft. The F-16s are expected to replace many of these losses, but delivery timelines are long.\n",
      "\n",
      "**Analysis:**\n",
      "Both sides face challenges in maintaining the quality of their equipment. Russia is increasingly relying on older models, while Ukraine is struggling to equip new brigades adequately despite receiving modern equipment from allies.\n",
      "\n",
      "#### Manpower and Industrial Bottlenecks\n",
      "\n",
      "**Russia:**\n",
      "- **Industrial Bottlenecks:** Russia faces bottlenecks in upgrading and maintaining equipment. For example, only a small minority of tanks pulled from storage receive wartime upgrade packages.\n",
      "- **Diverse Equipment Quality:** The disparity in equipment quality among units is significant, with some units receiving modern T-90Ms and others older models from the 1950s or 60s.\n",
      "\n",
      "**Ukraine:**\n",
      "- **Manpower:** Ukraine has faced significant manpower shortages but has increased induction rates through changes to mobilization rules and initiatives like the \"Polish Legion.\"\n",
      "- **Domestic Production and Reactivation:** There is limited public data on how much equipment Ukraine is producing or reactivating domestically, but it is happening, and budget is being allocated to it.\n",
      "\n",
      "**Analysis:**\n",
      "Ukraine has managed to address some of its manpower issues through mobilization initiatives, whereas Russia faces significant industrial bottlenecks that hinder its ability to upgrade and maintain equipment. The disparity in equipment quality among Russian units could also affect their operational effectiveness.\n",
      "\n",
      "#### Conclusion\n",
      "\n",
      "**Which Side is Better Off?**\n",
      "- **Ukraine** appears to be in a relatively better position due to substantial resupply from allies, captured Russian equipment, and initiatives to address manpower shortages. However, it still faces challenges in adequately equipping new brigades and replacing aircraft losses.\n",
      "- **Russia** is facing significant equipment depletion, industrial bottlenecks, and a reliance on older, lower-quality equipment. While it has ramped up production and sought foreign aid, these measures may not be sufficient to sustain long-term operations at the current intensity.\n",
      "\n",
      "**How Long Until Significant Problems?**\n",
      "- **Russia** could encounter significant problems sooner, given the rapid depletion of its stockpiles and the challenges in maintaining and upgrading its equipment. The reliance on older models and foreign aid suggests that Russia may face critical shortages within the next year or two if the current rate of attrition continues.\n",
      "- **Ukraine** could sustain its war effort longer, provided that the resupply from allies continues and domestic production can be ramped up. However, the long delivery timelines for aircraft like the F-16s and the need to equip new brigades could pose challenges in the medium term.\n",
      "\n",
      "In summary, while both sides face significant challenges, Ukraine's external support and captured equipment give it a relative advantage over Russia, which is grappling with severe equipment depletion and industrial bottlenecks.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=prompt,\n",
    "        temperature=0.3,\n",
    "        top_p=0.7,\n",
    "        max_tokens=4096,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "        )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the synchronous function go to therapy?\n",
      "\n",
      "Because it couldn't handle the wait!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from genai_sdk import PwCAzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Construct the full path to the env file\n",
    "directory = os.getcwd()\n",
    "file_name = 'conf.env'  # Replace with the name of file\n",
    "file_path = os.path.join(directory, file_name)\n",
    "\n",
    "# Load environment variables from .env file (for a service account --> set override = True)\n",
    "load_dotenv(file_path)\n",
    "\n",
    "# Get the API_KEY\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "llm = PwCAzureChatOpenAI(\n",
    "    apikey=api_key,\n",
    "    api_version=\"2023-03-15-preview\",\n",
    "    model='gpt-4o',\n",
    "    max_tokens=4096,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(llm.invoke('Tell me a joke about synchronous function!').content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "KEY = \"07d376b4bbce4baf8a746f8be5437d0d\"\n",
    "ENDPOINT = \"https://lucasaieastus2servicesopen.cognitiveservices.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "        api_key=KEY,\n",
    "        api_version=\"2023-12-01-preview\",\n",
    "        azure_endpoint=ENDPOINT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the synchronous function go to therapy?\n",
      "\n",
      "Because it couldn't handle the wait!\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant that helps the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about synchronous function!\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        top_p=0.7,\n",
    "        max_tokens=4096,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "        )\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsotest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
